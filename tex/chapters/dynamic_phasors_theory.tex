%--------------------------------------------------------------------------------------------------
\chapter{Dynamic Phasors Theory}\label{chapter:dynamic_phasor_theory}
%--------------------------------------------------------------------------------------------------

	 In a direct language, the essence of phasors is that if one is amenable to disregarding the transient response of the system, the steady-state solution of the states of the system can be found as some particular orbit of the differential equation. When the excitation is comprised of static sinusoids of frequency $\omega$, static sinusoids of frequency $\omega$ comprise a particular solution of the differential equations defined by the system; due to the exponentially stable nature of passive linear circuits, this sinusoidal particular orbit is also the exponentially stable steady-state behavior of the system. In simpler terms, phasors are a ``sneaky'' way to solve sinusoidally excited LTI differential equations, given one is willing to discard transient phenomena.

	In the past chapter, some very important tools of circuit analysis — Kirchoff's Voltage and Current laws, the Superposition Theorem, and the Thèvenin-Norton Theorems — were left out because they will be proven for the more generalized Dynamic Phasor case. Using these theorems, the capacitive conductance and inductive impedances are defined as $Y_C = j\omega C$ and $Z_L = j\omega L$, and these entities are applied to an electrical grid making the phasorial analysis self-sufficient in the sense that the time-domain analysis does not need to be undertaken first before using phasors. Finally, the complex power $S = V \overline{I}$ is shown to be a direct representation of the instantaneous AC power of a circuit, and its real part the average power developed by that circuit.

	For all its ellegance, however, the Classical Phasor Theory only embraces a very specific type of signals: static sinusoids — meaning constant amplitude, frequency and phase. However, in most Electrical Engineering studies, the sinusoidal voltages of agents and nodes are not ``static'', in the sense that they exhibit transient effects such as time-varying amplitudes, phases and even frequencies. Particularly in Electric Power Systems, such phenomena are ubiquituous and a common occurence after disturbances like loads or faults.

%-------------------------------------------------
\section{Nonstationary sinusoidal signals: the current theory of Dynamic Phasors} %<<<2

	We want to introduce the theory of Dynamic Phasors, which can be summarized as being the time-varying alternatives to classical phasors. This means that the objective is to embrace a larger, more general class signals of a certain ``sinusoidal shape'', like in definition \ref{def:sinusoid} .

\begin{definition}[Sinusoid] \label{def:sinusoid} %<<<
	A signal $x(t)\in\left[\mathbb{R}\to\mathbb{R}\right]$ is a \textbf{sinusoid} if there are two functions $m(t)$ called a modulus or amplitude (\textit{moduli} in the plural) and $\theta(t)$ called the angle such that $x(t) = m(t)\cos\left(\theta(t)\right)$. Furthermore, $x$ is a \textbf{stationary sinusoid} or phase if $m$ and $\dot{\theta}$ are constant, and \textbf{nonstationary} if else.
\end{definition} %>>>

	Particularly, we are interested in sinusoids which angle can be decomposed in a time-varying notion of frequency and phase. By correlation, because in static phasors the frequency multiplies the time $t$, consider first the signals

\begin{equation} x(t) = m\left(t\right)\cos\left[\omega(t) t + \phi(t)\right], \label{eq:dynamic_sinusoid_example}\end{equation}

	\noindent where the amplitude $m$, frequency $\omega$ and phase $\phi$ are time-varying correlative quantities of the amplitude, frequency and phase of static phasors.

	Notably, the Static Phasor Operator is only applicable to the signal \eqref{eq:dynamic_sinusoid_example} if m, $\omega$ and $\phi$ are constants, meaning that the Classical Phasor Theory is unnaplicable otherwise, that is, there is no phasor representation possible for signal \eqref{eq:dynamic_sinusoid_example} that can allow for the solution of the linear ODEs. In this scenario, a natural question is wether there is some extended idea of phasor, variant in time, to denote such a signal; the intuitive candidate would be

\begin{equation} X(t) = m\left(t\right)e^{j\phi(t)}. \end{equation}

	Despite being elementary to note how this phasor reconstructs $x(t)$, this is not a bijective transformation that can take a signal in time to translate it into a complex number in the frequency-space algebraic domain that, when solved, can reconstruct the original solution to the original differential equations, like the Static Fourier Phasors can.

	In summary, the classic idea of phasor, while allowing for phasorial representation of signals, can only do so for static sinusoidal signals and fails to give a mathematical tool that can solve more sophisticated nonstationary signals. As shown in the introduction, the justification of applying the CPT to phasorial dynamical systems requires the Quasi-Static Modelling, that is, the supposition that the grid dynamics need to be supposed much faster than the transients of the agents that act upon it, allowing modelling the grid in its static purely sinusoidal behavior, thus allowing for its complexification. This assumption is broken when switched power systems like inverters are at play, because the timescale of their dynamics are comparable to the timescales of grid dynamics.

	To develop Dynamic Phasors, the literature by default escalates Classical Phasors to integral transforms, inspired by a branch of mathematics called Time-Frequency Analysis, founded with the intent of expressing nonstationary time signals in the frequency domain. The most used strategies revolve around integral transformations of some form, that is, to decompose a signal $x(t)\in\left[\mathbb{R}\to\mathbb{C}\right]$ into a combination of translations (dilations, contractions and shifts) of a base or reference function that has a defined frequency spectrum. Call this base function $\mu\left(t\right)$; then the transformation of $x(t)$ with respect to $\mu$ is given by the inner product

\begin{equation} \left< x \right>_{\left(a,b\right)} = \left<x,\mu \left(\dfrac{\tau-a}{b}\right)\right> = \int_{-\infty}^{\infty} x\left(\tau\right)\overline{\raisebox{5mm}{} \mu \left(\dfrac{\tau-a}{b}\right)} d\tau, \end{equation}

	The idea is to adopt a base function $\mu$ that when translated forms a basis over the Hilbert Space of square-integrable functions $L^2\left(\mathbb{R}\right)$, that is, the base function $\mu$ needs to have finite energy (equivalent to being square integrable), the translations must be orthogonal and the span of the translations must be $L^2\left(\mathbb{R}\right)$. The result $\left< x \right>_{\left(a,b\right)}$ is then called the \textbf{component} or \textbf{harmonic} of $x(t)$ at the parameters $a$ and $b$. Famous examples of these integral transforms are the Short-Time Fourier Transform (STFT) and the Wavelet Transform (WT). In the STFT, the base function $\mu$ is the exponential $e^{-j\omega \tau}$ multiplied by a windowing function $w\left(\tau\right)$:

\begin{equation} \mu_{\left(\text{STFT}\right)}\left(\omega,\tau\right) = w\left(\tau\right)e^{j\omega\tau}.\end{equation}
	
	Typical cases of the windowing function are the gaussian distribution or a simple rectangular function. These windowings mean that the STFT is in essence a Fourier Transform of the original signal $x(t)$ limited by the window; as the time $t$ slides, so does the window. Therefore, the STFT corresponds to the time-sliding Fourier Transform of the windowed signal $x(t)$. It can be further shown that a rectangular  windowing represents a convolution with the sinc function in the frequency domain, which introduces ringing artifacts and oscillations in short time windows; this can be seen as a consequence of the Nyquist Theorem, which states that as the sampling frequency window gets narrower, the convolution yields alising phenomena. To mitigate this, some windows as the Gaussian Function or the Hann Window act as filters that avoid such behavior.

	In the case of the Wavelet Transform, the base function is called the \textit{mother wavelet}, and there are several ones to choose from in the literature — such as the Ricker Wavelet, the Morlet Wavelet, Butterworth Wavelet, Ormsby Wavelet as compared in \cite{ryanRickerOrmsbyKlander1994} — each option featuring benefits and disadvantages in signal processing. The benefit of the Wavelet Transform is that, unlike the STFT, it provides high-frequency resolution at lower frequency by means of its bidimensional transform \pcite{Guo2022}. Multidimensional wavelet transformations are also available in the literature, first conceived by \cite{zouDiscreteOrthogonalMband1992}.

	In this thesis, the theoretical bases for Dynamic Phasors based on the rectangular windowing of Short Time Fourier Transforms will be presented as the most common candidate for the theoretical basis of Dynamical Phasors in the Electric Power System literature. In the next subsections, the STFT will be presented and defined rigorously; it will be proven that the STFT does offer the notion of a time-frequency representation of nonstationary sinusoids and it does expand on the notion of impedances. Because the developments are based on the Fourier Transform, the Dynamic Phasors resulting from this analysis are thenceforth called STFT Dynamic Phasors (STFT-DPs).

	It will be further shown that there are two fundamental shortcomings with the STFT, namely the fact that it fails to offer a reconstruction of the original time signals of an electrical grid, and it fails to offer a well-defined notion of a complex power. It will also be shown that, unlike their static phasor counterpart, Fourier Dynamic Phasors lack the ability to transform time differential equations into algebraic ones; rather, they produce infinite sets of complex differential equations. Consequently, the STFT framework offers little solace when it comes to the representation of non-static sinusoidal signals.

	Further, we also take a look at the Hilbert Transform (HT), which while able to produce \textit{some} notion of time-varying phasors, it has very specific characteristics that make it very limited in scope. Moreover, it is really only applicable to a limited class of signals — namely those which Fourier Transform of the amplitude has a support limited by the transform of the angle — and that it also fails to produce dynamic counterparts to active and reactive power.

	In other words, neither the STFT or the HT offer alternatives to theorems \ref{theo:phasors_solutions} and \ref{theo:sfp_complex_apparent_power}, that is, the complex signals obtained through those transforms are not guaranteed to be losslessly reconstruct the solutions to the original time differential equations of the electrical system, and the instantaneous power cannot be obtained from the inner product of the complex space induced by the Dynamic Phasors of neither STFT or HT. Furthermore, the solution of the equivalent system in the frequency domain is significantly more difficult than in the time domain, rendering it questionable when it comes to practicality and usefulness.

%------------------------------------------------
\section{Short-Time Fourier Transform of nonstationary signals} %<<<1

	Take the signal $x(t)$ of Figure \ref{fig:stft_schematic}, sampled through a window $w(t)$ of size $T$ (which can be time-varying as long as it is positive), generating a windowed signal $y(s) = x(s)w(s-T)$; then the Fourier Transform of this sampled signal is taken:

\begin{equation} \mathbf{F}\left[y\right] = \int_{\ \mathbb{R}} x\left(s\right)w\left(s - t\right) e^{-j\omega s}ds . \label{eq:stft_generic}\end{equation}

% GAUSS STFT DIAGRAM <<<
\begin{figure}[t]
\centering
	\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
	\draw [black!30, line width=5mm, -{Stealth[inset=0mm,length=10mm,angle'=50]}] (-12mm,17mm) -- (7mm,17mm);
	\node[black] at (-5mm, 17mm)  (stftlabel) {STFT};

		\begin{axis}[
			at={(-75mm,0mm)},
			width=75mm,
			height=50mm,
			tick style={draw=none},
			axis lines=middle,
			xmin = -2, xmax = 9,
			ymin = -0.3, ymax = 3,
 			xticklabel={\empty}, yticklabel={\empty},
			xlabel = {$s$}
			]
			\addplot  [thick, stewartblue, domain=-2:9, smooth, samples=100] {0.1*((0.9*x - 1.4)^3 - 3*(x - 1.4)^2 - 0.4*(x-1.4) + 15)};
			\addplot  [thick, stewartpink, domain=-2:9, smooth, samples=100] {1.8*exp(-(x/2-1.5)^2)};
			\addplot +[thick, mark=none,stewartpink, dashed] coordinates {(3, 0) (3, 1.8)};

			\node[stewartblue] at (axis cs:7.5,2.2)  (xlabel) {$x(s)$};
			\node[stewartpink] at (axis cs:3,2.2)  (wlabel) {$w(s-t)$};
			\node[stewartpink] at (axis cs:3,-0.2)  (wlabel) {$t$};
		\end{axis}

		\begin{axis}[
			at={(10mm,0mm)},
			width=75mm,
			height=50mm,
			tick style={draw=none},
			axis lines=middle,
			xmin = -2, xmax = 9,
			ymin = -0.25, ymax = 2,
 			xticklabel={\empty}, yticklabel={\empty},
			xlabel = {$s$}
			]
			\addplot  [thick, stewartgreen, domain=-2:9, smooth, samples=100] { 1.8*exp(-(x/2-1.5)^2)*(0.1*((0.9*x - 1.4)^3 - 3*(x - 1.4)^2 - 0.4*(x-1.4) + 15))};
			\node[stewartgreen] at (axis cs:6.3,1.5)  (xlabel) {$y(s) = x(s)w(s-t)$};
		\end{axis}

	\end{tikzpicture}
	\caption
[Schematization of the STFT through a gaussian window to produce a windowed signal.]
{Schematization of the STFT: a signal $x(t)$ is sampled through a window (in this case a gaussian) to produce a windowed signal which is subject to a Fourier Transform.}
	\label{fig:stft_schematic_generic}
\end{figure} %>>>

	As the time $t$ grows, the translated window $w(s-t)$ ``slides'', such that at each time $t$ the periodic signal resulting changes. This causes $\mathbf{F}\left[y\right] = Y\left(\omega,t\right)$ to be a complex time function in both frequency $\omega$ and the time $t$. Therefore, these harmonics become time-varying. Figure \ref{fig:stft_schematic_generic} shows a schematic of this process using a gaussian window, that is, a contraction-translation of $w(x) = e^{-x^2}$.

	Different windows yield particular sampling characteristics — for instance, the gaussian window naturally yields statistical properties of minimizing the standard deviation for a generic sampled signal. It is clear to see that this process makes it considerably difficult to model generic signals seen as it results two-dimensional complex functions, and it requires $x(t)$ to be defined everywhere the window is also defined (in the case of the gaussian window, the entirety of reals). Therefore, for the purposes of modelling, the most used window is the ``boxcar'' or simply rectangular window; the windowed signal produced is a periodic restriction of the original signal $x(t)$, therefore it can be written as a Fourier Series at the frequency $\omega$. This ``substitutes'' the frequency component $\omega$ for integer indexes, that is, harmonics at the multiple frequencies $k 2\pi/T(t)$. Again, as the time $t$ grows, the window ``slides'', such that at each time $t$ the periodic signal resulting changes, and so do its harmonics; therefore, these harmonics become time-varying.

% STFT DIAGRAM <<<
\begin{figure}[t]
\centering
	\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
	\draw [black!30, line width=5mm, -{Stealth[inset=0mm,length=10mm,angle'=50]}] (-12mm,17mm) -- (7mm,17mm);
	\node[black] at (-5mm, 17mm)  (stftlabel) {STFT};

		\begin{axis}[
			at={(-50mm,0mm)},
			width=50mm,
			height=50mm,
			tick style={draw=none},
			axis lines=middle,
			xmin = -0.25, xmax = 7,
			ymin = -0.3, ymax = 2.2,
 			xticklabel={\empty}, yticklabel={\empty},
			xlabel = {$s$}
			]
			\addplot  [thick, stewartblue, domain=-0.25:7, smooth, samples=100] {0.1*((0.9*x - 1.4)^3 - 3*(x - 1.4)^2 - 0.4*(x-1.4) + 15)};
			\addplot  [thick, stewartpink, dashed, domain=1:5, smooth, samples=100] {1.6};
			\addplot +[thick, mark=none,stewartpink, dashed] coordinates {(1, 0) (1, 1.6)};
			\addplot +[thick, mark=none,stewartpink, dashed] coordinates {(5, 0) (5, 1.6)};

			\node[stewartblue] at (axis cs:5.6,2)  (xlabel) {$x(s)$};
			\node[stewartpink] at (axis cs:3,1.8)  (wlabel) {$w(s-t)$};
			\node[stewartpink] at (axis cs:1,-0.2)  (tlabel) {$t-T$};
			\node[stewartpink] at (axis cs:5,-0.2)  (tTlabel) {$t$};
		\end{axis}

		\begin{axis}[
			at={(10mm,0mm)},
			width=100mm,
			height=50mm,
			tick style={draw=none},
			axis lines=middle,
	%		axis line style={black!50},
			xmin = -9, xmax = 9,
			ymin = -0.25, ymax = 1.8,
 			xticklabel={\empty}, yticklabel={\empty},
			xlabel = {$s$}
			]
			\foreach \x  [evaluate=\x as \xeval] in {-3,...,2}
			{
				\addplot[thick, stewartblue, domain=4*\xeval:4*(\xeval + 1), smooth, samples=100] {0.1*((0.9*(x+1-4*\xeval) - 1.4)^3 - 3*((x+1-4*\xeval) - 1.4)^2 - 0.4*((x+1-4*\xeval)-1.4) + 15)};
			}
			\foreach \x  [evaluate=\x as \xeval] in {-3,-2,-1,1,2} {
				\addplot +[thick, mark=none,stewartpink, dashed] coordinates {(4*\xeval, 0) (4*\xeval, 1.8)};
				\edef\temp{\noexpand \node[thick, stewartpink,below] at (axis cs:4*\xeval,0)  (\x T) {$\x T$};} \temp
			}
		\end{axis}

	\end{tikzpicture}
	\caption
[Schematization of the STFT using a rectangular window.]
{Schematization of the STFT using a rectangular window: a signal $x(t)$ is sampled to produce a periodic restriction which is subject to a Fourier Series.}
	\label{fig:stft_schematic}
\end{figure} %>>>

	Thus, the process is as follows: take a signal $x(t)$ and a period signal $T(t)$ equivalent to a frequency $\omega(t) = 2\pi/T(t)$, and define a periodic signal as the restriction of $x(t)$ on $\left[t-T(t),t\right]$, that is,

\begin{equation} y: \left\{\begin{array}{rcl} \left[0,T\right] &\to& \mathbb{C} \\[3mm] s &\mapsto& x\left(s + T - t\right) \end{array}\right. \end{equation}

	\noindent then the harmonics are calculated as the Fourier Series of this periodic restriction:

\begin{equation} \left\langle y \right\rangle_k = \dfrac{1}{T}\int\limits_{0}^{T}y\left(s\right) e^{-jk\omega s}ds  \end{equation}

	\noindent and Theorem \ref{theo:stft_analysis} proves that this transformation reconstructs $x(t)$.

\begin{theorem}[Short-Time Fourier Transform Analysis \pcite{volpatoDynamicPhasorTransform2022}] \label{theo:stft_analysis}%<<<
	Let $x(t)\in\left[\mathbb{R}\to\mathbb{C}\right]$ square-integrable in some window interval $\left(t-T,t\right]$, with $T\in\mathbb{R}_+^*$ the window period (that may be time-varying) and $\omega = 2\pi T^{-1}$ the corresponding window angular frequency. Let $\left\langle x \right\rangle_k\hspace{-0.5mm} \left(t\right)\in\left[\mathbb{R}\to\mathbb{C}\right]$ be the k-th order harmonic of the Fourier Series of $x(t)$ limited to the interval, that is,

\begin{equation} \left\langle x \right\rangle_k\hspace{-0.5mm} \left(t\right) = \dfrac{1}{T}\int\limits_{t-T}^{t}x\left(\lambda\right) e^{-jk\omega\lambda}d\lambda .\label{eq:harmonics} \end{equation}

	Then, for every $\tau$ in $\left(t-T,t\right]$,

\begin{equation} x\left(\tau\right) = \sum\limits_{k\in\mathbb{Z}} \left\langle x \right\rangle_k\hspace{-0.5mm} \left(t\right)\ e^{jk\omega\tau}. \label{eq:fourierSeries} \end{equation}

\end{theorem}

\textbf{Proof}. For every instant $t$, define the periodic limitation $y\left(\phi\right)$ of $x(t)$, such that

\begin{equation} y\left(\phi\right) = x\left(s + T - t\right), s\in\left[0,T\right]. \label{eq:ySubstitution} \end{equation}

        Thence the Fourier Analysis of $y$ yields

\begin{equation} y\left(s\right) = \sum\limits_{k\in\mathbb{Z}} \left\langle y \right\rangle_k\hspace{-1mm} \ e^{jk\omega s}, \label{eq:ydef} \end{equation}

where

\begin{equation} \left\langle y \right\rangle_k = \dfrac{1}{T}\int\limits_{0}^{T}y\left(u\right) e^{-jk\omega u}d u \label{eq:yfour} \end{equation}

        Manipulating \eqref{eq:ydef},

\begin{equation} y\left(s\right) = \sum\limits_{k\in\mathbb{Z}} \left[\left\langle y \right\rangle_ke^{-jk\omega\left(t-T\right)}\right] e^{jk\omega\left(s - t + T\right)} \end{equation}

        And, from the definition, of $\left\langle y \right\rangle_k$ \eqref{eq:yfour},

\begin{equation} \left\langle y \right\rangle_k e^{-jk\omega\left(t-T\right)} = \dfrac{1}{T}\int\limits_{0}^{T}y\left(u\right) e^{-jk\omega\left(u - t +T\right)}du .\end{equation}

        Using \eqref{eq:ySubstitution}, adopt $\lambda = u - t + T$:

\begin{equation} \left\langle y \right\rangle_k e^{-jk\omega\left(t-T\right)} = \dfrac{1}{T}\int\limits_{t-T}^{t}x\left(\lambda\right) e^{-jk\omega\lambda}d\lambda \label{eq:ySubs2} \end{equation}

        Then define

\begin{equation} \left\langle x \right\rangle_k \coloneqq \left\langle y \right\rangle_ke^{-jk\omega\left(t-T\right)} \label{eq:ySubs3} \end{equation}

        Hence \eqref{eq:ySubs2} and \eqref{eq:ySubs3} imply

\begin{equation} \left\langle x \right\rangle_k = \dfrac{1}{T}\int\limits_{t-T}^{t}x\left(\lambda\right) e^{-jk\omega\lambda}d\lambda \end{equation}

        And, from \eqref{eq:ydef},

\begin{equation} y\left(s\right) = \sum\limits_{k\in\mathbb{Z}} \left\langle x \right\rangle_k e^{jk\omega\left(s - t + T\right)} \end{equation}

        Finally, using $\tau = s - t + T$,

\begin{equation} x\left(\tau\right) = \sum\limits_{k\in\mathbb{Z}} \left\langle x \right\rangle_k e^{jk\omega\tau} \end{equation}

\hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
% >>>

	It is trivial to prove that the sequence of harmonics is unique, that is: two signals $y(t)$ and $x(t)$ can only share the same harmonics for all time instants if and only if $x(t)=y(t)$ for all time instants.

\begin{theorem}[Uniqueness of Fourier Dynamic Phasors]\label{theo:fdp_uniqueness}%<<<
	Let $x(t)$ and $y(t)$ be two real signals. Then $x(t) = y(t)$ for all time instants if and only if $\left\langle x \right\rangle_k\hspace{-0.5mm} \left(t\right) = \left\langle y \right\rangle_k\hspace{-0.5mm} \left(t\right)$ for all $t$ and for all $k$. \end{theorem}
\textbf{Proof:} it is easy to prove that if $x(t) = y(t)$ for all time instants $t$, then the harmonics follow:

\begin{equation}
        \left\langle x \right\rangle_k\hspace{-0.5mm} \left(t\right) - \left\langle y \right\rangle_k\hspace{-0.5mm} \left(t\right) = \dfrac{1}{T}\int\limits_{t-T}^{t} \left[x\left(\lambda\right) - y\left(\lambda\right)\right]e^{-jk\omega\lambda}d\lambda = \dfrac{1}{T}\int\limits_{t-T}^{t} \left(0\right) e^{-jk\omega\lambda}d\lambda = 0
\end{equation}

	For the other direction, suppose $x$ and $y$ share the same harmonics for all time instants $t$. Then, for all $\tau\in\left(t-T,t\right]$,

\begin{equation}
        x\left(\tau\right) - y\left(\tau\right) = \sum\limits_{k\in\mathbb{Z}} \overbrace{\left[\left\langle x \right\rangle_k\hspace{-0.5mm} \left(t\right) - \left\langle y \right\rangle_k\hspace{-0.5mm} \left(t\right)\right]}^{=0\ \forall t,k}\ e^{jk\omega\tau} = 0
\end{equation}
        \hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	Theorem \ref{theo:fdp_uniqueness} implies that the transformation is bijective; therefore it can be defined as a functional operator $\mathbf{STFT}\left[\cdot\right]$.

\begin{definition}[STFT Dynamic Phasor Transform]%<<<
	Let $x(t)\in\left[\mathbb{R}\to\mathbb{C}\right]$ be a complex signal and $T(t)$ a positive time window parameter; then its STFT Dynamic Phasor Transform over period $T$ (or over frequency $\omega = 2\pi T^{-1}$), denoted $\mathbf{STFT}\left[x\right]$, maps $x$ to its series of harmonics, that is, a series of complex signals, such that

\begin{equation}
        \mathbf{STFT}\left[\cdot\right]: \left\{\begin{array}{rcl}
	\left[\mathbb{R}\to\mathbb{C}\right] &\to& \left[\mathbb{R}\to\mathbb{C}\right]^{\left[\mathbb{Z}\right]} \\[5mm]
%
        x\left(t\right) &\mapsto& \left\{ \left\langle x \right\rangle_k\hspace{-0.5mm}\left(t\right) = \displaystyle\dfrac{1}{T}\int\limits_{t-T}^{t}x\left(\lambda\right) e^{-jk\omega\lambda}d\lambda\right\}_{k\in\mathbb{Z}}
\end{array}\right.
\end{equation}

        Conversely, the inverse transformation over $T$ takes a series of complex harmonic signals and reconstructs a time signal:

\begin{equation}
        \mathbf{STFT}^{-1}\left[\cdot\right]: \left\{\begin{array}{rcl}
	\left[\mathbb{R}\to\mathbb{C}\right]^{\left[\mathbb{Z}\right]} &\to& \left[\mathbb{R}\to\mathbb{C}\right] \\[5mm]
%
        \left\{\raisebox{4mm}{} \left\langle x \right\rangle_k\hspace{-0.5mm}\left(t\right) \right\}_{k\in\mathbb{Z}} &\mapsto& x(t) = \displaystyle\sum\limits_{k\in\mathbb{Z}} \left\langle x \right\rangle_k e^{jk\omega\tau}
\end{array}\right. \label{def:inverse_fdp}
\end{equation}

\end{definition} %>>>

%------------------------------------------------
\subsection{Operational properties of STFT Dynamic Phasors} %<<<2

	Much the same way like Static Phasors, STFT DPs inherit the same niceties and propeties of the Fourier Transform: for all integers $k$ and any complex $\alpha$, $\left\langle x + \alpha y\right\rangle_k = \left\langle x\right\rangle_k + \alpha\left\langle y\right\rangle_k$, meaning the transform is linear. For the derivative, using the multiplication rule for integrals,

\begin{align}
	\left\langle \dfrac{dx(t)}{dt}\right\rangle_k &= \int^{t}_{t-T} x'(s)e^{-jk\omega s} ds = \left[x(s) e^{-jk\omega s}\right]_{t-T}^t - \int^{t}_{t-T} x(s) \dfrac{d}{ds} e^{-jk\omega s} ds = \nonumber\\[3mm]
	&= \left[x(s) e^{-jk\omega s}\right]_{t-T}^t - \int^{t}_{t-T} x(s) \left(-jk\omega\right) e^{-jk\omega s} ds = \left[x(s) e^{-jk\omega s}\right]_{t-T}^t + jk\omega \int^{t}_{t-T} x(s) e^{-jk\omega s} \nonumber\\[3mm]
	&= \left[x(s) e^{-jk\omega s}\right]_{t-T}^t + jk\omega \left< x\right>_k \label{eq:stft_derivative_1}
\end{align}

	Now note that by Leibnitz' Rule for Integrals,

\begin{align}
	\dfrac{d \left\langle x\right\rangle_k}{dt} = \dfrac{d}{dt} \int^{t}_{t-T} x(s)e^{-jk\omega s} ds &= \left[x(s)e^{-jk\omega s}\right]_{s=t}\dfrac{dt}{dt} - \left[x(s)e^{-jk\omega s}\right]_{\left(s=t-T\right)}\dfrac{d\left(t-T\right)}{dt} \nonumber\\[3mm]
	&= \left[x(s)e^{-jk\omega s}\right]_{s=t} - \left[x(s)e^{-jk\omega s}\right]_{\left(s=t-T\right)} = \left[x(s)e^{-jk\omega s}\right]^t_{t-T} \label{eq:stft_derivative_2}
\end{align}

	Therefore join \eqref{eq:stft_derivative_1} and \eqref{eq:stft_derivative_2},

\begin{equation} \left\langle \dfrac{dx(t)}{dt}\right\rangle_k = \dfrac{d\left\langle x\right\rangle_k}{dt} + jk\omega \left\langle x\right\rangle_k \label{eq:stft_derivative_3} \end{equation}

	\noindent meaning that the harmonics of derivatives can be obtained by the harmonics themselves. Thus much the same way as the Static Fourier Phasors, these properties allow for establishing relationships between the harmonics of voltage and current of linear devices. In the STFT-DPs, however, these relationships are not exactly impedances: because the relationship of the transformation of the derive must be written for every single order $k$, each harmonic order represents its own equation. Consider a voltage $v = m_v\cos\left(\omega t + \phi_v\right)$ over a capacitor of value $C$; then

\begin{equation} i(t) = C\dfrac{dv(t)}{dt} \Rightarrow \left\langle i\right\rangle_k = C\left[ \dfrac{d\left\langle v\right\rangle_k}{dt} + jk\omega\left\langle v\right\rangle_k\right] \label{eq:fdp_capacitive_equation}\end{equation}

	Now consider a current $i = m_i\cos\left(\omega t + \phi_i\right)$ through an inductor $L$; then

\begin{equation} v(t) = L\dfrac{di(t)}{dt} \Rightarrow \left\langle v\right\rangle_k = L\left[ \dfrac{d\left\langle i\right\rangle_k}{dt} + jk\omega\left\langle i\right\rangle_k\right] \label{eq:fdp_inductive_equation} \end{equation}

	If the phasors involved are static the differentials of the harmonics are null and these equations are equivalent to $\left\langle i\right\rangle_k = jk\omega C\left\langle v\right\rangle_k$ for the capacitor and  $\left\langle v\right\rangle_k = jk\omega L\left\langle i\right\rangle_k$ for the inductor, which coincide with the impedance equations of the static phasors. However, this new frame defines differential equations, meaning that the complexification of an electrical grid yields infinitely many complex differential systems, one for each harmonic order; the process of solving an electrical grid would then become an interative process for every single order $k$:

\begin{enumerate}
	\item Substitute inductances as differential equations of the form \eqref{eq:fdp_inductive_equation}, capacitances as differential equations of the form \eqref{eq:fdp_capacitive_equation}  and resistances as impedances $R$;
	\item Write the complex differential equations of the network;
	\item Substitute voltage and current sources by their STFT-DP equivalents;
	\item In the frequency domain, solve the complex differential equations of the node voltages and branch currents obtaining their equivalent phasors;
\end{enumerate}

	After this process, a sequence of harmonics as time functions indexed by $k$ is obtained; the inverse transform as defined in \eqref{def:inverse_fdp} is applied to obtain the equivalent signals in time.

%------------------------------------------------
\subsection{Shortcomings of Short-Time Fourier Dynamic Phasors} %<<<2

%-------------------------------------------------
\subsubsection{Gabor's Inequality} %<<<3

	One of the glaring questions pertaining to the effectiveness of the STFT is the choice $T$ of the length of the window. As an example, take the signal

\begin{equation} x(t) = u(-t) \cos\left(2\pi\times 2\times 10^3 \times t\right) + u(t) \cos\left(2\pi\times 4\times 10^3 \times t\right), \label{eq:stft_signal} \end{equation}

	\noindent where $u(t)$ is the heaviside step, which is a signal comprised of a sinusoid at the frequency 2kHz for $t < 0$, but at $t = 0$ changes to 4kHz. Figure \ref{fig:stft_heatmap} shows the \textit{heatmap} of the STFT of the signal \eqref{eq:stft_signal} using two sampling frequencies, a ``high frequency'' $f_H = 1kHz$ (top plot, pertaining to a shorter window length $T_h = f_h^{-1} = 1ms$), a ``low frequency'' $f_s = 100Hz$ (bottom plot, pertaining to a longer window length $T_s = f_s^{-1} = 10ms$) and a third ideal scenario (bottom plot).

% STFT DIFFERENT FREQUENCIES COMPARISON <<<
\definecolor{cfefefe}{RGB}{254,254,254}
\definecolor{cfdfdfd}{RGB}{253,253,253}
\definecolor{cfcfcfc}{RGB}{252,252,252}
\definecolor{cfbfbfb}{RGB}{251,251,251}
\definecolor{cfafafa}{RGB}{250,250,250}
\definecolor{cf9f9f9}{RGB}{249,249,249}
\definecolor{cf8f8f8}{RGB}{248,248,248}
\definecolor{cf7f7f7}{RGB}{247,247,247}
\definecolor{whitesmoke}{RGB}{245,245,245}
\definecolor{cf3f3f3}{RGB}{243,243,243}
\definecolor{cf2f2f2}{RGB}{242,242,242}
\definecolor{ce5e5e5}{RGB}{229,229,229}
\definecolor{ce4e4e4}{RGB}{228,228,228}
\definecolor{silver}{RGB}{192,192,192}
\definecolor{cbfbfbf}{RGB}{191,191,191}
\definecolor{ccccccc}{RGB}{204,204,204}
\definecolor{grey}{RGB}{128,128,128}
\definecolor{c7f7f7f}{RGB}{127,127,127}
\definecolor{ccccccc}{RGB}{204,204,204}
\definecolor{ccbcbcb}{RGB}{203,203,203}
\definecolor{ce3e3e3}{RGB}{227,227,227}
\definecolor{cf1f1f1}{RGB}{241,241,241}
\definecolor{cf6f6f6}{RGB}{246,246,246}
\definecolor{cf7f7f7}{RGB}{247,247,247}
\definecolor{ce4e4e4}{RGB}{228,228,228}
\definecolor{cf2f2f2}{RGB}{242,242,242}
\definecolor{cf4f4f4}{RGB}{244,244,244}
\begin{figure}[htb!]
\centering
\scalebox{0.975}{
	\input{data/stft/100}
}
	\caption
[Heatmap of the STFT transform of the example signal at two sampling frequencies and the ``ideal scenario''.]
{Heatmap of the STFT transform of signal \eqref{eq:stft_signal} at two sampling frequencies of $f_H = 1kHz$ (top plot) and at $f_L = 100Hz$ (middle plot). Bottom plot shows an ``ideal scenario'' composed of initely fine, instantly changing lines.}
	\label{fig:stft_heatmap}
\end{figure} %>>>

	The heatmap is a color plot where the absolute value of the STFT is shown in time in the band of frequencies; darker colors mean a higher absolute value. Thus it shows how the frequency spectrum is concentrated energy-wise as time passes. Reestated, taking a vertical line at $t = t_0$, each vertical point in the line shows the energy distribution along the frequency axis at the particular time instant $t_0$. Conversely, taking a horizontal line at $f = f_0$, each point in the horizontal line shows the time distribution of that particular frequency, that is, the evolution of the contribution of that particular frequency to the signal energy spectrum.

	At a first glance, because the signal is monotonic at a frequency of $2kHz$ at $t<0$ and at $4kHz$ at $t > 0$, one intuitively expects the STFT to show a harmonic concentrated at $2kHz$ for $t<0$ and $4kHz$ for $t\geq 0$. The heatmap would start as an infinitely thin black line at 2kHz, which at t = 0 immediately changes to another infinitely thin line at 4kHz; this is the ideal case depicted in figure \ref{fig:stft_heatmap}. Nevertheless such is not the case when the STFT is computed; as Figure \ref{fig:stft_heatmap} shows, at the higher sampling frequency, the heatmap is scattered vertically, but the horizontal scatter is small, that is, the frequency lines are thick and exists over a wide band of frequencies but the transition around t = 0 is quick. However, at the slower sampling frequency, the vertical scatter is small, but the horizontal scatter is large, that is, the the frequency lines are thinner but they linger for very long.

	In essence, what is happening is that the higher sampling frequency has a shorter window length; thus it can accurately detect the time intervals when frequency swings happen. However, since the time window is shorter, the window stretches high and varies too quickly, multiplying the signal and capturing multiple frequency bands; therefore, while there is precision in estimating the time instants where frequency swings happen, the amplitude of these frequency swings is not so well captured. The inverse happens when a shorter window length is used. This phenomenon is known as Gabor's Inequality.

\begin{theorem}[Gabor's Inequality or the Fundamental Principle of Communication \pcite{Gabor1946TheoryOC}]\label{theo:principle_comm} %<<<

	Let $\psi(t)\in\left[\mathbb{R}\to\mathbb{C}\right]$ some square-integrable complex signal, $\phi(\omega) = \mathbf{F}\left[\psi\right]$ its Fourier Transform. Define the quantities

\begin{equation} \omega^* = \sqrt{ \dfrac{\displaystyle\int \overline{\phi(\omega)} \omega^2 \phi(\omega) d\omega}{\displaystyle\int\overline{\phi(\omega)}\phi(\omega) d\omega }} = \sqrt{\dfrac{\displaystyle\int \overline{\dfrac{d\psi(t)}{dt}} \dfrac{d\psi(t)}{dt} dt}{\displaystyle\int \psi(t)\overline{\psi(t)}dt} } \end{equation}

	\noindent called \textbf{effective frequency} and

\begin{equation} t^* = \sqrt{\dfrac{\displaystyle\int \overline{\psi(t)}t^2\psi(t) dt}{\displaystyle\int \psi(t)\overline{\psi(t)}dt}} \end{equation}

	\noindent called the \textbf{mean epoch}, and the deviations

\begin{equation} \left\{\begin{array}{l} \Delta t = \sqrt{2\pi\left\langle\left(t - t^*\right)^2\right\rangle} \\[3mm] \Delta \omega = \sqrt{2\pi\left\langle\left(\omega - \omega^*\right)^2\right\rangle}\end{array}\right.  \end{equation}

	\noindent as the \textbf{effective duration} and \textbf{effective frequency width}, where $\left\langle\cdot\right\rangle$ represents average value. In short, these deviations are the mean RMS average of the signal with respect to the mean epoch, and the mean RMS average deviation of the signal spectrum with respect to the effective frequency, multiplied by $\sqrt{2\pi}$. Then

\begin{equation} \Delta t\ \Delta \omega \geq \pi . \label{eq:gabor_inequality}\end{equation}

\hrule
\end{theorem}
\vspace{5mm}
%>>>

	In short, for an arbitrary signal, the ``variation in frequency swings'' and the ``variation in interval'' that these frequency swings occur are closely related; reducing one means enlarging the other, and vice-versa. Further, \cite{Gabor1946TheoryOC} asks what is the signal that achieves the minimum value, that is, the signal that satisfies $\Delta t\Delta \omega = \pi$. That signal is

\begin{equation} \psi(t) = e^{-\alpha^2\left(t - t_0\right)^2}e^{j\left(\omega_0 t + \phi\right)} \label{eq:optimal_signal}\end{equation}

	\noindent where $\alpha,\omega,\phi$ are fixed, called \textbf{Gabor's Wavelet}. The Fourier Transform of this signal is

\begin{equation} \mathbf{F}\left[\psi\right] = \phi\left(\omega\right) = e^{-\left(\frac{2}{\alpha}\right)^2 \left(\omega - \omega_0\right)^2}e^{\left[-t_0\left(\omega - \omega_0\right) + \phi\right]} \label{eq:optimal_signal_fourier}\end{equation}

	Thus the shapes of the signal amplitude and its spectrum are gaussian curves, with $t_0$and $\omega_0$ means as shown in figure \ref{fig:gabor_minimum_signal}. The mean epoch and effective frequency are

\begin{equation} \Delta t = \sqrt{\dfrac{\pi}{2}}\dfrac{1}{\alpha},\ \Delta\omega = \sqrt{2\pi} \alpha \end{equation}

	\noindent and, indeed, one notices that this signal achieves the equality of \eqref{eq:gabor_inequality}.

% MINIMUM SIGNAL CURVES <<<
\begin{figure}[htb]
\centering
	\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
		\begin{axis}[
			at={(0mm,0mm)},
			width= 75mm,
			height=75mm,
			tick style={draw=none},
			axis lines=middle,
			xmin = -0.2, xmax = 2.2,
			ymin = -0.2, ymax = 1.1,
 			xticklabel={\empty}, yticklabel={\empty},
			xlabel = {$t$},
			ylabel = {$\left\lvert\psi(t)\right\rvert$}
			]
			\addplot  [thick, stewartblue, domain=0:2, smooth, samples=100] {e^(-16*(x-1)^2)};
			\addplot  [thick, stewartblue, domain=0:2, dashed, line cap=round, samples=100] coordinates {(1,0)(1,1)};

			\addplot  [thick, black, dashed, line cap=round, samples=100] coordinates {(   1+(1/8)     ,  { e^(-16*( 1 + (1/8) - 1)^2) + 0.05})( 1 + (1/8) ,1.09)};
			\addplot  [thick, black, dashed, line cap=round, samples=100] coordinates {(   1-(1/8)     ,  { e^(-16*( 1 - (1/8) - 1)^2) + 0.05})( 1 - (1/8) ,1.09)};
			\draw[<-] (axis cs: {1 + 1/8} ,1.04) -- (axis cs: {1 + 1/8 + 0.1} ,1.04);
			\draw[<-] (axis cs: {1 - 1/8} ,1.04) -- (axis cs: {1 - 1/8 - 0.1} ,1.04);
			\node at(axis cs:1,1.05) {$\Delta t$};
			\node at(axis cs:1,-0.1) {$t_0$};
		\end{axis}
		\begin{axis}[
			at={(75mm,0mm)},
			width= 90mm,
			height=75mm,
			tick style={draw=none},
			axis lines=middle,
			xmin = -0.2, xmax = 3.2,
			ymin = -0.2, ymax = 1.1,
 			xticklabel={\empty}, yticklabel={\empty},
			xlabel = {$\omega$},
			ylabel = {$\left\lvert\phi(\omega)\right\rvert$}
			]
			\addplot  [thick, stewartgreen, domain=0:3, smooth, samples=100] {0.8*e^(-4*(x-1.2)^2)};
			\addplot  [thick, black, dashed, line cap=round, samples=100] coordinates {(   1.2+(0.25)/(0.8^2)     ,  { 0.8*e^(-4*( 1.2 + (0.25)/(0.8^2) - 1.2)^2) + 0.05})( 1.2 + (0.25)/(0.8^2) ,1.05)};
			\addplot  [thick, black, dashed, line cap=round, samples=100] coordinates {(   1.2-(0.25)/(0.8^2)     ,  { 0.8*e^(-4*( 1.2 - (0.25)/(0.8^2) - 1.2)^2) + 0.05})( 1.2 - (0.25)/(0.8^2) ,1.05)};
			\draw[<->] (axis cs: {1.2 + (0.25)/(0.8^2)} ,1) -- (axis cs: {1.2 - (0.25)/(0.8^2)} ,1);
			\addplot  [thick, stewartgreen, line cap=round, domain=0:2, dashed, samples=100] coordinates {(1.2,0)(1.2,0.8)};
			\node at(axis cs:1.2,-0.1) {$\omega_0$};
			\node at(axis cs:1.2,1.05) {$\Delta\omega$};
		\end{axis}
	\end{tikzpicture}
	\caption
[Amplitude and spectrum of the ``optimal signal'' \eqref{eq:optimal_signal}.]
{Amplitude and spectrum of the ``optimal signal'' \eqref{eq:optimal_signal} showing the mean epoch and effective frequency as well as the mean deviations.}
	\label{fig:gabor_minimum_signal}
\end{figure} %>>>

	Due to this statistical property of the wavelet, it can work as a generator for a kernel called Gabor Kernel using a reference version of \eqref{eq:optimal_signal} where $\alpha = \sqrt{\pi}$ and $\phi = 0$:

\begin{equation} \mu_G\left(s, t,\omega\right) = e^{-\pi\left(s - t_0\right)^2}e^{j\omega_0 s} \label{eq:gabor_kernel}\end{equation}

	\noindent incepting the Gabor Transform, defined as

\begin{equation} \mathbf{G}\left[x\right] = X\left(t,\omega\right) = \int_\mathbb{R} x(s)\overline{\mu_G\left(s, t,\omega\right)}ds = \int_\mathbb{R} x(s)e^{-\pi\left(s - t\right)^2}e^{-j\omega s}ds \label{eq:gabor_transform} ,\end{equation}

	\noindent which one can recognize as a STFT \eqref{eq:stft_generic} with a gaussian window. This transform has been used extensively in the literature, for instance in Power Quality assessment \pcite{szmajdaGaborTransformGaborWigner2010}, detection of radar signals \pcite{shu-longjiDetectionRadarSignals1992}, representation of time systems \pcite{rotsteinGaborTransformTimevarying1999} and image processing \pcite{jieyaoGeneralizedGaborTransform1995}.

	Thus, in short, it is the nature of the Fourier Transform process — which is obviously underlying to the STFT decomposition — that there is a tradeoff between the ``frequency resolution'' and the ``time resolution'' acquired from the transform. This happens even in ``simple signals'', like that of \eqref{eq:stft_signal}, and it means that this transform is inexorably inefficient in capturing transient nonstationary phenomena in various timescales: a great many details are needed to find a particular window length (or frequency signal) that adheres to a satisfactory compromise between both. Even when the optimal windowing is used, in the for of Gabor's Transform, the issue persists. This is a particular problem for Power Systems, because transient effects in such systems can manifest in various bandwidths and timescales, meaning that the choice of a particular frequency signal means certain phenomena will probably be neglected for precision in time resolution in a particular frequency band.

%-------------------------------------------------
\subsubsection{Infinite complex systems}\label{subsec:infinite_complex_systems} %<<<2

	Another downside of the STFT framework is that the voltage-current relationships it implies define infinite complex differential systems, one for each harmonic; this can be seen as both a downside and a benefit — the capability of modelling harmonics in realtime, while being useful for the analysis of power quality during operation of electrical grids, makes it impractical to model the grid in simulations. It is immediate to notice that this process represents too much a sophistication to be considered practical: the solution of infinite complex differential systems is most certainly computationally impossible. In order to solve this, the simplifying hypotheses that the signals involved are mainly concentrated in the first harmonic, or fundamental, is made; under this assumption all higher-order harmonics can be ignored for practical purposes, summarizing the analysis to a single differential system. To this regard, the accuracy of the transform is be sacrificed, as the higher harmonics supposed innocuous and only the fundamental harmonic dominating as per equations \eqref{sys:fdp_sys_fundamental_harmonic}. Thus the voltage-current relationships become

\begin{equation} \left\{\begin{array}{l} I = C\dfrac{dV}{dt} + j\omega CV \\[5mm] V = L\dfrac{dI}{dt} + j\omega LI \end{array}\right. \label{sys:fdp_sys_fundamental_harmonic} \end{equation}

	\noindent where the capital $V$ and $I$ are notations for the fundamental harmonics $\left\langle v\right\rangle_1$ and $\left\langle i\right\rangle_1$, producing the ``approximated'' Dynamic Phasors of voltage and current $V$ and $I$, which will be called the ``STFT Dynamic Phasors'' or simply STFT-DPs. This process then abdicates precision for conveniency, that is, is gives approximations to the solution of the time Differential Equations to the Electrical Grid being modelled; inasmuch as they allow traditional phasorial representation, the phasors they represent guaranteed not to mirror signals in time, but only approximately — underwhelmingly so, for while Static Phasors can be proven to perfectly reconstruct the stable steady-state solution of the time ODEs of the grid. 

	It is natural to ask what is the effectiveness of this approximation — how close the signals reconstructed from STFT-DPs are from the signals that solve the time differential equations of the grid. In \cite{volpatoDynamicPhasorTransform2022}, I and Prof. Luís prove that this approximation is valid under the Quasi-Stationary Hypothesis, that is, if the magnitude and frequency signals are ``slow'', the signals reconstructed from these phasors approximate the solution of these ODEs. The proof uses a concept of bandwidth of the modulus and phase angle; once these bandwidths get smaller and the signals get ``slower'', the Dynamic Phasor of the original signal $x$ gets arbitrarily close to an ``averaged'' signal $x_A$ which amplitude is the average amplitude of $x(t)$ and which phase is the average phase. This proof is shown in theorem \ref{theo:fdp_quasi_static}. Further, in \cite{volpatoDynamicPhasorTransform2022} we then prove that this implies that $x$ gets arbitrarily close to the static phasor that represents the solutions to the original time differential equations of the system.

\begin{theorem}[Quasi-static modelling of STFT-DPs \pcite{volpatoDynamicPhasorTransform2022}]\label{theo:fdp_quasi_static}%<<<
Let 

\begin{equation} x(t) = m(t)\cos\left(\omega(t) t + \phi(t)\right) \end{equation}

	\noindent be a nonstationary sinusoid where $m$ and $\phi$ are $C^1$-class. Let the approximated Dynamic Phasor $x_A$ be calculated as

\begin{equation} x_A(t) = \sum_{k\in\left\{-1,1\right\}} \dfrac{1}{2} m_A (t) e^{j\phi_A(t)} e^{jk\omega t} = m_A(t)\cos\left(\omega(t) t + \phi_A(t)\right), \end{equation}

	\noindent where $m_A$ and $\phi_A$ are the averaged approximations of the modulus and phase angle during $\left[t-T,t\right]$:

\begin{equation}
\left\{\begin{array}{l}
	m_A(t) = \dfrac{1}{T} \displaystyle\int_{t-T}^t m(\tau)d\tau \\[5mm]
	\phi_A(t) = \dfrac{1}{T} \displaystyle\int_{t-T}^t \phi(\tau)d\tau
\end{array}\right. .
\end{equation}

	Then 

\begin{equation} \left\lvert x(t) - x_A(t) \right\lvert  < \dfrac{4\pi \left(B_m + B_\phi \left\lvert m_A\right\rvert\right)}{\omega} \left(1 + 4\pi \dfrac{B_\phi}{\omega}\right) \left(1 + \dfrac{\pi^2}{3}\right), \end{equation}

	where $B_m$ and $B_\phi$ are the bandwidths of the modulus and phase angle signals defined as


\begin{equation} B_z = \sup_{\left[t-T,t\right]} \left\lvert \dfrac{dz(t)}{dt}\right\rvert \end{equation}
\end{theorem}
\hrule
\vspace{5mm}
%>>>

%-------------------------------------------------
\subsubsection{Power signals} %<<<2

	Finally, the Fourier Dynamic Phasors are unable to give an alternative to theorem \ref{theo:sfp_complex_apparent_power}, that is, the Dynamic Phasors are unable to prove that the complex power induced by their inner product does not reflect the instantaneous power developed by the circuit being studied. Indeed, if $\left(\left\langle v\right\rangle_k\right)_{k\in\mathbb{Z}}$ and $\left(\left\langle i\right\rangle_k\right)_{k\in\mathbb{Z}}$ represent the time-varying harmonics of voltage and current, then the harmonics of the power (their product) are calculated by a convolution

\begin{equation} \left\langle p\right\rangle_k = \sum_{m\in\mathbb{Z}} \left\langle v\right\rangle_m\left\langle i\right\rangle_{(k-m)} \label{eq:stft_power_def}\end{equation}

	\noindent and it is obvious that extracting components like active and reactive power from that is not possible. Using the single-harmonic approximation, that is, supposing  $\left\lvert\left\langle x\right\rangle_k\right\rvert \ll \left\lvert\left\langle x\right\rangle_1\right\rvert$ for both $v$ and $i$, then using the approximations on \eqref{eq:stft_power_def} yields that for the k-th power harmonic, 

\begin{equation} \left\lvert\left\langle p\right\rangle_k\right\rvert \ll \left\lvert\left\langle p\right\rangle_1\right\rvert \text{ and } \left\langle p\right\rangle_1 \approx \left\langle v\right\rangle_1\left\langle i\right\rangle_1 . \end{equation}

	\noindent and one can define 

\begin{equation} S = \frac{1}{2}\left\langle p\right\rangle_1,\ P = \text{Re}\left(S\right),\ Q = \text{Im}\left(S\right) .\end{equation}

	However, it is obvious that the first-harmonic approximation is particularly problematic here, because the errors commited by approximating both current and voltage propagate throughout all harmonics of $p$. This is particularly grievous in Power Systems because virtually all modern electric power systems are equipped with active and reactive power control units, like Droop control or Maximum Power Tracking Point algorithms, meaning complex power must be a direct reflection of the actual instantaneous power otherwise these power controls are not guaranteed to work properly.

	If another window that is not the boxcar window is used, then the situation becomes more difficult. Supposing a continuous arbitrary window, then the convolution becomes the integral on the frequency space of the Dynamic Phasors $V\left(t,\omega\right)$  for voltage and $I\left(t,\omega\right)$ for current:

\begin{equation} P\left(t,\omega\right) = \int_{\mathbb{R}} V\left(t,\omega - \kappa\right) I\left(t,\kappa\right) d\kappa ,\label{eq:stft_continuous_power_def}\end{equation}

	\noindent making the process of obtaining an expression for $P$ more difficult, let alone even extracting the active and reactive components. As discussed in the introduction of this thesis, the literature features several attempts to solve this conundrum, most of which involve quite contrived representations of complex power as higher complex algebras or employing distortion as a component of power — none of which theories have been been quite adopted in the literature in the literature.

%--------------------------------------------------------------------------------------------------
\section{Representation of sinusoidal signals using the Hilbert Transform}\label{chapter:hilbert_transform} %<<<1
%--------------------------------------------------------------------------------------------------

%-------------------------------------------------
\subsection{Analytical Representation of real signals} %<<<2

%	In Complex Analysis, an analytic complex function $f(z)$ at some particular open set $D$ is that which for every $z_0\in D$, $f(z)$ is locally given by some convergent power series, that is, there exists a complex sequence $\left(a_k\right)_{k\in\mathbb{N}}$ such that
%
%\begin{equation} f(z) = \sum_{k\in\mathbb{N}} a_k\left(z-z_0\right)^k .\end{equation}
%
%	It is a fundamental result of Complex Analysis that any complex-differentiable (holomorphic) function in an open set $D$ is analytic and vice-versa; moreover, another big result is that a complex-differentiable function in an open set is also infinitely differentiable. As such, in the space of complex functions, holomorphicity and analicity are equivalent.
%
%\begin{definition}[Holomorphic complex functions] %<<<
%	For an arbitrary $f\in\left[\mathbb{C}\to\mathbb{C}\right]$, the following definitions are equivalent:
%
%\begin{itemize}
%	\item $f$ is analytic in an open set $D$, that is, for every $z_0\in D$ there exists a convergent power series for $f$;
%	\item $f$ is holomorphic in $D$, that is, it is infinitely differentiable in a vicinity of any $z_0\in D$;
%	\item $f$ and its derivatives are polynomically limited, that is, for every compact $K\subset D$ there exists some constant $k$ such that for any $z_0\in K$ and $i\in\mathbb{N}$,
%	\begin{equation} \left\lvert \dfrac{d^if}{dz^i}\left(z_0\right)\right\rvert \leq i!k^i \end{equation}
%\end{itemize}
%\end{definition} %>>>
%
%	It is clear from the definition that analicity is a very strong condition for a complex function, giving it many properties.

	It is known, and simple to inspect, that the Fourier Transform of an arbitrary real signal $x(t)$ has negative frequency components \pcite{smithMathematicsDiscreteFourier2007} that are symmetric with respect to conjugation, that is, the Fourier Transform $X\left(\omega\right) = \mathbf{F}\left[x\right]$ is Hermitian symmetric:

\begin{equation} X\left(-\omega\right) = \overline{X\left(\omega\right)} \end{equation}

	\noindent meaning that the negative frequency parts can be safely discarded because they can be reconstructed from the positive frequency components. Thus a signal with only positive frequency spectrum can be built as $S\left(\omega\right)$:

\begin{equation} S\left(\omega\right) = X\left(\omega\right) \left[ 1 + \text{sgn}\left(\omega\right)\right] = \left\{\begin{array}{l} 2X\left(\omega\right), \text{ if } \omega > 0 \\[3mm] X\left(\omega\right), \text{ if } \omega = 0 \\[3mm] 0, \text{ if } \omega < 0 \\[3mm]\end{array}\right. \end{equation}

	\noindent which contains only the positive frequency components of $X\left(\omega\right)$. The operation is revertible because $X\left(\omega\right)$ can be obtained from $S\left(\omega\right)$ through

\begin{equation} X\left(\omega\right) = \left\{\begin{array}{l} \frac{1}{2} S\left(\omega\right), \text{ if } \omega > 0 \\[3mm] S\left(\omega\right), \text{ if } \omega = 0 \\[3mm] \frac{1}{2}\overline{ S\left(-\omega\right)}, \text{ if } \omega < 0 \\[3mm]\end{array}\right. = \dfrac{ S\left(\omega\right) + \overline{S\left(-\omega\right)} }{2} .\end{equation}

	Naturally one wonders what signal does $S\left(\omega\right)$ reconstruct. This signal is

\begin{align} \mathbf{F}^{-1}\left[S\right] &= \mathbf{F}^{-1}\left[X\left(\omega\right) + X\left(\omega\right) \text{sgn}\left(\omega\right)\right] = \mathbf{F}^{-1}\left[X\right] + \mathbf{F}^{-1}\left[X\left(\omega\right) \text{sgn}\left(\omega\right)\right] = \nonumber\\[3mm] &= x(t) + \overbrace{\mathbf{F}^{-1}\left[X\right] * \mathbf{F}^{-1}\left[\text{sgn}\left(\omega\right)\right]}^{\text{Convolution}} = x(t) + x(t) * \left(j\dfrac{1}{\pi t} \right) = x(t) + j \left[x(t) * \dfrac{1}{\pi t}\right]\end{align}

	\noindent and by definition this convolution is given by

\begin{equation} x(t) * \dfrac{1}{\pi t} = \dfrac{1}{\pi}\int_\mathbb{R} \dfrac{x\left(\tau\right)}{t-\tau} d\tau \label{eq:hilb_transf_def}\end{equation}

	\noindent which is the naïve definition of the \textbf{Hilbert Transform} of the signal $x(t)$, denoted $\mathbf{H}\left[x\right]$. Therefore, the \textbf{Analytic Representation} of $x(t)$, defined as the signal reconstructed from $S\left(\omega\right)$ is given by

\begin{equation} x_a(t) = x(t) + j\mathbf{H}\left[x\right] .\end{equation}

	This process is known as \textbf{Hilbert Filtering} , that is, ``removing'' the negative frequencies. The objective now becomes to show that this representation yields desirable properties; most importantly, it allows for easily obtaining modulation and de-modulation techniques.

	It is simple to see that this process yields some idea of a time-varying phasor; for instance, one can adopt $m_x(t) = \left\lvert x_a(t)\right\rvert$ as the time-varying amplitude of $x(t)$, also called an \textbf{envelope}. Further, one can adopt $\phi_x(t) = \arg\left(x_a(t)\right)$ as the time-varying phase and $\omega_x = \dot{\phi_x}$ as the equivalent time-varying frequency in radians or $f_x = \omega_a/2\pi$ as its value in hertz.

\begin{example}[Analytical signal of $e^{-x^2}\cos\left(20\pi x\right)$]\label{example:hilbert_analytical}

	The analytical signal $x_a(t)$ corresponding to

\begin{equation} x(t) = e^{-t^2}\cos\left(2\pi\times 10\times t\right) \end{equation}

	\noindent is such that $x_a = x(t) + j\mathbf{H}\left[x\right] = e^{-t^2}e^{j2\pi\times 10\times t}$, which is a Gabor Wavelet. The plots of $x(t)$, $\left\lvert x_a\right\rvert$ and $f_a = \omega_a/2\pi$ are shown in Figure \ref{fig:analytical_example}.

% HILBERT TRANSFORM ENVELOPE <<<
\begin{figure}[htb]
\centering
	\begin{tikzpicture}
	\begin{axis}[
		at = {(0,65mm)},
		width=150mm,
		height=75mm,
		%tick style={draw=none},
		axis lines=middle,
		xtick = {-4,-6,...,4},
		ytick = {-1,1},
		x tick style={color=black},
		y tick style={color=black},
		xmin = -4.2, xmax = 4.2,
		ymin = -1.1, ymax = 1.1,
		xlabel = {$t\ \left(s\right)$},
		%xlabel style = {at={(axis cs:50,0.5)},anchor=north},
		ylabel = {$x(t) = e^{-t^2}\cos\left(2\pi\times 2\times t\right)$},
		ylabel style = {at={(axis cs:0.2,1)},anchor=west},
		trig format=rad
		]
		%\addplot  [thick, stewartblue, domain=-0.25:7, smooth, samples=100] {0.1*((0.9*x - 1.4)^3 - 3*(x - 1.4)^2 - 0.4*(x-1.4) + 15)};
		\addplot[color=stewartblue, domain=-4:4, samples=2000, smooth, thick] {exp(-x^2)*cos(2*pi*2*x)};
	\end{axis}

	\begin{axis}[
		at = {(0,0)},
		width=150mm,
		height=75mm,
		%tick style={draw=none},
		axis lines=middle,
		xtick = {-4,-6,...,4},
		ytick = {1},
		x tick style={color=black},
		y tick style={color=black},
		xmin = -4.2, xmax = 4.2,
		ymin = -0.2, ymax = 1.1,
		xlabel = {$t\ \left(s\right)$},
		%xlabel style = {at={(axis cs:50,0.5)},anchor=north},
		ylabel = {$\left\lvert x_a(t)\right\rvert$},
		ylabel style = {at={(axis cs:0.2,1)},anchor=west},
		trig format=rad
		]
		\addplot[color=stewartgreen,smooth, samples=1000, thick] {exp(-x^2)};
	\end{axis}
	\begin{axis}[
		at = {(0,-65mm)},
		width=150mm,
		height=75mm,
		%tick style={draw=none},
		axis lines=middle,
		xtick = {-4,-6,...,4},
		ytick = {0,0.5,...,2},
		x tick style={color=black},
		y tick style={color=black},
		xmin = -4.2, xmax = 4.2,
		ymin = 0, ymax = 2.2,
		xlabel = {$t\ \left(s\right)$},
		%xlabel style = {at={(axis cs:50,0.5)},anchor=north},
		ylabel = {$f_a (Hz)$},
		ylabel style = {at={(axis cs:0.1,2.1)},anchor=west}
		]
		\addplot[color=stewartpink,smooth, thick] {2};
	\end{axis}
	\end{tikzpicture}
	\caption{Example of signal and its analytic correspondent.}
	\label{fig:analytical_example}
\end{figure} %>>>

\examplebar
\end{example}

%-------------------------------------------------
\subsection{The Cauchy Principal Value} %<<<2

	The main operational problem with the Hilbert Transform, as defined in \eqref{eq:hilb_transf_def}, is that for limited signals its integral is not defined because the denominator has a singularity at $t=\tau$. This makes computing the transform of the simplest signals like polynomials and sinusoids impossible. In order to circumvent that, the Cauchy Principal Value, or simply principal value, is used. This is mathematical tool in Theory of Singularities that allows assigning some value to certain improper integrals that would otherwise remain undefined. The definition of the principal value depends on the particular function it is applied to: let $f(x)\in\left[\mathbb{R}\to\mathbb{C}\right]$ a complex function such that $b$ is a singularity of $f$, that is, $f(b)$ is not defined. Suppose $b$ is some finite real number and consider the interval $\left[a,c\right]$ containing $b$, where

\begin{equation} \lim\limits_{\varepsilon\to 0^+} \int_{a}^{b-\varepsilon} f(x)dx = \pm \infty\end{equation}

	\noindent and

\begin{equation} \lim\limits_{\varepsilon\to 0^+} \int_{b+\varepsilon}^{c} f(x)dx = \mp \infty,\end{equation}

	\noindent which is to say that the improper integrals from $a$ to $b$ and from $b$ to $c$ diverge with opposite signs. Because of this, it is obvious that the integral

\begin{equation} \int_{a}^{c} f(x)dx \label{eq:hilbert_transform_impossible_integral} \end{equation}

	\noindent cannot be defined because of the difficult point $b$. Then the Cauchy Principal Value of $f$ on $\left[a,c\right]$ is denoted as a ``dashed integral'' and defined as

\begin{equation} \dashint_{a}^{c} f(x)dx = \lim\limits_{\varepsilon\to 0^+} \left[\int_{a}^{b-\varepsilon} f(x)dx + \int_{b+\varepsilon}^{c} f(x)dx  \right]\end{equation}

	\noindent which is a way of assigning some value to the divergent integral \eqref{eq:hilbert_transform_impossible_integral}. To integrate $f$ over the reals, then

\begin{equation} \dashint_{\mathbb{R}} f(x)dx = \dashint_{-\infty}^{\infty} f(x)dx = \lim\limits_{\varepsilon\to 0^+} \left[ \lim\limits_{a\to -\infty} \int_{a}^{b-\varepsilon} f(x)dx + \lim\limits_{c\to\infty} \int_{b+\varepsilon}^{c} f(x)dx  \right]\end{equation}

	For instance, let $f(x) = 1/\left(x-b\right)$, where $b$ is some real: famously, the integral of this function on any interval containing $b$ cannot be defined due to the divergent nature at $b$. Taking the principal value, for any real or infinite $a$, 

\begin{equation} \dashint_{-a}^{a} \dfrac{1}{x-b}dx = 0. \end{equation}

	Now suppose that the singularity of $f$ is located at infinity; then

\begin{equation} \dashint_{-\infty}^{\infty} f(x)dx = \lim\limits_{a\to\infty} \left[\int_{-a}^{a} f(x)dx \right], \end{equation}

	\noindent where

\begin{equation} \lim\limits_{a\to\infty} \int_{-a}^{0} f(x)dx = \pm \infty\end{equation}

	\noindent and

\begin{equation} \lim\limits_{a\to\infty} \int_{0}^{a} f(x)dx = \mp \infty .\end{equation}

	For instance, integrating the sine function over the reals yields an undefined improper integral because the function is underfined at infinity. Applying the principal value yields

\begin{equation} \dashint_{-\infty}^{\infty} \sin\left(x\right) dx = \lim\limits_{a\to\infty} \left[\int_{-a}^{a} \sin\left(x\right) dx \right] = 0 . \end{equation}

	Finally, in the more difficult case where $f$ has a singularity at both infinity and a finite point $b$, then the definition needs to consider the particular point $b$ and the singularity at infinity:

\begin{equation} \dashint_{\mathbb{R}} f(x)dx = \lim\limits_{a\to\infty} \left\{ \lim\limits_{\varepsilon\to 0^+} \left[\int_{b-a}^{b-\varepsilon} f(x)dx + \int_{b+\varepsilon}^{b+a} f(x)dx \right] \right\}, \end{equation}

	\noindent which is the more general definition.

\begin{example}[CPV of the Sinc function] %<<<
	Take the sinc function

\begin{equation} f\left(x\right) = \dfrac{\sin\left(x\right)}{x}\end{equation}

	\noindent for some real number $b$. Then

\begin{equation} \dashint_{\mathbb{R}} \dfrac{\sin\left(x\right)}{x}dx = \lim\limits_{a\to\infty} \left\{ \lim\limits_{\varepsilon\to 0^+} \left[\int_{-a}^{-\varepsilon} \dfrac{\sin\left(x\right)}{x}dx + \int_{\varepsilon}^{a} \dfrac{\sin\left(x\right)}{x}dx \right] \right\} \end{equation}

	Because both sine and $1/x$ are odd functions, the sinc function is even, meaning

\begin{equation} \dashint_{\mathbb{R}} \dfrac{\sin\left(x\right)}{x}dx = 2\lim\limits_{a\to\infty} \left\{ \lim\limits_{\varepsilon\to 0^+} \left[ \int_{\varepsilon}^{a} \dfrac{\sin\left(x\right)}{x}dx \right] \right\} \end{equation}

	Now, because

\begin{equation} \lim\limits_{x\to 0} \dfrac{\sin\left(x\right)}{x}dx = 1, \end{equation}

	\noindent then the limit on $\varepsilon$ can be removed:

\begin{equation} \dashint_{\mathbb{R}} \dfrac{\sin\left(x\right)}{x}dx = 2\lim\limits_{a\to\infty} \int_{0}^{a} \dfrac{\sin\left(x\right)}{x}dx  \end{equation}

	Despite this antiderivative function having no analytic expression, it is known that its improper version converges and is equal to

\begin{equation} \lim\limits_{a\to\infty} \int_{0}^{a} \dfrac{\sin\left(x\right)}{x}dx = \dfrac{\pi}{2} \end{equation}

	\noindent meaning

\begin{equation} \dashint_{\mathbb{R}} \dfrac{\sin\left(x\right)}{x}dx = \pi. \end{equation}

\examplebar
\end{example} %>>>

	Finally, the formal definition of the Cauchy Principal Value induces the definition of the Hilbert Transform.

\begin{definition}[Hilbert Transform] \label{def:hilbert_transform} %<<<

	The Hilbert Transform of a complex function $x\in\left[\mathbb{R}\to\mathbb{C}\right]$ is defined as the principal value of the convolution of $x$ with the function $1/\pi t$, called the Cauchy Kernel:

\begin{equation} \mathbf{H}\left[x\right] = \dfrac{1}{\pi}\hspace{2mm} \dashint_{\mathbb{R}} \dfrac{x\left(\tau\right)}{t - \tau} d\tau \end{equation}

	\noindent provided that this integral exists as a principal value.

\end{definition} 
%>>>

%-------------------------------------------------
\subsection{Properties and application to signals of interest} %<<<2

	The operational properties of the Hilbert Transform are immediate from its definitions. For instance, its linearity: 

\begin{equation} \mathbf{H}\left[x + \alpha y\right] = \dfrac{1}{\pi}\hspace{2mm} \dashint_{\mathbb{R}} \dfrac{x\left(\tau\right) + \alpha y\left(\tau\right)}{t - \tau} d\tau \end{equation}

	\noindent because the limits of the definition \ref{def:hilbert_transform} are linear, as wel as the integral, this means that the principal value is linear:

\begin{align}
	\mathbf{H}\left[x + \alpha y\right] &= \dfrac{1}{\pi} \left[\hspace{2mm}\dashint_{\mathbb{R}} \dfrac{x\left(\tau\right)}{t - \tau} d\tau + \dashint_{\mathbb{R}} \dfrac{\alpha y\left(\tau\right)}{t - \tau} d\tau\right] = \nonumber\\[3mm]
	&= \dfrac{1}{\pi} \left[\hspace{2mm}\dashint_{\mathbb{R}} \dfrac{x\left(\tau\right)}{t - \tau} d\tau\right]  + \dfrac{1}{\pi}\left[\hspace{2mm}\dashint_{\mathbb{R}} \dfrac{\alpha y\left(\tau\right)}{t - \tau} d\tau\right] = \nonumber\\[3mm]
	&= \dfrac{1}{\pi} \left[\hspace{2mm}\dashint_{\mathbb{R}} \dfrac{x\left(\tau\right)}{t - \tau} d\tau\right]  + \dfrac{1}{\pi}\alpha\left[\hspace{2mm}\dashint_{\mathbb{R}} \dfrac{y\left(\tau\right)}{t - \tau} d\tau\right] = \nonumber\\[3mm]
	&= \mathbf{H}\left[x\right] + \alpha\mathbf{H}\left[y\right]
\end{align}

	As for derivatives, we smartly swap the convolution variables

\begin{equation} \dfrac{d}{dt}\mathbf{H}\left[x\right] = \dfrac{d}{dt} \left[\hspace{2mm}\dashint_{\mathbb{R}} \dfrac{x\left(t-\tau\right)}{\tau} d\tau \right] \end{equation}

	\noindent and using Leibnitz' Rule for integrals, since the integrand is on $\tau$ and not on $t$,

\begin{equation} \dfrac{d}{dt}\mathbf{H}\left[x\right] = \dashint_{\mathbb{R}} \dfrac{d}{dt} \left[\dfrac{x\left(t-\tau\right)}{\tau} \right]d\tau =  \dashint_{\mathbb{R}} \dfrac{x'\left(t-\tau\right)}{\tau} d\tau = \mathbf{H}\left[\dfrac{dx}{dt}\right] \label{eq:hilvert_diff}\end{equation}

	\noindent which is to say that the differential functional and the Hilbert Transform commute: $\mathbf{D}_\mathbb{C}\circ\mathbf{H} = \mathbf{H}\circ\mathbf{D}_\mathbb{C}$.

	In the representation of phasorial quantities, the most important property of the Hilbert transform is the fact that the transform of the exponential complex function yields a quadrature signal:

\begin{equation}
	 \mathbf{H}\left[e^{j\omega t} \right] = \left\{\begin{array}{l} e^{j\left(\omega t - \frac{\pi}{2}\right)}, \text{ if } \omega > 0 \\[5mm] e^{j\left(\omega t + \frac{\pi}{2}\right) } , \text{ if } \omega < 0 \end{array}\right.
\end{equation}


	\noindent which in turn means that the transform of a sine or cosine yields a quadrature sine or cosine, that is,

\begin{gather}
	 \mathbf{H}\left[\cos\left(\omega t + \phi\right)\right] = \left\{\begin{array}{l} \cos\left(\omega t + \phi - \dfrac{\pi}{2} \right) = \sin\left(\omega t + \phi\right), \text{ if } \omega > 0 \\[5mm] \cos\left(\omega t + \phi + \dfrac{\pi}{2} \right) = -\sin\left(\omega t + \phi\right), \text{ if } \omega < 0 \end{array}\right. \\[10mm]
%
	 \mathbf{H}\left[\sin\left(\omega t + \phi\right)\right] = \left\{\begin{array}{l} \sin\left(\omega t + \phi - \dfrac{\pi}{2} \right) = -\cos\left(\omega t + \phi\right), \text{ if } \omega > 0 \\[5mm] \sin\left(\omega t + \phi + \dfrac{\pi}{2} \right) = \cos\left(\omega t + \phi\right), \text{ if } \omega < 0  \end{array}\right.
\end{gather}

	\noindent for a positive $\omega$. This property allows, in turn, the definition of a ``quadrature signal'' of periodic functions; let $f(t)$ be a function of period $T$. Taking the Fourier Series of $f$ yields

\begin{equation} f(t) = \sum_{k\in\mathbb{Z}} a_k e^{jk\omega t}, a_k = \dfrac{1}{T} \int_{T} f(x)e^{-jk\omega x}{dx}\end{equation}

	Therefore

\begin{equation}  \mathbf{H}\left[ f(t) \right] = \mathbf{H}\left[ \sum_{k\in\mathbb{Z}} a_k e^{jk\omega t}\right] = \sum_{k\in\mathbb{Z}} a_k e^{j\left(k\omega - \sign\left(k\right)\frac{\pi}{2}\right)} = -\sum_{k\in\mathbb{Z}}\sign\left(k\right) a_k j e^{jk\omega} \end{equation}

	In the specific realm of Dynamic Phasor Theory, the most explored property of the Hilbert Transform is the Bedrosian Identity, as presented in theorem \ref{theo:extended_bedrosian}.

\begin{definition}[Support of a complex function]
	Let $f\in\left[\mathbb{R}\supseteq X\to \mathbb{C}\right]$; then the \textbf{support} of $f$, denoted $\supp\left(f\right)$, is the closure of its complementary pre-image of zero, that is,

\begin{equation} \supp\left(f\right) = \overline{\left\{x\in X: f\left(x\right)\neq 0\right\}} \end{equation}

	\noindent where the overline represents the closure of a set.

\end{definition}
\begin{theorem}[Extended Bedrosian Identity \pcite{Xu2006}]\label{theo:extended_bedrosian} %<<<
	Let $a\leq 0$, $b \geq 0$ and $f,g\in L^2\left(\mathbb{R}\right)$ such that

\begin{equation} \supp\left(\mathbf{F}\left[f\right]\right)\subset \left[a,b\right] \text{ and } \supp\left(\mathbf{F}\left[g\right]\right)\subset \left(-\infty,b\right)\cup \left(a,\infty\right) \label{eq:bedrosian_condition}\end{equation}

	\noindent where $\mathbf{F}\left[\cdot\right]$ represents the Fourier Transform. Then $f$ and $g$ satisfy

\begin{equation} \mathbf{H}\left[f(t)g(t)\right] = f(t)\mathbf{H}\left[g(t)\right] \end{equation}
\end{theorem}
\hrule
\vspace{5mm}
%>>>

	The Bedrosian Identity defines that when two functions $f$ and $g$ satisfy \eqref{eq:bedrosian_condition}, then their multiplication is such that, then the ``slow'' $f(t)$ can be cast out of the Hilbert operator while the ``faster'' component $g(t)$ is kept inside, greatly simplifying the transformation process. Intuitively, \eqref{eq:bedrosian_condition} means that $f(t)$ is ``slower'' than $g(t)$ in the sense that the spectrum of $g$ ``envelopes'' that of $f$, since it is composed of higher harmonics.

	\cite{Xu2006} then show that this property is especially useful in the research and study of Power Systems because it allows representing phasorial signals of interest, for instance, phase signals where the amplitude varies or there is a rapid increase in frequency. For instance, suppose a signal $x(t) = m(t)e^{j\left(\omega t + \phi\right)}$, where $m(t)$ is slower than $\omega$, that is,

\begin{equation} \supp\left(\mathbf{F}\left[m\right]\right) \subset \left(-\omega,\omega\right) \end{equation}

	\noindent then

\begin{equation} \mathbf{H}\left[m(t)\cos\left(\omega t + \phi\right)\right] = m(t) \mathbf{H}\left[\cos\left(\omega t + \phi\right)\right] = m(t)e^{j\left(\omega t + \phi\right)} \end{equation}

	Much alike, \cite{derviskadicPhasorsModelingPower2020} shows that the Bedrosian Identity can be used to produce Dynamic Phasors for certain signals of interest in Power Systems. For instance, consider

\begin{equation} x(t) = M \cos \left(\omega t + \phi + R t^2\right), \label{eq:ramping_coefficient_signal}\end{equation}

	\noindent where $M$ is a constant amplitude and $R$ is a frequency ramping coefficient, modelling an unstable frequency growing linearly in time. Then

\begin{equation} \mathbf{H}\left[M \cos\left(\omega t + \phi + Rt^2\right)\right] = M \mathbf{H}\left[\cos\left(\omega t + \phi + Rt^2\right)\right] = Me^{j\left(\omega t + \phi + Rt^2\right)} .\end{equation}

	Also consider the signal

\begin{equation} x(t) = M_0\left[1 + k\theta(t)\right]\cos \left(\omega t + \phi\right), \end{equation}

	\noindent where $\theta(t)$ is the Heaviside step, modelling a sudden change in amplitude. Then

\begin{equation} \mathbf{H}\left[x\right] = M_0\left[1 + k\theta(t)\right] e^{j\left(\omega t + \phi\right)}. \label{eq:amplitude_delta_signal}\end{equation}

%-------------------------------------------------
\subsection{Shortcomings of the Hilbert Transform} %<<<2

%-------------------------------------------------
\subsubsection{Representation of signals in time}

	The matter of fact is that, while powerful the Hilbert Transform fails in the most basic of tasks sought, since not all signals in time can be easily represented, only those that adhere to the Bedrosian Identity. Reestated, the capacity of the HT to produce easily representable complex function relies on a very specific nature of the signals being considered, meaning only a certain class of signals can be contemplated. Further, being an integral transform, it relies on the fact that the only real signals applicable are those that have a ``nice'' (as in, analytically representable) transform.

	For instance, \eqref{eq:ramping_coefficient_signal} models a signal with a linearly rampant frequency while \eqref{eq:amplitude_delta_signal} models a signal with amplitude variation. These are clearly simplifications of certain transient phenomena which, in practicality, are much more sophisticated.

%-------------------------------------------------
\subsubsection{Differentials}

	While the SFTF is able to produce complex differential systems that are somehow simulatable and indeed present numerical benefits, such is not the case with the Hilbert Transform. For instance, given some linear system

\begin{equation} \sum_{k=0}^n \alpha_k x^{(k)} - f(t) = 0.\label{eq:hilbert_original_system}\end{equation}

	Apply the HT to this equation

\begin{equation} \mathbf{H}\left[\sum_{k=0}^n \alpha_k x^{(k)}\right] - \mathbf{H}\left[f\right] = 0 \end{equation}

	\noindent and using the HT's linearity,

\begin{equation} \sum_{k=0}^n \alpha_k \mathbf{H}\left[x^{(k)}\right] - \mathbf{H}\left[f\right] = 0 .\end{equation}

	Now using the HT differentiation property \eqref{eq:hilvert_diff}, 

\begin{equation} \sum_{k=0}^n \alpha_k \left(\mathbf{H}\left[x\right]\right)^{(k)} - \mathbf{H}\left[f\right] = 0 \label{eq:hilbert_transf_system}\end{equation}

	\noindent thus summing up both equations yields

\begin{equation} \eqref{eq:hilbert_original_system} + j\times\eqref{eq:hilbert_transf_system}: \sum_{k=0}^n \alpha_k x_a^{(k)} - f_a = 0\end{equation}

	\noindent where $x_a,f_a$ are the analytic signals of $x(t)$ and $f(t)$. This resulting equation is the exact same differential equation as the original, presenting no particular benefits on the application of the HT to linear systems. One might even argue that the resulting equation is even more difficult to solve than the original, because it has an added dimension. In short, the Hilbert Transform is unable to transform linear differential equations into complex (``phasorial'') equivalents that present modelling or numerical benefits over the original equations of the system, defeating the purpose of transforms in the first place.

%-------------------------------------------------
\subsubsection{Power signals}

	Finally, the Hilbert Transform is unable to represent power signals. \cite{derviskadicPhasorsModelingPower2020} cites that the HT is able to produce some notion of complex power, by the following construction. Let $v(t),i(t)$ the voltage across and current through a bipole, and denote their analytic signals as $\hat{v} = v(t) + j\mathbf{H}\left[v\right]$ and $\hat{i} = i(t) + j\mathbf{H}\left[i\right]$. Then consider the quantities

\begin{equation}\left\{\begin{array}{l} p_1(t) = \hat{v}\hat{i} = v(t)i(t) - \mathbf{H}\left[v\right]\mathbf{H}\left[i\right] + j\left( \mathbf{H}\left[v\right]i(t) - v(t)\mathbf{H}\left[i\right]\right) \\[3mm] p_2(t) = \hat{v}\overline{\hat{i}} = v(t)i(t) + \mathbf{H}\left[v\right]\mathbf{H}\left[i\right] + j\left( \mathbf{H}\left[v\right]i(t) - v(t)\mathbf{H}\left[i\right]\right)\end{array}\right. .\end{equation}

	Then the sum of $p_1$ and $p_2$ yields

\begin{equation} p_3(t) = p_1(t) + p_2(t) = 2v(t)i(t) + j2\mathbf{H}\left[v\right]i(t) .\end{equation}

	\noindent so that the real part of $p_3(t)$ is twice the instantaneous power $p(t)$, while the imaginary part does not have any specific meaning and is cited as a ``modelling artifact''. Thus, while the Hilbert Transform can produce \textit{some} notion of complex power, it can only reconstruct the instantaneous power but cannot produce solid notions of active and reactive power.

%-------------------------------------------------
\section{Proposed Dynamic Phasors Theory} \label{sec:proposed_dptheory} %<<<1

	It becomes now clear that the current techniques fail at some point:

\begin{itemize}
	\item The STFT does produce differential complex systems that have \textit{some} accuracy in representing signals in time (depending on how ``slow'' the signals are, as per theorem \ref{theo:fdp_quasi_static}), it requires approximations to do so, and has a particular problem when expressing power signals;
	\item The Hilbert Transform can represent \textit{some} signals of interest, but it does not produce convenient differential models and does not represent power signals in time.
\end{itemize}

	The first path to filling the gaps of these current techniques is to adopt a proper representation of the signals involved. Inspired by the ``simple'' PLL subsystem of Figure \ref{fig:pll_example} and by the IEEE Standard C37.118.1-2011 for Synchrophasor Measurements for Power Systems \pcite{IEEEStandardSynchrophasor2011}, the following representation is proposed: instead of considering signals that can be expressed by \eqref{eq:dynamic_sinusoid_example} where the time-varying frequency multiplies time, let us consider signals of the form $x(t) = m(t)\cos\left(\theta(t)\right)$ where the angle $\theta(t)$ can be written as the sum of a time-varying phase and the integral of the time-varying frequency frequency. Reestated, $x(t)$ is such that, for some time-varying frequency $\omega(t)$ chosen, there is a solution $\phi(t)$ to \eqref{eq:apparent_angle_def}, called the \textbf{apparent phase} of $x(t)$ with respect to $\omega(t)$.

\begin{equation} \theta(t) = \psi(t) + \phi(t),\ \psi(t) = \int_{t_0}^t \omega(s)ds . \label{eq:apparent_angle_def}\end{equation}

	With this representation in mind, we rewrite definition \ref{def:sinusoid} to a more precise version. Definition \ref{def:sinusoid_dynamic} describes a \textbf{generalized sinusoid}, or simply \textbf{sinusoid}, as a signal that has a ``sinewave shape'' with time-varying amplitude and frequency, such that the absolute angle can be broken down into an accumulated angle $\psi(t)$ and a time-varying phase $\phi(t)$ as per \eqref{eq:apparent_angle_def}.

\begin{definition}\label{def:sinusoid_dynamic}%<<<
	A \textbf{generalized sinusoid} or simply sinusoid is a $x(t)\in\left[\mathbb{R}\to\mathbb{R}\right]$ if there are two real signals called \textbf{amplitude} $m(t)$ and \textbf{absolute angle} $\theta(t)$ such that $x(t) = m(t)\cos\left(\theta(t)\right)$. Further, given some apparent frequency $\omega(t)$, $x(t)$ is a \textbf{generalized sinusoid at the apparent frequency $\boldsymbol{\omega}(t)$} if \eqref{eq:apparent_angle_def} has a solution $\phi(t)$ called the \textbf{apparent phase}.
\end{definition} %>>>

	In this definition, the \textbf{absolute angle} of $x(t)$, $\theta(t)$, is the ``whole angle'' as measured by the measuring device, with $\omega(t)$, called the \textbf{apparent frequency} the notion of the time-varying frequency, $\phi(t)$ the \textbf{apparent phase} the notion of time-varying phase and $\psi(t)$ is the angle accumulated by $\omega(t)$ from some initial time $t_0$, most probably $t_0 = 0$. In the case of a PLL, $\omega(t)$ is given by a feedback loop (as will be shown later); in case of the transform proposed here, $\omega(t)$ is supposed arbitrary in principle, and more requirements will be added later. We further divide \textbf{generalized sinusoids} into two categories: \textbf{static or stationary sinusoids} if $m(t)$ and $\dot{\theta}(t)$ are constant, and \textbf{nonstationary sinusoids} if either or both $m(t)$ and $\dot{\theta}(t)$ are time-varying. Notably, in a stationary sinusoidal case, $m,\omega$ and $\phi$ are constant, so $\psi(t) = \omega t$. For cleanliness of the test, we shorten the terminology and refer to generalized sinusoidas as simply ``sinusoids'', even though classically the word means only the static ones.

	The question on the nature of generalized sinusoids and the feasibility of such representation is discussed thoroughly on section \ref{sec:discussion_proposed_representation}. For now it suffices to say that when we assume a signal admits a sinusoidal representation we will say so explicitly as in "\textbf{assume $x(t)$ has a sinusoidal representation}", however weak this assumption is.

\begin{definition}[Admissibility of a sinusoidal representation] A signal $x(t)\in\left[\mathbb{R}\to\mathbb{R}\right]$ \textbf{admits a sinusoidal representation} if there exist functions $m(t),\ \theta(t)$ such that $x(t) = m(t)\cos\left(\theta(t)\right)$. Additionally, $x(t)$ admits a sinusoidal representation \textbf{at the frequency $\omega(t)$} if there exists a solution $\phi$ to $\phi(t) = \theta(t) - \psi(t),\ \psi(t) = \int_0^t \omega(s)ds$.

	Equivalently, $x(t)$ admits a sinusoidal representation if there exists a function $f(t)\in\left[\mathbb{R}\to\mathbb{C}\right]$ such that $x(t) = \Re\left[f(t)\right]$. The signal $x(t)$ then admits a representation at $\omega(t)$ if $f(t)$ is such that there exists a solution $\phi$ to $\phi(t) = \arg\left[f(t)\right] - \psi(t)$.
\end{definition}

%-------------------------------------------------
\subsection{Construction of the Dynamic Phasor Transform} %<<<2

	One of the issues with the current literature is the fact that the representation of a signal $x(t) = m(t)\cos\left(\psi(t) + \phi(t)\right)$ as a time-varying complex function $X(t) = m(t)e^{j\phi(t)}$ is assumed but the exact process by which this representation is constructed is not given. For instance, the IEEE Standard C37.118.1-2011 states that the synchrophasor \eqref{eq:synchrophasor_complex} represents \eqref{eq:synchrophasor_time}; yet this affirmation is only a representation and the exact mechanics by which one quantity is constructed from the other is not shown.

	 Such construction is proposed as follows. First we note that the core of the PLL is a two-fold process: a ``$\alpha\beta$'' transform followed by a ``dq'' transform, the latter dependent on some frequency signal $\omega(t)$ supplied by some control. If $x(t)$ is a single-phase quantity, we assume that it assumes a sinusoidal representation, as discussed thoroughly in subsection \ref{subsec:comments_def_sin}. What is generally called the $\alpha\beta$ transform is in fact the transformation of the input signal $x(t)$ into a generator function $f(t) = x_\alpha(t) + jx_\beta(t)$, that is, $x(t)$ is represented by two components $x_\alpha$ and $x_\beta$ such that $x_\alpha$ is in phase with $x(t)$ and $x_\beta$ is in quadrature:

\begin{equation} \mathbf{x}_{\alpha\beta} = \left[\begin{array}{c} x_\alpha(t) \\[3mm] x_\beta(t) \end{array}\right].\end{equation}

	If $x(t)$ admits a sinusoidal representation and the amplitude $m(t)$ and the argument $\theta(t)$ of the generator function $f(t) = m(t)e^{j\theta(t)}$ are known, then $x_\alpha$ and $x_\beta$ are intuitively obtained as

\begin{equation} \mathbf{x}_{\alpha\beta} = \left[\begin{array}{c} x_\alpha(t) \\[3mm] x_\beta(t) \end{array}\right] = m(t)\left[\begin{array}{c} \cos\left(\theta\right) \\[3mm] \sin\left(\theta\right) \end{array}\right] .\end{equation}

% PLL SUBSTYSTEM <<<
\begin{figure} 
\centering
\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]

\node [draw, minimum width=2cm, very thick, minimum height=2cm, left=0] (tab_block) {};

\draw (tab_block.south west) -- (tab_block.north east);

\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]tab_block.north west) {\large $t$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]tab_block.south east) {\large $\alpha\beta$};

\node at ([shift=({-15mm,0})]tab_block.west) (signalinput) {$x(t)$};
\draw[->] ([shift=({4mm,0})]signalinput.center) -- ([shift=({-1mm,0})]tab_block.west);

\node [draw, minimum width=2cm, very thick, minimum height=2cm, right=2cm of tab_block] (abdq_block) {};
\draw (abdq_block.south west) -- (abdq_block.north east);
\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]abdq_block.north west) {\large $\alpha\beta$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]abdq_block.south east) {\large $dq$};

\draw[->] ([shift=({0, 3.33mm})]tab_block.east) -- ([shift=({-1mm, 3.33mm})]abdq_block.west) node[midway, above] {$x_\alpha(t)$};
\draw[->] ([shift=({0,-3.33mm})]tab_block.east) -- ([shift=({-1mm,-3.33mm})]abdq_block.west) node[midway, below] {$x_\beta(t)$};

\draw[->] ([shift=({0, 3.33mm})]abdq_block.east) -- ([shift=({10mm, 3.33mm})]abdq_block.east) node[right] {$x_d(t)$};
\draw[->] ([shift=({0,-3.33mm})]abdq_block.east) -- ([shift=({10mm,-3.33mm})]abdq_block.east) node[right] {$x_q(t)$};

\node [draw, minimum width=1cm, very thick, minimum height=1cm, below=1cm of abdq_block] (omega_integrator) {$\int$};

\node [below=1cm of omega_integrator.south] (omega_signal) {$\omega(t)$};

\draw[->] (omega_signal.north) -- ([shift=({0,-1mm})]omega_integrator.south);
\draw[->] (omega_integrator.north) -- ([shift=({0,-1mm})]abdq_block.south) node [midway, right] {$\psi(t)$};

\end{tikzpicture}
\caption{Example PLL block for inspiration of the Differential Dynamic Phasors.}
\label{fig:pll_example}
\end{figure}
%>>>
More deeply, this process is justified because if $x(t)$ admits a sinusoidal representation it is, in essence, a two-dimensional signal: it depends on an amplitude signal and an angle signal. This is akin to the fact that the Static Phasor Operator transforms a one-dimensional static sinusoid into a complex number, which is two-dimensional. Ultimately, given $m(t)$ and $\theta(t)$, no information is gained or lost due to the $\alpha\beta$ transformation.

	Naturally, this transform is linear and invertible: given the two-dimensional vector $\left[x_\alpha(t),x_\beta(t)\right]^\transpose$, then this vector is naturally diffeomorphic to a complex number $x_\alpha + jx_\beta$, thus

\begin{equation} x(t) = x_\alpha(t) = m_x(t)\cos\left(\theta_x\right) \text{, where } m_x(t) = \left\lvert x_\alpha + jx_\beta\right\rvert \text{ and } \theta_x = \arg\left(x_\alpha + jx_\beta\right) .\end{equation}

	Therefore this transform is also invertible. Further, since complex addition is linear, then this $\alpha\beta$ transform, as well as its inverse, are also invertible. Then, rotate the vector $\mathbf{x}_{\alpha\beta}$ by a rotational transformation

\begin{equation} \mathbf{T}_\psi = \left[\begin{array}{cc} \cos\left(\psi\right) & \sin\left(\psi\right) \\[3mm] -\sin\left(\psi\right) & \cos\left(\psi\right)\end{array}\right],\ \mathbf{T}_\psi^{-1} = \left[\begin{array}{cc} \cos\left(\psi\right) & -\sin\left(\psi\right) \\[3mm] \sin\left(\psi\right) & \cos\left(\psi\right)\end{array}\right]  \end{equation}

	\noindent resulting in

\begin{equation} \mathbf{x}_{dq} = \left[\begin{array}{c} x_d(t)\\[3mm] x_q(t) \end{array}\right] = \mathbf{T}_\psi\mathbf{x}_{\alpha\beta} = m(t)\left[\begin{array}{c} \cos\left(\phi\right) \\[3mm] \sin\left(\phi\right) \end{array}\right] \end{equation}

	\noindent where $\phi(t)$ is the solution to \eqref{eq:apparent_angle_def} . The ``dq'' notation is directly inherited from the Power System literature: the ``d'' component stands for \textit{direct} (in phase) axis and the ``q'' component for \textit{quadrature}, and the terminology will be explained later. Naturally, since $\mathbf{x}_{\alpha\beta}$ can be represented as a complex number, so can $\mathbf{x}_{dq}$ be represented by the number $x_d + jx_q$. Quickly one recognizes $\mathbf{T}_\psi$ is a rotation matrix at the angle $-\psi(t)$; as such, its inverse $\mathbf{T}_\psi^{-1}$ is the rotation matrix at the angle $\psi(t)$, and this inverse always exists because the determinant of $\mathbf{T}_\psi$ is always unitary independently of $\psi(t)$. This means that the entire process is bijective and unique: given the frequency signal $\omega(t)$, $\mathbf{x}_{dq}$ reconstructs $x(t)$ and vice-versa biunvocally.

	We now want to show that the ``dq'' transformed quantity $\mathbf{x}_{dq}$ is equivalent (infinitely diffeomorphic, in fact) to a function the complex plane, which we will call the Dynamic Phasor representation. A couple hints at this fact are that it is obvious that if the aplitude $m(t)$ and the angle $\theta(t)$ of the signal \eqref{eq:apparent_angle_def} are known (therefore so are the $x_\alpha$ and $x_\beta$ components), then one can define the time-varying complex function $X_p(t)$ given by

\begin{equation} X_p(t) = x_\alpha(t) + jx_\beta(t) = m(t)e^{j\theta(t)} . \label{eq:xab_xp}\end{equation}

	Because the linear transform $\mathbf{T}_\psi$ is a rotation matrix at the angle $-\psi(t)$, applying it to  $\mathbf{x}_{\alpha\beta}$ means that the complex equivalent $x_p(t)$ is rotated by $e^{-j\psi(t)}$, yielding

\begin{equation} X(t) = m(t)e^{j\theta(t)} e^{-j\psi(t)}= m(t)e^{j\left(\theta(t) - \psi(t)\right)} = m(t)e^{j\phi(t)} \label{eq:xcomp_xdq}\end{equation}

	\noindent which is exactly $X(t) = x_d(t) + jx_q(t)$. In order to prove this line of thought, we define a \textit{complexification operator} that transforms a two-dimensional real function (that is, any $\mathbf{x}\in\left[\mathbb{R}\to\mathbb{R}^2\right]$) into a complex function. The theorem proves that this transform is not only invertible, but that itself and its inverse are linear and infinitely differentiable in the space $\left[\mathbb{R}\to\mathbb{R}^2\right]$.

\begin{theorem}[$dq$ and complex space equivalence]\label{theo:rho_diff_inf} %<<<
	Consider a function $\mathbf{x} = \left[u(t),v(t)\right]^\intercal\in\left[\mathbb{R}\to\mathbb{R}\right]^2$, and let $\rho$ denote a complex equivalence functional mapping given by

\begin{equation} \rho: \left\{\begin{array}{rcl} \left[\mathbb{R}\to\mathbb{R}\right]^2 &\to& \left[\mathbb{R}\to\mathbb{C}\right] \\[3mm] \mathbf{x} &\mapsto& \left[1,j\right]\mathbf{x} \end{array}\right. \end{equation}

	\noindent that is, $\rho$ takes a two-dimensional real function of a single real variable $\mathbf{x}(t)$ and delivers a complex function $X(t) = u(t) + jv(t)$. The inverse transform is given by

\begin{equation} \rho^{-1}: \left\{\begin{array}{rcl} \left[\mathbb{R}\to\mathbb{C}\right] &\to& \left[\mathbb{R}\to\mathbb{R}\right]^2 \\[3mm] X(t) &\mapsto& \left[\begin{array}{c} \Re\left[X\left(t\right)\right]\\[3mm] \Im\left[X\left(t\right)\right]\end{array}\right] \end{array}\right. \end{equation}

	Then $\rho$ is a canonic infinite diffeomorphism, that is: it is unique (except for some complex scaling), infinitely differentiable, bijective and the inverse is also infinitely differentiable. 
\end{theorem}

\textbf{Proof:} because $\rho$ is a function of a function — called a \textit{functional} — the derivative used is the Fréchet Derivative as defined in \eqref{eq:def_frechet}. The functional derivative of $\rho$ at $\mathbf{x}$ is defined as the bounded linear map $\mathbf{A}\left[\mathbf{x}\right]$ that satisfies

\begin{equation} \lim\limits_{\left\lVert \Delta\mathbf{x}\right\rVert\to 0} \dfrac{\left\lVert \rho\left[\mathbf{x} + \Delta\mathbf{x}\right] - \rho\left[\mathbf{x}\right] - \mathbf{A}\left[\mathbf{x}\right]\Delta\mathbf{x}\right\rVert}{\left\lVert \Delta\mathbf{x}\right\rVert} = 0\end{equation}

	\noindent where $\mathbf{A}\left[\mathbf{x}\right]\Delta\mathbf{x}$ denotes $\mathbf{A}$ calculated at $\mathbf{x}$ applied onto a $\Delta\mathbf{x}$, and $\Delta\mathbf{x}\in\left[\mathbb{R}\to\mathbb{R}\right]^2$ is small enough so the limit exists.  It is clear that $\mathbf{A}\left[\mathbf{x}\right]\Delta\mathbf{x} = \left[1,j\right]\Delta\mathbf{x}$ for any $\mathbf{x}$:

\begin{equation} \left[1,j\right]\left(\mathbf{x} + \Delta\mathbf{x}\right) - \left[1,j\right]\mathbf{x} - \left[1,j\right]\Delta\mathbf{x} = 0 . \end{equation}

	Therefore $\rho$ is a linear functional as its derivative is constant. Most importantly, however, is that the differential of $\rho$ calculated at any $\mathbf{x}$ is equal to $\rho$ itself; these fact mean that $\rho$ is infinitely differentiable and all subsequent differentials will be equal to $\rho$ — all higher-order derivatives $\delta^n \rho\left[\mathbf{x}\right]$, calculated at any $\mathbf{x}$ where they exist, will be identical to $\rho$ itself.

	As for the inverse $\rho^{-1}$, because real and imaginary parts are also infinitely differentiable \pcite{ahlfors1979complex}, infinite differentiability of $\rho^{-1}$ is easy to prove: the differential of $\rho^{-1}$ at $X$ applied on a $\Delta X$ is calculated as

\begin{equation} \dfrac{\delta\left(\rho^{-1}\right)}{\delta X} \Delta X = \left[\begin{array}{c} \Re\left[\Delta X(t)\right] \\[5mm] \Im\left[\Delta X(t)\right] \end{array}\right] .\end{equation}

	Indeed,

\begin{equation} \left[\begin{array}{c} \Re\left[X(t) + \Delta X(t)\right] \\[5mm] \Im\left[X(t) + \Delta X(t)\right] \end{array}\right] - \left[\begin{array}{c} \Re\left[X(t)\right] \\[5mm] \Im\left[X(t)\right] \end{array}\right] - \left[\begin{array}{c} \Re\left[\Delta X(t)\right] \\[5mm] \Im\left[\Delta X(t)\right] \end{array}\right] = \left[\begin{array}{c} 0 \\[5mm] 0 \end{array}\right]  \end{equation}

	And it is easy to see that the subsequent differentials $\delta^n\left(\rho^{-1}\right)\left[X\right]$ will be identical. It is now only left to prove that $\rho$ is unique apart from a scalar multiplication. Take some non-zero real $\alpha$. Then $\rho_\alpha$ can be defined as

\begin{equation} \rho_\alpha\left[\mathbf{x}\right] \mathbf{y}(t) = \alpha\left[1,j\right]\mathbf{y} \text{ and } \rho^{-1}_\alpha\left[X\right] Y= \dfrac{1}{\alpha}\left[\begin{array}{c} \Re\left(Y\right)\\[3mm] \Im\left(Y\right)\end{array}\right]. \end{equation}

	\noindent which are also infinitely diffeomorphic. Adopt the \textit{canonic} transformation as the version of $\alpha = 1$. \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	The existence of a (quite a mouthful) canonic infinite diffeomorphism between $\mathbf{x} = \left[u(t),v(t)\right]^\transpose$ and $X(t) = u(t) + jv(t)$ means that these entities are effectively one the same, but represented in two different topological spaces; because of this, the notation $\mathbf{x} \simeq X$ will be used. This can be read as \textit{$\mathbf{x}$ and $X$ are equivalent}, or that \textit{$X$ is the complex version (or equivalent) of $\mathbf{x}$}. Also, the $\rho$ operator will thenceforth be called the \textit{complex equivalence operator} or simply \textit{complexification}.

	This functional mapping $\rho$ then justifies the bijection between the $\alpha\beta$ transform of a sinusoid and the complex number $x_\alpha(t) + jx_\beta(t)$, as per \eqref{eq:xab_xp}, and the bijection between $\mathbf{x}_{dq}$ and $X(t) = x_d(t) + jx_q(t)$ as per \eqref{eq:xcomp_xdq}. Using the tandem process comprised of the $\alpha\beta$ transform, followed by the $dq$ transform at some frequency signal $\omega(t)$ and then the complexification $\rho$, one achieves a Dynamic Phasor Transform (DPT), that is, a bijection between a sinusoid $x(t)$ and a time-varying complex function $X(t)$, as in Figure \ref{fig:complexification_process_dps}.

% COMPLEXIFICATION IN THE DP DOMAIN <<<
\begin{figure}[htb]
\centering
	\scalebox{0.75}{
	\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=2.5mm,angle'=50]}]
	%\node[left] (xinput) at (0,0) {$m(t)\cos\left(\psi(t) + \phi(t)\right)$};
	\node[left] (xinput) at (0,0) {$x(t)$};
	\node [draw, minimum width=15mm, very thick, minimum height=15mm, right=20mm of xinput]   (ab_block)  {$\alpha\beta$};
	\node [draw, minimum width=15mm, very thick, minimum height=15mm, right=20mm of ab_block] (dq_block)  {$\mathbf{T}_\psi$};
	\node [draw, minimum width=15mm, very thick, minimum height=15mm, right=20mm of dq_block] (rho_block) {$\rho$};
	%\node [right, right=20mm of rho_block] (output) {$X(t) = x_d + jx_q = m(t)e^{j\phi(t)}$};
	\node [right, right=20mm of rho_block] (output) {$X(t)$};

	\draw [->] (xinput.east) -- ([shift=({-1mm,0})]ab_block.west); 
	\draw [->] (ab_block.east) -- ([shift=({-1mm,0})]dq_block.west) node[midway,above] {$\mathbf{x}_{\alpha\beta}$}; 
	\draw [->] (dq_block.east) -- ([shift=({-1mm,0})]rho_block.west) node[midway,above] {$\mathbf{x}_{dq}$}; 
	\draw [->] (rho_block.east) -- (output.west); 

	\node [draw, minimum width=15mm, very thick, minimum height=15mm, above=15mm of dq_block] (int_block) {$\int$};

	\node[above] (omegat) at ([shift=({0,20mm})]int_block) {$\omega(t)$} ;
	\draw [->] (omegat) -- ([shift=({0,1mm})]int_block.north);
	\draw [->] (int_block.south) -- ([shift=({0,1mm})]dq_block.north) node[near start,right] {$\psi(t)$};

	\node [draw, rounded corners, stewartblue, very  thick, dashed, minimum width=10cm, minimum height=3cm] at (dq_block.center)  (invol) {};

	\node [draw, minimum width=15mm, very thick, stewartblue, minimum height=15mm, below=60mm of dq_block] (ps_block) {$\mathbf{P_D^\omega}$};

	%\node[left] (xinput_ps) at ([shift=({-20mm,0})]ps_block.west) {$m(t)\cos\left(\psi(t) + \phi(t)\right)$};
	\node[left] (xinput_ps) at ([shift=({-20mm,0})]ps_block.west) {$x(t)$};
	\draw [->] (xinput_ps) -- ([shift=({-1mm,0})]ps_block.west) ;
	%\node[right] (output_ps) at ([shift=({20mm,0})]ps_block.east) {$X(t) = x_d + jx_q = m(t)e^{j\phi(t)}$};
	\node[right] (output_ps) at ([shift=({20mm,0})]ps_block.east) {$X(t)$};
	\draw [->] (ps_block.east) -- ([shift=({-1mm,0})]output_ps.west) ;

	\node [draw, minimum width=15mm, very thick, minimum height=15mm, above=15mm of ps_block] (intps_block) {$\int$};
	\node[above] (omegatps) at ([shift=({0,20mm})]intps_block) {$\omega(t)$} ;
	\draw [->] (omegatps) -- ([shift=({0,1mm})]intps_block.north);
	\draw [->] (intps_block.south) -- ([shift=({0,1mm})]ps_block.north) node[midway,right] {$\psi(t)$};

	\draw [dashed, very thick, stewartblue]  (invol.south east) -- (ps_block.north east);
	\draw [dashed, very thick, stewartblue]  (invol.south west) -- (ps_block.north west);
	\end{tikzpicture}
	}
	\caption{The process of \textit{complexification} of a sinusoid into a complex Dynamic Phasor.}
	\label{fig:complexification_process_dps}
\end{figure} %>>>

	Notably, this process is invertible and diffeomorphic: from a complex function $X(t)$ one uses the inverse $\rho^{-1}$ yielding two $dq$ components; then, these are transformed to $\alpha\beta$ quantities using $\mathbf{T}^{-1}_{\psi(t)}$, and the first component of the $\alpha\beta$ vector is taken to deliver $x(t)$. Therefore, we can define the proposed Dynamic Phasor Transform.

\begin{definition}[Dynamic Phasor Transform (DPT)]\label{def:dptransform}
	Consider a frequency signal $\omega(t)$. The Dynamic Phasor Transform at $\omega$, denoted $\mathbf{P_D}^\omega\left[x\right]$ is defined as 

\begin{equation} \mathbf{P_D^{\omega}}: \left\{\begin{array}{rcl} \left[\mathbb{R}\to\mathbb{R}\right] &\to& \left[\mathbb{R}\to\mathbb{C}\right] \\[3mm] m(t)\cos\left(\psi(t) + \phi(t)\right) &\mapsto& X(t) = m(t)e^{j\phi(t)} \end{array}\right. \end{equation}

	\noindent and its inverse is defined as

\begin{equation} \mathbf{P_D^{\left(-\omega\right)}}\left[X\right]: \left\{\begin{array}{rcl} \left[\mathbb{R}\to\mathbb{C}\right] &\to& \left[\mathbb{R}\to\mathbb{R}\right] \\[3mm] X(t) &\mapsto& \Re\left[X(t)e^{j\psi(t)}\right] \end{array}\right. \end{equation}

\end{definition}

	Here one wonders if any signal $x(t)$ is ``phasorializable'', that is if an arbitrary signal $x(t)$ admits a Dynamic Phasor representation. This is equivalent to asking whether any signal $x(t)$ can be written as a generalized sinusoid $m(t)\cos\left(\omega(t) + \phi(t)\right)$, because if this is true then $X(t) = m(t)e^{j\phi(t)}$ is the Dynamic Phasor of $x(t)$ at the apparent frequency $\omega(t)$. As discussed in subsection \ref{subsec:comments_def_sin}, this representation is rather forgiving — the restrictions for the sinusoidal representation seem to be nonexistant. Therefore, it would seem that the requirements for a real signal to be phasorializable are very weak. Again in the name of mathematical rigour we will explicitly say \textbf{we assume the signal is phasorializable} when this is assumed, and this assumption is equivalent to supposing the signal admits a sinusoidal representation.

\begin{definition}[Phasorializability] A signal $x(t)$ is \textbf{phasorializable} if it admits a sinusoidal representation $m(t)\cos\left(\psi(t) + \phi(t)\right)$ at some apparent frequency $\omega(t)$. In this case, $X(t) = m(t)e^{j\phi(t)}$ is the Dynamic Phasor of $x(t)$ at $\omega(t)$. \end{definition}

%-------------------------------------------------
\subsection{Properties of the Dynamic Phasor Transform}

	Having constructed the proposed Dynamic Phasor Transform, we must assert its properties; the first and possibly cornerstone propery being that the DPT generalizes the Static Phasor Operator, in the sense that the SPO is a particular case of the DPT. Indeed, given statically sinusoidal signals at a particular frequency $\omega_0$, $\mathbf{p_S}$ is equivalent to $\mathbf{P_D^{\left(-\omega_0\right)}}$. Take a sinusoidal signal $x(t) = m\cos\left(\theta(t)\right)$ with a constant amplitude, and such that there exists a positive real $\omega_0$ for which there exists a constant solution $\phi$ for the equation $\theta = \omega t + \phi$. Then obviously 

% DYNAMIC PHASOR DIAGRAM <<<
\begin{figure}[htb]
\centering
	\begin{tikzpicture}[scale=2,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
		\draw [fill=none,gray, thick] (0,0) circle (10 mm) node [gray] {};
		\draw [->, thick, black!30] (   -20mm,  0   ) -- (   20mm,  0   );
		\draw [->, thick, black!50] (      0, -15mm ) -- (   0   ,  17mm);

		\draw [->, thick, black] (0,0) -- (14mm,0) coordinate(realvec);
		\draw [->, thick, black] (0,0) -- (0,14mm) coordinate(imagvec);

		\node (realveclabel) at ([shift=({0,-2mm})]realvec) {$R = 1e^{j0}$};
		\node (realveclabel) at ([shift=({4mm,-2mm})] imagvec) {$I = 1e^{j\frac{\pi}{2}}$};

		\node [black!50] (reAxisLabel) at (22mm,0) {Re};
		\node [black!50] (imAxisLabel) at (0,19mm) {Im};

	%	\node [label={[label distance=0.1mm]30:$X = m(t)e^{j\phi(t)}$}] (X) at ({10mm*cos(40)},{10mm*sin(40)}) {};
	%	\draw [->,thick] (0,0) -- (X.center);
	%	\draw [->,stewartblue,thick] ({8mm*cos(0)},{8mm*sin(0)}) arc[start angle=0, end angle = 38, radius = 8mm];
		\draw [->,stewartblue,thick] ({8mm*cos(290)},{8mm*sin(290)}) arc[start angle=290, end angle = 328, radius = 8mm];

	%	\node [color=stewartblue] (philabel) at ({12mm*cos(20)},{12mm*sin(20)}) {$\phi(t)$};
		\node [color=stewartblue] (philabelrotated) at ({6mm*cos(310)},{6mm*sin(310)}) {$\phi(t)$};

		\node (rotX) at ({cos(330)},{sin(330)}) {};
		\node (XomegatLabel) at ({13mm*cos(340)},{13mm*sin(340)}) {$Xe^{j\psi(t)}$};
		\draw [->,thick] (0,0) -- (rotX.center);

		\node (rotRe) at ({13mm*cos(290)},{13mm*sin(290)}) {};
		\node (rotIm) at ({13mm*cos(20)},{13mm*sin(20)}) {};
		\node (RomegatLabel) at ({14mm*cos(290)},{14mm*sin(290)}) {$Re^{j\psi(t)}$};
		\node (IomegatLabel) at ({16mm*cos(20)},{16mm*sin(20)}) {$Ie^{j\psi(t)}$};
		\draw [->,thick] (0,0) -- (rotRe.center);
		\draw [->,thick] (0,0) -- (rotIm.center);

		\draw [->,thick,gray] (0,0) -- ({10mm*cos(240)},{10mm*sin(240)});
		\node [label={[gray,label distance=0.0mm]245:$m(t)$}] (mt) at ({9mm*cos(245)},{9mm*sin(245)}) {};

		%\draw [->,gray,thick] ({6mm*cos(25)},{6mm*sin(25)}) arc[start angle=25, end angle = 63, radius = 6mm];
		\node [green!50!black] (omegat) at ({6mm*cos(135)},{6mm*sin(135)}) {$\psi(t)$};

		\draw [-{Stealth[inset=0mm,length=3mm,angle'=50]},green!50!black, line width = 1mm] (4mm,0) arc[start angle=0, end angle = 287, radius = 4mm];
		
		% SINEWAVE PLOT AXES
		\draw [->, gray, thick]  (   -15mm, -20mm  ) -- (   15mm,  -20mm  );
		\draw [->, gray, thick]  (      0,  -20mm  ) -- (    0  ,  -50mm  );

		\node[gray] (axistlabel) at (0mm,-52mm) {$t$};

		\begin{axis}[color=stewartblue, at={(-12.5mm,-52mm)}, rotate=-90, width=35mm, height=25mm, scale only axis, yticklabel=\empty, xticklabel=\empty, axis line style={draw=none}, tick style={draw=none}, scaled ticks = false]
			\addplot[domain=0:0.033, smooth, samples=100] {0.001*(1 + 0.5*exp(-12*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x)))))};
			\addplot[domain=0.033:0.075, smooth, dashed, dash pattern=on 1pt off 1pt, samples=100] {0.001*(1 + 0.5*exp(-12*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x)))))};
		\end{axis}

		\node (rotXaxis) at ([shift=({0,-27.5mm})]rotX.center) {};
		\draw [color=stewartblue,fill] (rotXaxis) circle (0.5mm);

		\draw[dashed,color=stewartblue, thick] (rotX) -- (rotXaxis) ;

		\node [stewartblue] (xsignal) at (25mm, -40mm) {$x(t)  = m(t)\cos\left(\psi(t) + \phi(t)\right)$};

	\end{tikzpicture}
	\caption{Generalized sinusoidal signal as the real projection of a rotated dynamic phasor.}
	\label{fig:dynamic_phasor_representation}
\end{figure} %>>>

\begin{equation} \mathbf{P_D^{\omega_0}}\left[x\right] = me^{j\phi} = \mathbf{p_S}\left[x\right]. \end{equation}

	Further, given any complex number $X = me^{j\phi}$ and a constant frequency signal $\omega_0$, then

\begin{equation} \mathbf{P_D^{\left(-\omega_0\right)}}\left[me^{j\phi}\right] = \Re\left[me^{j\phi}e^{j\omega_0 t}\right] = m\cos\left(\omega_0 t + \phi\right) \end{equation}

	\noindent proving that $\mathbf{p_S}$ is a particular case of $\mathbf{P_D}$, and the same relationship holds for the inverse transforms. Due to this, an adaptation of figure \ref{fig:static_phasor_representation} is unavoidable; such adaptation is represented in figure \ref{fig:dynamic_phasor_representation}. This figure shows a snapshot in time, where the signal $x(t)$ is represented as a Dynamic Phasor $X(t)$. The real axis is represented by the number $R = 1e^{j0}$. The phasor $X(t)$ is rotated by an angle of $\psi(t)$, and the signal $x(t)$ is the real projection of the rotated phasor $Xe^{j\psi(t)}$.

	Due to this representation, the DPT allows generating a rotating axis, known as the ``DQ'' axis, whence the ``direct-quadrature'' nomenclature is born. Looking at Figure \ref{fig:dynamic_phasor_representation}, one notes that with respect to the static real-imaginary frame the number $Xe^{j\psi(t)}$ is such that

\begin{equation} Xe^{j\psi(t)} = x_\alpha + jx_\beta,\end{equation}

	\noindent meaning that $Xe^{j\psi(t)}$ is the complexification of $\mathbf{x}_{\alpha\beta}$ on the static frame. This is equivalent to saying that $Xe^{j\psi(t)}$ represents $x(t)$ in a static reference frame that is the real-imaginary axis of Figure \ref{fig:dynamic_phasor_imreaxis}. By definition, the real or horizontal projection of $x_\alpha + j x_\beta$ is the sinusoid $x(t)$, also represented in the figure.

	The static real-imaginary frame is generated at $t=0$ when the system starts counting time, so that if the initial time adopted is delayed or advanced, this frame is tilted accordingly but stays static along time. If, however, one adopts as a reference the rotating axes made by the rotating vectors $Re^{j\psi(t)}$ and $Ie^{j\psi(t)}$, then projecting the rotating phasor $Xe^{j\psi(t)}$ into this new frame one obtains $X(t) = x_d + jx_q = m(t)e^{j\phi(t)}$ itself, because the angular distance between the rotated phasor $X(t)e^{j\psi(t)}$ and the rotated real reference $R(t)e^{j\psi(t)}$ is always $\phi(t)$ and the rotation process does not alter the amplitude $m(t)$. Figure \ref{fig:dynamic_phasor_imreaxis} shows the real-imaginary rotated by $\psi(t)$, generating the DQ frame. In this gist, we can adopt not the static real and imaginary axes as references, but these rotated real and imaginary frames. These frames will be called ``DQ'' frame; here, ``D'' stands for ``direct'' because this axis is in direct phase with the rotated real axis, whereas ``Q'' stands for ``quadrature'' because the Q axis is in quadrature with the rotated real axis. As such, the projections of phasors against this frame are called their ``dq'' components — thus generating the nomenclature and notation ``dq'' for $\mathbf{x}_{dq}$.

	In other words, fundamentally what the ``dq'' transform (represented by $\mathbf{T}_{\psi}$ in the bidimensional frame or $e^{-j\psi(t)}$ in the complex domain) does is generating a new rotating frame such that the complex quantities involved, when projected against this frame, do not depend directly on the frequency $\omega(t)$ or its corresponding accumulated angle $\psi(t)$, but represent the time-varying Dynamic Phasor functions directly as opposed to their static frame versions — that is, the quantity $x_\alpha + jx_\beta$. Because the DQ frame is rotated at $\psi(t)$, this means that the frequency of rotation is $\omega(t)$, such that the vector $Xe^{j\psi(t)}$ is represented in this frame by $X(t)$. In other words, this rotating frame is the direct representation of the DPT, as the Dynamic Phasors produced by $\mathbf{P_D^{\omega}}$ are represented directly onto this frame.

% DYNAMIC PHASOR DIAGRAM OF IMAGE REAL STATIC FRAME <<<
\begin{figure}[htb!]
\centering
\scalebox{0.8}{
	\begin{tikzpicture}[scale=2,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]

		\node (origin) at (0,0) {};
		\draw [->] (   -2mm,  0   ) -- (   40mm,  0   ) node (xaxis) {};
		\draw [->] (      0, -2mm ) -- (   0   ,  40mm) node (yaxis) {};

		\node (reAxisLabel) at (42mm,0) {Re};
		\node (imAxisLabel) at (0,42mm) {Im};

		\draw [->, black!50, name path = daxis] (0,0) -- ({45mm*cos(25)}, {45mm*sin(25)});
		\draw [->, black!50, name path = qaxis] (0,0) -- ({45mm*cos(115)},{45mm*sin(115)});

		\node[right,black!50] (DAxisLabel) at ({47mm*cos(25) - 2mm} ,{47mm*sin(25)})  {$D = Re^{j\psi(t)}$};
		\node[black!50]       (QAxisLabel) at ({47mm*cos(115)},      {47mm*sin(115)}) {$Q = Ie^{j\psi(t)}$};

		\node [black!50] (omegat) at ({42mm*cos(32)},{42mm*sin(32)}) {$\omega(t)$};
		\draw [-{Stealth[inset=0mm,length=3.5mm,angle'=50]}, black!50, line width = 1mm] ({42mm*cos(20)},{42mm*sin(20)}) arc[start angle=20, end angle = 30, radius = 42mm];

		\draw [->,black!50,thick] ({38mm*cos(0)},{38mm*sin(0)}) arc[start angle=0, end angle = 23, radius = 38mm];
		\node [right,gray] (psilabel) at ({39mm*cos(11)},{39mm*sin(11)}) {$\psi(t)$};

		\node [right, stewartblue] (elabel) at ({35mm*cos(65)},{35mm*sin(65)}) {$x_\alpha + j x_\beta = Xe^{j\psi(t)}$};
		\node [right, stewartblue] (amplilabel) at ({17mm*cos(74)},{17mm*sin(74)}) {$m(t)$};
		\draw [->, stewartblue] (0,0) -- (elabel);

		\draw [gray, dashed] (elabel) -- (elabel |- xaxis) node (xtproj) {} ;
		\node [gray, below]  (xtprojlabel) at (xtproj.center) {$x(t)$} ;

		\draw [->, stewartblue] ({25mm*cos(25)},{25mm*sin(25)}) arc[start angle=25, end angle=54, radius = 25mm];
		\node [stewartblue] (phivlabel) at ({22mm*cos(39)},{22mm*sin(39)}) {$\phi(t)$};

		% Obtaining Ed and Eq projections
		\path [name path = edprojection] (elabel) -- +($(origin)-0.5*(QAxisLabel)$);
		\path [name intersections={of=edprojection and daxis, by=Ed}];
		\node [circle,fill=stewartblue,inner sep=1.5pt,label={[text=stewartblue]-60:$x_d$}] at (Ed) {};
		\draw [dashed,stewartblue] (Ed) -- (elabel);
		
		\path [name path = eqprojection] (elabel) -- +($(origin)-(DAxisLabel)$);
		\path [name intersections={of=eqprojection and qaxis, by=Eq}];
		\node [circle,fill=stewartblue,inner sep=1.5pt,label={[text=stewartblue]-120:$x_q$}] at (Eq) {};
		\draw [dashed,stewartblue] (Eq) -- (elabel);

	\end{tikzpicture}
	}
	\caption
[Phasorial schematic of the Dynamic Phasor Transform as a rotational transform.]
{Phasorial schematic of the Dynamic Phasor Transform as a rotational transform. In black the real-imaginary static frame, and in gray the rotated ``DQ'' frame that rotates at the apparent frequency $\omega(t)$. Naturally, the function $f(t) = x_\alpha + jx_\beta$ when projected onto the real frame is $x(t)$; however, when this quantity is projected onto the DQ frame one obtains the Dynamic Phasor $X(t) = x_d + jx_q$.}
	\label{fig:dynamic_phasor_imreaxis}
\end{figure} %>>>

	Notably, since the Static Phasor Operator is a particular case of the Dynamic Phasor Transform, the SPO is given as the particular case where the DQ frame that rotates at a fixed frequency $\omega_0$, such that the sinusoidal signals considered, when projected into this new rotating frame, become static complex numbers. In the literature, it is often said that the sinusoidal static signals are ``rotating vectors'' that decompose as static numbers onto the frame.

	In the Power System literature, the achievement of a DQ frame with time-varying rotating frequency is paramount because it allows the adoption of reference frames which frequency are time-varying. In general, phasorial diagrams of power systems are represented with respect to a reference or ``slack'' bus, generally rotating at the fixed synchronous frequency, even though the machine attached to this bus is subject to transient phenomena of amplitude and frequency. This generates a confusing phenomena that the ``slack'' machine is not static with respect to the ``slack'' reference; with the time-varying generalization of Figure \ref{fig:dynamic_phasor_imreaxis}, this allows adoption of a reference DQ frame which rotating frequency is time varying. Adopting this frequency as the frequency of the reference bus, then this slack bus is static with respect to the reference frame.
	Further, it follows from the definition is that the DPT is linear, as is its inverse. This can be proven in two ways: first, it stems directly from the basic fact that all the transforms involved ($\alpha\beta$,dq and $\rho$) are linear. Alternatively, one can easily rewrite theorem \ref{theo:ps_morphism} with time-varing quantities. Again, the comparison with Static Phasors is again inevitable; Figure \ref{fig:dpt_linearity} shows the linearity schematic of the DPT, in line with the linearity of static phasors as in Figure \ref{fig:spo_linearity}.

% STATIC PHASOR DIAGRAM LINEARITY <<<
\begin{figure}[htb]
\centering
	\begin{tikzpicture}[scale=1.5,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
		\draw [->, thick, black!50] (   -30mm,  0   ) -- (   30mm,  0   );
		\draw [->, thick, black!50] (      0, -5mm ) -- (   0   ,  15mm);

		\node [gray] (imlabel) at ( 0mm, 17mm) {Im};
		\node [gray] (relabel) at (32mm,  0mm) {Re};

		\node (O) at (0,0)       {};
		\node (X) at (9mm,12mm)  {};
		%\node [stewartblue] (Xlabel) at ([shift=({0,2mm})]X)  {$X(t)e^{j\psi(t)}$};
		\node (Y) at (7mm,2mm)   {};
		\node (2Y) at (14mm,4mm) {};
		%\node [stewartgreen] (Ylabel) at ([shift=({-3mm,-3.5mm})]Y)  {$Y(t)e^{j\psi(t)}$};
		\node (2Y) at (14mm,4mm) {};
		%\node [stewartyellow] (2Ylabel) at ([shift=({3.5mm,-1mm})]2Y)  {$2Y(t)e^{j\psi(t)}$};

		\node [label={[color=stewartblue]right:$X(t)e^{j\psi(t)}$}] at           (33mm, 15mm) {};
		\node [label={[color=stewartgreen]right:$Y(t)e^{j\psi(t)}$}] at           (33mm, 12mm) {};
		\node [label={[color=stewartyellow]right:$2Y(t)e^{j\psi(t)}$}] at           (33mm, 09mm) {};
		\node [label={[color=stewartpurple]right:$\left[X(t) + 2Y(t)\right]e^{j\psi(t)}$}] at           (33mm, 06mm) {};

		\node (SUM) at (23mm,16mm) {};
		%\node [stewartpurple] (SUMlabel) at ([shift=({2mm, 2mm})]SUM)  {$\left[X(t) + 2Y(t)\right]e^{j\psi(t)}$};

		\draw [->, thick, stewartblue]  (O.center) -- (X.center);
		\draw [->, thick, stewartyellow]  (O.center) -- (2Y.center);
		\draw [->, thick, stewartgreen]  (O.center) -- (Y.center);
		\draw [thick, dashed, stewartblue!50]  (2Y.center) -- (SUM.center);
		\draw [thick, dashed, stewartgreen!50] (X.center) -- (SUM.center);

		\draw [->, thick, stewartpurple] (O.center) -- (SUM.center);

		\draw [->, gray, thick]  (   -35mm, -10mm  ) node (beginXaxis) {} -- (   35mm,  -10mm  ) node (endXaxis) {};
		\draw [->, gray, thick]  (      0,  -10mm  ) node (beginYaxis) {} -- (    0  ,  -70mm  ) node (endYaxis) {} ;

		\node [gray] (tlabel) at ( 0mm,-72mm) {$t$};

		\draw [thick,dashed,stewartblue!50]     (X) -- (beginXaxis -|   X);
		\draw [thick,dashed,stewartgreen!50]    (Y) -- (beginXaxis -|   Y);
		\draw [thick,dashed,stewartyellow!50]   (2Y) -- (beginXaxis -|  2Y);
		\draw [thick,dashed,stewartpurple!50] (SUM) -- (beginXaxis -| SUM);

		\begin{axis}[at={(-33.5mm,-71.5mm)}, rotate=-90, width=67mm, height=71mm, scale only axis, yticklabel=\empty, xticklabel=\empty, axis line style={draw=none}, tick style={draw=none}, scaled ticks = false, ymin = -1, ymax = 1.1]
			\addplot[domain=0:0.07, smooth, color = stewartblue  , samples=100] {0.25*(1 + 0.5*exp(-12*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x)))))};
			\addplot[domain=0:0.07, smooth, color = stewartgreen , samples=100] {0.22*(1 -1.5*exp(-50*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x)))))};
			\addplot[domain=0:0.07, smooth, color = stewartyellow, samples=100] {0.41*(1 -1.5*exp(-50*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x)))))};
			\addplot[domain=0:0.07, smooth, color = stewartpurple, samples=100] {0.25*(1 + 0.5*exp(-12*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x))))) + 0.44*(1 -1.5*exp(-50*x)*sin(deg(500*3.14159*x)))*cos(deg(200*x*(1 - 6*exp(-150*x)*sin(deg(10*3.14159*x)))))};
	%		\addplot[color = stewartblue, domain=0:720, smooth, samples=100] {15mm*cos(x+53.13)};
	%		\addplot[color = stewartyellow, domain=0:720, smooth, samples=100] {2*7.28mm*cos(x+15)};
	%		\addplot[color = stewartpurple, domain=0:720, smooth, samples=100] {28.02mm*cos(x+34.82)};
		\end{axis}

		\node [label={[color=stewartblue]right:$x(t)$}] at           (33mm, -40mm) {};
		\node [label={[color=stewartgreen]right:$y(t)$}] at          (33mm, -43mm) {};
		\node [label={[color=stewartyellow]right:$2y(t)$}] at        (33mm, -46mm) {};
		\node [label={[color=stewartpurple]right:$x(t) + 2y(t)$}] at (33mm, -49mm) {};

	\end{tikzpicture}
	\caption{Dynamic Phasor Transform linearity schematic.}
	\label{fig:dpt_linearity}
\end{figure} %>>>

%-------------------------------------------------
\section{The Dynamic Phasor Transform applied to linear systems} %<<<1

	We now apply the DPT and use its properties to study how it transforms linear systems. Given a nonstationarily-excited linear system differential model of the form

\begin{equation} \sum\limits_{k=0}^{n} \alpha_k x^{\left(k\right)} - m(t)\cos\left(\psi(t) + \phi(t)\right) = 0, \label{eq:linsys_sinusoidal_forcing}\end{equation}

	\noindent that is, $x(t)$ is governed by a LTI differential equation excited by some sinusoidal forcing at an apparent frequency $\omega(t)$, then how do the ``dq'' components of $x(t)$ behave at that same apparent frequency? In a geometric interpretation, let us assume that the static-frame quantity $f(t) = x_\alpha(t) + jx_\beta(t)$ of figure \ref{fig:dynamic_phasor_imreaxis} is the solution of

\begin{equation} \sum\limits_{k=0}^{n} \alpha_k f^{\left(k\right)} -  m\left(t\right)e^{j\left[\psi(t) + \phi(t)\right]} = 0, \label{eq:linsys_sinusoidal_forcing_complex}\end{equation}

	\noindent then how does the representation of $f(t)$ with respect to the DQ frame behave?

%-------------------------------------------------
\subsection{A dq-equivalent linear system} %<<<2

	To achieve an equivalent dq equation for the linear system, lemmas \ref{theo:dq_1p_diff} and \ref{lemma:1p_t_ndifftminus_product} prove operational and differential properties of the dq transform $\mathbf{T}_{\psi(t)}$ to be used in the proof of theorem \ref{theo:1p_ode_solution}, which provides the sought ``dq equivalent system'' from the original linear system . Lemma \ref{theo:dq_1p_diff} shows that $d^k\mathbf{x}_{dq}/dt^k$ can be written as compositions of the differentials of $\mathbf{T}$ and $x$, and the other way around.  This lemma allows for a matricial time derivation of dq quantities, that is, obtain $\dot{\mathbf{x}}_{dq}$ from $\dot{x}$ and vice-versa. Lemma \ref{lemma:1p_t_ndifftminus_product} generalizes the Chain Rule for a k-th order differential of a complex matrix by using then generalization of the k-th order Chain Rule for single-variable functions, known as Faà Di Bruno's formula \pcite{DiBruno1855}.
	
\begin{lemma}[n-th order time differentiation of $dq$ transformed phasor quantities]\label{theo:dq_1p_diff}%<<<
	Let $n\in\mathbb{N}^*$, $\mathbf{x}_{\alpha\beta}$ the $\alpha\beta$ transform of a sinusoid $x(t)$, $\mathbf{T}_\theta$ the $dq$ Transform operator where $\theta(t)$ is $C^n$-class, and $\mathbf{x}_{dq} = \mathbf{T}_\theta \mathbf{x}_{\alpha\beta}$. Then

\begin{equation} \dfrac{d^n \mathbf{x}_{dq}}{dt^n} = \dfrac{d^n\left(\mathbf{T}_\theta \mathbf{x}_{\alpha\beta}\right)}{dt^n} = \sum\limits_{k=0}^{n} {n\choose k} \left(\dfrac{d^{k} \mathbf{T}_\theta}{dt^k}\right) \left(\dfrac{d^{\left(n-k\right)} \mathbf{x}_{\alpha\beta}}{dt^{\left(n-k\right)}}\right), \end{equation}

	\noindent and

\begin{equation} \dfrac{d^n\mathbf{x}_{\alpha\beta}}{dt^n} = \dfrac{d^n\left(\mathbf{T}^{-1}_\theta \mathbf{x}_{dq}\right)}{dt^n} = \sum\limits_{k=0}^{n} {n\choose k} \left(\dfrac{d^{k} \mathbf{T}^{-1}_\theta}{dt^k}\right) \left(\dfrac{d^{\left(n-k\right)} \mathbf{x}_{dq}}{dt^{\left(n-k\right)}}\right) .\end{equation}

	Particularly for $n=1$,

\begin{equation} \dfrac{d\mathbf{x}_{\alpha\beta}}{dt} = \dfrac{d}{dt} \left(\mathbf{T}^{-1}_\theta \mathbf{x}_{dq}\right) = \mathbf{T}^{-1}_\theta \dfrac{d\mathbf{x}_{dq}}{dt} + \dfrac{d\mathbf{T}^{-1}_\theta}{dt} \mathbf{x}_{dq}, \end{equation}

	\noindent and

\begin{equation} \dfrac{d\mathbf{x}_{dq}}{dt} = \dfrac{d}{dt} \left(\mathbf{T}_\theta \mathbf{x}_{\alpha\beta}\right) = \mathbf{T}_\theta \dfrac{d\mathbf{x}_{\alpha\beta}}{dt} + \dfrac{d\mathbf{T}_\theta}{dt} \mathbf{x}_{\alpha\beta}, \end{equation}
 
	\noindent where

\begin{equation}
        \dfrac{d\mathbf{T}_\theta }{dt} =
\dfrac{d\theta}{dt}
\left[\begin{array}{cc}
         -\sin\left(\theta\right) &  \cos\left(\theta\right)                    \\[5mm]
         -\cos\left(\theta\right) & -\sin\left(\theta\right)
\end{array}\right]\end{equation}

	and

\begin{equation}
        \dfrac{d\mathbf{T}^{-1}_{\theta} }{dt} =
\dfrac{d\theta}{dt}
\left[\begin{array}{cc}
         -\sin\left(\theta\right) & -\cos\left(\theta\right)                    \\[5mm]
          \cos\left(\theta\right) & -\sin\left(\theta\right)
\end{array}\right]
\end{equation}

\end{lemma}
\textbf{Proof:} let $\mathbf{M}\in\left[\mathbb{R}\to\mathbb{C}^{m\times n}\right]$ and $\mathbf{G}\in\left[\mathbb{R}\to\mathbb{C}^{p\times q}\right]$ for some naturals $m,n,p,q$. $\mathbf{M}$ and $\mathbf{G}$ are defined by their elements $a_{ij}\left(t\right)\in\left[\mathbb{R}\to\mathbb{C}\right]$ such that $a_{ij}$ are $C^n$ class. Then the tangent matrix is defined as the element-wise differentiation

\begin{equation} \dfrac{d\mathbf{M}}{dt} = \left\{\dfrac{dm_{ij}}{dt}\right\} . \end{equation}

	Also suppose that $m,n,p,q$ are such that $\mathbf{MG}$ exists. Then

\begin{equation} \dfrac{d\left(\mathbf{MG}\right)}{dt} = \mathbf{M}\left(\dfrac{d\mathbf{G}}{dt}\right) + \left(\dfrac{d\mathbf{M}}{dt}\right) \mathbf{G} \end{equation}

	\noindent which comes directly from matrix (tensor) calculus \pcite{bishopTensorAnalysisManifolds1980}. Note that this equation is similar to a differentiation product rule $\left(fg\right)' = fg' + f'g$. Differentiating for the second derivative, one obtains

\begin{equation} \dfrac{d^2\left(\mathbf{MG}\right)}{dt^2} = \mathbf{M}\left(\dfrac{d^2\mathbf{G}}{dt^2}\right) + 2\left(\dfrac{d\mathbf{M}}{dt}\right)\left(\dfrac{d\mathbf{G}}{dt}\right) + \left(\dfrac{d^2\mathbf{M}}{dt^2}\right) \mathbf{G} \end{equation}

	\noindent which also looks like a second-order differentiation product rule: $\left(fg\right)'' = fg'' + 2f'g' + f''g$. And again for the third,

\small
\begin{equation}
	\dfrac{d^3\left(\mathbf{MG}\right)}{dt^3} = \mathbf{M}\left(\dfrac{d^3\mathbf{G}}{dt^3}\right) + 3\left(\dfrac{d^2\mathbf{M}}{dt^2}\right)\left(\dfrac{d\mathbf{G}}{dt}\right) + 3\left(\dfrac{d\mathbf{M}}{dt}\right)\left(\dfrac{d^2\mathbf{G}}{dt^2}\right) + \left(\dfrac{d^3\mathbf{M}}{dt^3}\right) \mathbf{G}
\end{equation}
\normalsize

	\noindent akin to the third-order differentiation product rule. These results suggest that the matrix product rule is given by an equation akin to that of the Leibnitz Rule for single-variable complex functions:

\begin{equation} \dfrac{d^n\left(\mathbf{MG}\right)}{dt^n} = \sum\limits_{k=0}^{n} {n\choose k} \left(\dfrac{d^k \mathbf{M}}{dt^k}\right)  \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right), \end{equation}

	where the zero-degree derivative is equal to the identity. The proof follows by induction: as shown, the results are true for $n=1,2,3$. By inductive hypothesis, suppose the result is true for some $n-1$. Then for some $n$,

\begin{align}
	 \dfrac{d^n\left(\mathbf{MG}\right)}{dt^n}
	&= \dfrac{d}{dt}\left[\dfrac{d^{\left(n-1\right)} \left(\mathbf{MG}\right)}{dt^{\left(n-1\right)}}\right] = \nonumber\\[3mm]
	&= \dfrac{d}{dt}\left[\sum\limits_{k=0}^{n-1} {n-1\choose k} \left(\dfrac{d^k \mathbf{M}}{dt^k}\right)  \left(\dfrac{d^{\left(n-1-k\right)} \mathbf{G}}{dt^{\left(n-1-k\right)}}\right)\right] = \nonumber\\[3mm]
	&= \sum\limits_{k=0}^{n-1} {n-1\choose k} \left[\left(\dfrac{d^{\left(k+1\right)} \mathbf{M}}{dt^{\left(k+1\right)}}\right)  \left(\dfrac{d^{\left(n-1-k\right)} \mathbf{G}}{dt^{\left(n-1-k\right)}}\right) + \left(\dfrac{d^k \mathbf{M}}{dt^k}\right)  \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right)\right]
\end{align}

	Substituting $j = k+1$,

\begin{equation}
	 \dfrac{d^n\left(\mathbf{MG}\right)}{dt^n} = \sum\limits_{j=1}^{n} {n-1\choose j-1} \left(\dfrac{d^{\left(j\right)} \mathbf{M}}{dt^{\left(j\right)}}\right)  \left(\dfrac{d^{\left(n-j\right)} \mathbf{G}}{dt^{\left(n-j\right)}}\right) + \sum\limits_{k=0}^{n-1}{n-1\choose k}\left(\dfrac{d^{k} \mathbf{M}}{dt^k}\right)  \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right)
\end{equation}

	It is clear that both parts of the sum are in fact the same expression but lacking the extremes:

\small
\begin{align}
	\dfrac{d^n\left(\mathbf{MG}\right)}{dt^n}
	&= \sum\limits_{j=1}^{n-1} {n-1\choose j-1} \left(\dfrac{d^{\left(j\right)} \mathbf{M}}{dt^{\left(j\right)}}\right)  \left(\dfrac{d^{\left(n-j\right)} \mathbf{G}}{dt^{\left(n-j\right)}}\right) + \sum\limits_{k=1}^{n-1}{n-1\choose k}\left(\dfrac{d^{k} \mathbf{M}}{dt^k}\right)  \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right) + \nonumber\\[3mm] &\hspace{30mm} + {n-1\choose n-1}\left(\dfrac{d^{\left(n\right)} \mathbf{M}}{dt^{\left(n\right)}}\right)  \left(\dfrac{d^{\left(0\right)} \mathbf{G}}{dt^{\left(0\right)}}\right) + {n\choose 0}\left(\dfrac{d^{\left(0\right)} \mathbf{M}}{dt^{\left(0\right)}}\right)  \left(\dfrac{d^{\left(n\right)} \mathbf{G}}{dt^{\left(n\right)}}\right) \nonumber\\[3mm]
%
%
%
	&= \sum\limits_{k=1}^{n-1} \overbrace{\left[{n-1\choose k} + {n-1\choose k-1}\right]}^{={n\choose k}} \left(\dfrac{d^{k} \mathbf{M}}{dt^k}\right)  \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right) + \left(\dfrac{d^{\left(n\right)} \mathbf{M}}{dt^{\left(n\right)}}\right)  \left(\dfrac{d^{\left(0\right)} \mathbf{G}}{dt^{\left(0\right)}}\right) + \left(\dfrac{d^{\left(0\right)} \mathbf{M}}{dt^{\left(0\right)}}\right)  \left(\dfrac{d^{\left(n\right)} \mathbf{G}}{dt^{\left(n\right)}}\right) \nonumber\\[3mm]
	&= \sum\limits_{k=1}^{n-1} {n\choose k} \left(\dfrac{d^{k} \mathbf{M}}{dt^k}\right) \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right) + {n\choose 0}\left(\dfrac{d^{\left(n\right)} \mathbf{M}}{dt^{\left(n\right)}}\right)  \left(\dfrac{d^{\left(0\right)} \mathbf{G}}{dt^{\left(0\right)}}\right) + {n\choose n}\left(\dfrac{d^{\left(0\right)} \mathbf{M}}{dt^{\left(0\right)}}\right)  \left(\dfrac{d^{\left(n\right)} \mathbf{G}}{dt^{\left(n\right)}}\right) \nonumber\\[3mm]
	&= \sum\limits_{k=0}^{n} {n\choose k} \left(\dfrac{d^{k} \mathbf{M}}{dt^k}\right) \left(\dfrac{d^{\left(n-k\right)} \mathbf{G}}{dt^{\left(n-k\right)}}\right) 
\end{align}
\normalsize

	\noindent and this result can be seen as Leibnitz Rule for single-variable matrix functions. The index changing is done by adopting $p = n-k$:

\begin{equation} \dfrac{d^n\left(\mathbf{MG}\right)}{dt^n} = \sum\limits_{p=0}^{n} {n\choose \left(n-p\right)} \left(\dfrac{d^{\left(n-p\right)} \mathbf{M}}{dt^{\left(n-p\right)}}\right) \left(\dfrac{d^p \mathbf{G}}{dt^p}\right) \end{equation}

	From combinatorics, $C^n_{\left(n-p\right)} = C^n_p$:

\begin{equation} \dfrac{d^n\left(\mathbf{MG}\right)}{dt^n} = \sum\limits_{p=0}^{n} {n\choose p} \left(\dfrac{d^{\left(n-p\right)} \mathbf{M}}{dt^{\left(n-p\right)}}\right) \left(\dfrac{d^p \mathbf{G}}{dt^p}\right)  \end{equation}

	Results follow immediately by adopting $\mathbf{x}_{\alpha\beta} = \mathbf{T}_\theta \mathbf{x}_{dq}$ and $\mathbf{x}_{dq} = \mathbf{T}^{-1}_\theta \mathbf{x}_{\alpha\beta}$. \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

\begin{lemma} \label{lemma:1p_t_ndifftminus_product}%<<<

	Let $n\in\mathbb{N}$ and consider the rotational transform $\mathbf{T}_\theta$ and the inverse $\mathbf{T}_{\left(-\theta\right)}$ where $\theta$ is n-th order differentiable. Then

\begin{equation} \mathbf{T}_\theta \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt^n} = \sum\limits_{k=0}^n \mathbf{G}_k B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right), \end{equation}

	\noindent where $B_{\left(n,k\right)}$ are the incomplete exponential Bell Polynomials and

\begin{equation}
\mathbf{G}_k = 
\left[\begin{array}{ccc}
 \cos\left( \dfrac{k\pi}{2}\right) &  -\sin\left(\dfrac{k\pi}{2}\right) \\[5mm]
 \sin\left( \dfrac{k\pi}{2}\right) & \phantom{-} \cos\left(\dfrac{k\pi}{2}\right)
\end{array}\right].
\end{equation}

	Particularly for $n=1$,

\begin{equation} \mathbf{T}_\theta\dfrac{d\mathbf{T}_\theta^{-1}}{dt} = \dfrac{d\theta}{dt} \left[\begin{array}{ccc}    0 & -1 \\[5mm] 1 & 0 \end{array}\right] \end{equation}

%	And for $n=2$,
%
%\begin{equation} \mathbf{T}_\theta\dfrac{d^2\mathbf{T}_\theta^{-1}}{dt^2} = \ddot{\theta}\left[\begin{array}{cc} 0 & -1 \\ 1 & 0\end{array}\right] + \left(\dot{\theta}\right)^2\left[\begin{array}{cc} -1 & 0 \\ 0 & -1\end{array}\right] \end{equation}

\end{lemma} 

\textbf{Proof:} for an arbitrary order $n \geq 0$, one needs to use the Faà Di Bruno's formula \pcite{DiBruno1855} for the n-th order Chain Rule. The formula states that, for two single-variable n-th order differentiable functions $f$ and $g$, the chain rule is given by

\begin{equation} \dfrac{d^n}{dx^n} f\left(g\left(x\right)\right)= \sum\limits_{k=0}^n f^{\left(k\right)}\left(g\left(x\right)\right) B_{\left(n,k\right)}\left(g'\left(x\right),g''\left(x\right),...,g^{\left(n-k+1\right)}\left(x\right)\right), \end{equation}

	\noindent where the $B_{\left(n,k\right)}$ are the incomplete exponential Bell Polynomials. Consider $t^{-1}_{\left(i,j\right)}$ as the $i,j$ element of $\mathbf{T}^{-1}$. Then	

\begin{equation} \dfrac{d^n}{dt^n} t^{-1}_{\left(i,j\right)} \left(\theta\left(t\right)\right)= \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(i,j\right)}\left(\theta\right)}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right). \end{equation}

	But because the indexes $n$ and $k$ are not related to $i$ and $j$, 

\begin{gather} \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt^n} = \nonumber\\[5mm]
	\left[\begin{array}{cc}
\sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(1,1\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(1,2\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) \\[5mm]
\sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(2,1\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(2,2\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right)
\end{array}\right] = \nonumber\\[5mm]
%
	= \sum\limits_{k=0}^n  B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right)\left[\begin{array}{ccc}
\dfrac{d^k t^{-1}_{\left(1,1\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(1,2\right)}}{d\theta^k} \\[5mm]
\dfrac{d^k t^{-1}_{\left(2,1\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(2,2\right)}}{d\theta^k}
\end{array}\right]
\end{gather}

	\noindent which in matrix form means

\begin{equation} \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt^n} = \sum\limits_{k=0}^n \dfrac{d^k\mathbf{T}^{-1}_\theta}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right), \end{equation}

	But knowing that

\begin{equation}
	\left\{\begin{array}{l}
		\dfrac{d^n \cos\left(\theta\right)}{d\theta^n} = \cos\left(\theta + \dfrac{n\pi}{2}\right) \\[5mm]
		\dfrac{d^n \sin\left(\theta\right)}{d\theta^n} = \sin\left(\theta + \dfrac{n\pi}{2}\right)
	\end{array}\right. ,
\end{equation}

	\noindent then for $n\geq 1$,

\begin{equation} \dfrac{d^k \mathbf{T}^{-1}_\theta}{d\theta^k} =  \left[\begin{array}{cc} \cos\left(\theta + \dfrac{k\pi}{2}\right) & -\sin\left(\theta + \dfrac{k\pi}{2}\right) \\[5mm] \sin\left(\theta + \dfrac{k\pi}{2}\right) & \cos\left(\theta + \dfrac{k\pi}{2}\right)\end{array}\right] = \mathbf{T}^{-1}\left(\theta + \dfrac{k\pi}{2}\right).\end{equation}

	Now calculate the matrix multiplication:

\begin{align}
\mathbf{T}_\theta \dfrac{d^k\mathbf{T}_\theta^{-1}}{d\theta^k} = \mathbf{T}_\theta\mathbf{T}^{-1}\left(\theta + \dfrac{k\pi}{2}\right) = \mathbf{T}\left(-\dfrac{k\pi}{2}\right) &= \left[\begin{array}{cc} \cos\left(-\dfrac{k\pi}{2}\right) & \sin\left(-\dfrac{k\pi}{2}\right) \\[5mm] -\sin\left(-\dfrac{k\pi}{2}\right) & \phantom{-}\cos\left(- \dfrac{k\pi}{2}\right)\end{array}\right] \nonumber\\[5mm] &= \left[\begin{array}{cc} \cos\left(\dfrac{k\pi}{2}\right) & -\sin\left(\dfrac{k\pi}{2}\right) \\[5mm] \sin\left(\dfrac{k\pi}{2}\right) & \phantom{-}\cos\left(\dfrac{k\pi}{2}\right)\end{array}\right]
\end{align}

	Call this matrix $\mathbf{G_k}$ and the proof is complete. For the particular case $n=1$ one can use this result or simply compute directly:

\begin{align}
	\mathbf{T}_\theta\dfrac{d\mathbf{T}^{-1}_\theta}{dt} &= 
	\left[\begin{array}{cc}
		 \cos\left(\theta\right) & \sin\left(\theta\right) \\[5mm]
		-\sin\left(\theta\right) & \cos\left(\theta\right)
	\end{array}\right]\dfrac{d\theta}{dt}
	\left[\begin{array}{cc}
		-\sin\left(\theta\right) & -\cos\left(\theta\right) \\[5mm]
		\cos\left(\theta\right) & -\sin\left(\theta\right)
	\end{array}\right]
	%
	=\dfrac{d\theta}{dt}\left[\begin{array}{ccc}
		0 & -1 \\[5mm]
		1 &  0
	\end{array}\right]
\end{align}
\hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

\begin{theorem}[Solutions to LTI ODEs with phasorial forcing] \label{theo:1p_ode_solution}%<<<

	Let $m\left(t\right),\theta\left(t\right)\in\left[\mathbb{R}\to\mathbb{R}\right]$ and consider the Hurwitz-stable linear ODE with a phasorial forcing:

\begin{equation} \sum\limits_{k=0}^{n} \alpha_k x^{\left(k\right)} - f(t) = 0, \label{eq:theo_1p_ode_solution_original_ode}\end{equation}

	\noindent with a set of initial conditions $x_0,x'_0,...,x^{(n-1)}_0$ where $f(t)$ admits a sinusoidal representation at some apparent frequency $\omega(t)$, which is supposed a $C^{\left(n-1\right)}$-class real function. Consider the ``dq equivalent'' system

\begin{equation} \sum\limits_{i=0}^n \mathbf{K}_i(t) \left(\dfrac{d^i \mathbf{z}_{dq}}{dt^i }\right) - \mathbf{f}_{dq} = 0 , \label{eq:theo_1p_ode_solution_dq_equiv}\end{equation}

	\noindent with a set of initial conditions $(\mathbf{z}_{dq})_0,(\mathbf{z}'_{dq})_0,...,(\mathbf{z}^{(n-1)}_{dq})_0$ , where $\mathbf{f}_{dq}$ is the $dq$ transform of the forcing at the frequency $\omega(t)$,

\begin{equation} \mathbf{K}_i(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right] ,\label{eq:ki_transf} \end{equation}

	\noindent are time-variant matrices where $B_{\left(i,j\right)}$ are the incomplete exponential Bell Polynomials and and $\mathbf{G}_k$ are calculated as

\begin{equation}
\mathbf{G}_k = 
\left[\begin{array}{ccc}
 \cos\left( \dfrac{k\pi}{2}\right) &  -\sin\left(\dfrac{k\pi}{2}\right) \\[5mm]
 \sin\left( \dfrac{k\pi}{2}\right) & \phantom{-} \cos\left(\dfrac{k\pi}{2}\right)
\end{array}\right]
\end{equation}

	Then there exist two positive reals $a,b$ such that the solution $x$ to the original ODE \eqref{eq:theo_1p_ode_solution_original_ode} satisfies

\begin{equation} \left\lVert \mathbf{x}_{\alpha\beta} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq}\right\rVert \leq ae^{-bt}, \label{eq:theo_1p_ode_solution_exp}\end{equation}

	\noindent with $\mathbf{x}_{dq}$ is the unique solution to the dq system \eqref{eq:theo_1p_ode_solution_dq_equiv}. Reestated, the solution $\mathbf{z}_{\alpha\beta}$ reconstructed by \eqref{eq:theo_1p_ode_solution_dq_equiv} is the globally steady-state stable solution of \eqref{eq:theo_1p_ode_solution_original_ode}.

\end{theorem}
\textbf{Proof:} consider the original single-phase LTI ODE

\begin{equation} \sum\limits_{k=0}^n \alpha_k x^{\left(k\right)} - f(t) = 0.\end{equation}

	By hypothesis this system is Hurwitz stable, that is, the solution $x(t)$ tends exponentially to a particular solution: $\left\lVert x(t) - x_p(t)\right\rVert \leq ae^{-bt}$ for some two reals $a$ and $b$. Finding a particular solution $z(t)$, let $z_0,z'_0,...,z^{(n-1)}_0$ the initial conditions of the particular solution. Using the $\alpha\beta$ transform to generate an equivalent two-dimensional ODE:

\begin{equation} \sum\limits_{k=0}^n \alpha_k \left[\begin{array}{c} z_\alpha \\ z_\beta \end{array}\right]^{\left(k\right)} - \left[\begin{array}{c} f_\alpha(t)\\ f_\beta(t) \end{array}\right] = 0\end{equation}

	And transform the equation through $\mathbf{T}_\psi$:

\begin{gather}
	\mathbf{T}_\psi\left\{\sum\limits_{k=0} \alpha_k \left[\begin{array}{c} z_\alpha \\ z_\beta  \end{array}\right]^{\left(k\right)}\right\} - \mathbf{T}_\psi\left\{\left[\begin{array}{c} f_\alpha(t) \\ f_\beta(t) \end{array}\right]\right\} = 0 \\[5mm]
%
	\sum\limits_{k=0}^n \alpha_k \mathbf{T}_\psi\left[\begin{array}{c} z_\alpha \\ z_\beta \end{array}\right]^{\left(k\right)} - \mathbf{f}_{dq} = 0
\end{gather}

	\noindent where $\mathbf{f}_{dq}$ is the $dq$ transform of the forcing at the chosen frequency $\omega(t)$. Let $\mathbf{z}_{dq} = \mathbf{T}_\psi \left[z_\alpha,z_\beta\right]$ the $dq$ transform of $\mathbf{z}_{\alpha\beta}$:

\begin{equation} \sum\limits_{k=0}^n \alpha_k \mathbf{T}_\psi\left(\mathbf{T}_\psi^{-1}\mathbf{z}_{dq}\right)^{\left(k\right)} - \mathbf{f}_{dq} = 0,\  \psi(t) = \int_0^t \omega(s)ds \end{equation}

	Apply lemma \ref{theo:dq_1p_diff}:

\begin{equation} \sum\limits_{k=0}^n \alpha_k\left\{\mathbf{T}_\psi\left[ \sum\limits_{p=0}^{k} {k\choose p} \left(\dfrac{d^{p} \mathbf{T}^{-1}_\psi}{dt^p}\right) \left(\dfrac{d^{\left(k-p\right)} \mathbf{z}_{dq}}{dt^{\left(k-p\right)}}\right) \right]\right\} - \mathbf{f}_{dq} = 0 \end{equation}

	And because both $\mathbf{T}$ and $\mathbf{T}^{-1}$ are linear,

\begin{equation} \sum\limits_{k=0}^n \alpha_k \sum\limits_{p=0}^{k} {k\choose p} \mathbf{T}_\psi\left[\left(\dfrac{d^{\left(k-p\right)} \mathbf{T}^{-1}_\psi}{dt^{\left(k-p\right)}}\right) \left(\dfrac{d^p \mathbf{z}_{dq}}{dt^p}\right) \right] - \mathbf{f}_{dq} = 0 \end{equation}

	Now apply lemma \ref{lemma:1p_t_ndifftminus_product}:

\begin{equation} \sum\limits_{k=0}^n \alpha_k \left\{\sum\limits_{p=0}^{k} {k\choose p} \left[\sum\limits_{c=0}^{k-p} \mathbf{G}_c B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right) \right] \left(\dfrac{d^p \mathbf{z}_{dq}}{dt^p }\right)\right\} - \mathbf{f}_{dq} = 0 \end{equation}

	To isolate the derivatives of $\mathbf{z}_{dq}$, one must solve the triangular sum of this equation. The 0-th derivatives are present at all $k$ indexes; the first, for the $k$ indexes $1$ through $n$; the second for $2$ to $n$. In general, the i-th derivative is present for indexes $k$ from $i$ to $n$.

\begin{equation} \sum\limits_{i=0}^n \left\{\sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right]\right\} \left(\dfrac{d^i \mathbf{z}_{dq}}{dt^i }\right) - \mathbf{f}_{dq} = 0 .\end{equation}

	Finally, we group the matrix inside the sum as

\begin{equation} \mathbf{K}_i(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right] \end{equation}

	yielding

\begin{equation} \sum\limits_{i=0}^n \mathbf{K}_i(t) \left(\dfrac{d^i \mathbf{z}_{dq}}{dt^i }\right) - \mathbf{f}_{dq} = 0 .\end{equation}

	Therefore, $z(t) = z_\alpha(t)$ where $\mathbf{z}_{\alpha\beta} = \mathbf{T}^{-1}_{\psi(t)}\mathbf{z}_{dq}$ is a particular solution to the original system, and \eqref{eq:theo_1p_ode_solution_exp} follows. \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	In short, theorem \ref{theo:1p_ode_solution} shows how a linear system is ``converted'' into a ``dq version'', as depicted in Figure \ref{fig:dqification}. The figure shows the ``original'' linear system in time domain on top, and a ``dq version'' on the bottom. In short, theorem \ref{theo:1p_ode_solution} shows that the system in time domain, as a red block, can be converted into a dq version (blue block) by converting the input signal $\mathbf{f}(t)$ into its ``dq'' version $\mathbf{f}_{dq}(t)$, then processed in the dq frame, and which response $\mathbf{x}_{dq}(t)$ is reconstructed into its time counterpart through the inverse transformation $\mathbf{T}^{-1}_{\psi(t)}$.

	One very important note about theorem \ref{theo:1p_ode_solution} comes about initial conditions. In its presented form, the theorem supposes that the ``dq equivalent'' system \eqref{eq:theo_1p_ode_solution_dq_equiv} is such that the initial conditions of $\mathbf{z}_{dq}$ are arbitrary, that is, the initial conditions of $\mathbf{z}_{dq}$ and its derivatives up to the $(n-1)$-th of the equivalent dq system do not need to reconstruct the initial conditions of the original system $x_0,x'_0,...,x^{(n-1)}_0$. In this case, $\mathbf{z}_{dq}$ reconstructs $x(t)$ with fading exponential precision, as per \eqref{eq:theo_1p_ode_solution_exp}.

	This confusing arbitrariness in the initial conditions of the dq equivalent system is needed because it is often interesting to have this system not start exactly from the same conditions as the original system. Such necessity will become more apparent later. It is, however, immediate to note that if the initial conditions of $\mathbf{z}_{dq}$ and its derivatives reconstruct the initial conditions of $x(t)$, then  $\left\lVert \mathbf{x}_{\alpha\beta} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq}\right\rVert  = 0$ at initial time. Because in Hurwitz linear systems the only stability possible is the exponential, the difference beween the general solution $x(t)$ and the particular solution $\mathbf{T}^{-1}_\psi\mathbf{z}_{dq}$ can only decrease in time and, since this distance is null at initial time, it then remains null for all subsequent time instants. In other words, if the initial conditions of the dq system are chosen \textit{just right}, then its solution is exactly $\mathbf{x}_{dq}$, and $\mathbf{T}^{-1}_\psi\mathbf{x}_{dq}$ reconstructs $x(t)$ in time \textbf{loslessly}.

%-------------------------------------------------
\subsection{Complexification of LTI ODEs with phasorial forcing}\label{subsec:complexification} %<<<2
	
	We now want to use the complexification operator $\rho$ to escalate the results of theorem \ref{theo:1p_ode_solution} to the Dynamic Phasor $X(t)$ of the sinusoid $x(t)$. First it is shown that the $dq$ transform $\mathbf{T}_{\psi(t)}$, and its particularizations $\mathbf{G}_k$, are equivalent to rotations on the Dynamic Phasor space. This is shown by theorem \ref{theo:complex_space_operations} which proves that any countersymmetric matrix like $\mathbf{T}_\psi(t)$ is equivalent to a rotation in the complex domain.

\begin{theorem}[$dq$ and complex space operations] \label{theo:complex_space_operations}%<<<
	Consider $\mathbf{x}\in\left[\mathbb{R}\to\mathbb{R}^2\right]$, and $X \simeq \mathbf{x}$ its Dynamic Phasor representation. Take a countersymmetric matrix $\mathbf{A}\in\mathbb{R}^{2\times 2}$, that is, a matrix defined as

\begin{equation} \mathbf{A} = \left[\begin{array}{cc} a & -b \\ b & a \end{array}\right],\ a,b \in\mathbb{R} \label{eq:countersymmetric_matrix}. \end{equation}

	Then,

\begin{equation} \mathbf{A}\mathbf{x} \simeq \left(a + jb\right)X = Me^{j\phi} X \end{equation}

	\noindent where $M = \sqrt{\det\left(\mathbf{A}\right)} = \left\lvert a + jb\right\rvert = \sqrt{a^2 + b^2}$ and $\phi = \arg\left(a + jb\right)$.  Particularly, this implies the $\mathbf{T}_\psi(t)$ operator in the $\alpha\beta$ space is equivalent to a rotation by $-\psi(t)$ on the complex space, that is, a multiplication by $e^{-j\psi(t)}$:

\begin{equation} \mathbf{T}_{\psi(t)} \mathbf{x} \simeq e^{-j\psi(t)} X \end{equation}

	\noindent and also that the $\mathbf{G}_k$ operator in the $\alpha\beta$ space is equivalent to a rotation by $j^k$on the complex space:

\begin{equation} \mathbf{G}_k \mathbf{x} \simeq j^kX \end{equation}

\end{theorem}
\textbf{Proof:} calculate $\mathbf{y}$ such that

\begin{equation}
	\mathbf{y} = \mathbf{A}\mathbf{x} = M\left[\begin{array}{cc} a & -b \\ b & a \end{array}\right]\left[\begin{array}{c} x_d \\[3mm] x_q\end{array}\right] = \left[\begin{array}{c} ax_d - bx_q \\[3mm] ax_d + bx_q \end{array}\right]
\end{equation}

	At the same time, consider the complex number

\begin{equation} Y = \left(a + jb\right) X = \left(a + jb\right)\left(x_d + jx_q\right) = \left(ax_d - bx_q\right) + j\left(bx_d + ax_q \right) \end{equation}

	Meaning $Y = \left[1,j\right] \mathbf{y}$ and $\mathbf{y} = \left[\Re\left(Y\right),\Im\left(Y\right)\right]^\intercal$, therefore $Y \simeq  \mathbf{y}$. The results for $\mathbf{T}_{\psi}$ and $\mathbf{G}_k$ follows immediately adopting $\mathbf{A} = \mathbf{T}_{\psi}$ or $\mathbf{G}_k$. The polar form follows once $\mathbf{A}$ is written as

\begin{equation} \mathbf{A} = \sqrt{a^2 + b^2} \left[\begin{array}{cc} \dfrac{a}{\sqrt{a^2+b^2}} & \dfrac{b}{\sqrt{a^2+b^2}} \\[5mm] \dfrac{-b}{\sqrt{a^2+b^2}} & \dfrac{a}{\sqrt{a^2+b^2}} \end{array}\right] = M\left[\begin{array}{cc} \cos\left(\phi\right) & -\sin\left(\phi\right) \\[3mm] \sin\left(\phi\right) & \cos\left(\phi\right) \end{array}\right]  \end{equation}

\hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	Theorem \ref{theo:complex_space_operations} is a direct reflex of the fact that any countersymmetric matrix $\mathbf{A}$ as in \eqref{eq:countersymmetric_matrix} is diffeomorphic to a complex number $a + jb$. As a matter of fact the entire complex numbers can be constructed as a set of such matrices \pcite{ahlfors1979complex}. We now want to prove that the complexification operator $\rho$ maintains the differentiation operation, that is, if a signal $x(t)$ is represented by a Dynamic Phasor $X(t)$, then the derivatives of both signals are also related, that is, $x^{(k)}(t)$ is represented by $X^{(k)}(t)$ with $k\geq 1$.

\begin{theorem}[Invariancy of differentiation under the complex equivalence operator] \label{corollary:invariance_differentiation} %<<<
	The differentiation operation is invariant under the complex equivalence operator $\rho$, that is,

\begin{equation} \mathbf{D}^k_\mathbb{C}\left[\rho\left[\mathbf{x}\right]\right] = \rho\left[\mathbf{D}^k_{\mathbb{R}^2}\left[\mathbf{x}\right]\right]  \text{ for any } k\in\mathbb{N} \end{equation}

	\noindent or in shorter version,

\begin{equation} \mathbf{x} \simeq X \Leftrightarrow \dfrac{d^k\mathbf{x}}{dt^k} \simeq \dfrac{d^k X}{dt^k} \text{ for any } k\in\mathbb{N}\end{equation}
\end{theorem}
\textbf{Proof:} let $\mathbf{D}_{\mathbb{R}}$ denote the differential operator on $\mathbb{R}$, $\mathbf{D}_{\mathbb{C}}$ denote the simple derivative on the complex numbers, and $\mathbf{D}_{\mathbb{R}^2}$ the operator on $\mathbb{R}^2$, that is,

\begin{equation} \dfrac{d\mathbf{x}(t)}{dt} = \mathbf{D}_{\mathbb{R}^2}\left[\mathbf{x}\left(t\right)\right] = \left[\begin{array}{c} \mathbf{D}_{\mathbb{R}}\left[x_d(t)\right] \\[5mm] \mathbf{D}_{\mathbb{R}}\left[x_q(t)\right] \end{array}\right]. \end{equation}

	Two proofs are possible. The first uses functional analysis: by the Chain Rule on Banach Spaces, for some $\mathbf{x}\in\left[\mathbb{R}\to\mathbb{R}^2\right]$,

\begin{equation} \mathbf{D}_\mathbb{C}\left[\rho\left[\mathbf{x}\right]\right] = \mathbf{D}_\mathbb{C}\left[\rho \circ \mathbf{x}\right] = \dfrac{\delta \rho\left[\mathbf{x}\right]}{\delta \mathbf{x}}\left[\mathbf{D}_{\mathbb{R}^2}\left[\mathbf{x}\right]\right]. \end{equation}

	By theorem \ref{theo:rho_diff_inf}, the variational derivative $\delta\rho\left[\mathbf{x}\right]$ is equal to $\rho$ itself:

\begin{equation} \mathbf{D}_\mathbb{C}\left[\rho\left[\mathbf{x}\right]\right] = \rho\left[\mathbf{D}_{\mathbb{R}^2}\left[\mathbf{x}\right]\right], \end{equation}

	\noindent proving the proposition for $n=1$. For an arbitrary $n\in\mathbb{N}$ the process is the same, noting that the $n$-th derivative $\delta\rho\left[\mathbf{x}\right]$ always exists due to the infinitely diffeomorphic nature of $\rho$, is linear by definition and equal to $\rho$ itself. The second proof is done by simple inspection and induction. We can directly compute that $\rho$ applied to the derivative $d\mathbf{x}/dt$ is equivalent to a functional $\rho\left[D\left[\mathbf{x}\right]\right]$ combined:

\begin{equation} \rho\left(\dfrac{d\mathbf{x}}{dt}\right) = \rho\left[\mathbf{D}_{\mathbb{R}^2}\left[\mathbf{x}\right]\right] = \left(\rho\circ \mathbf{D}_{\mathbb{R}^2}\right)\left[\mathbf{x}\right]\end{equation}

	For a complex function $z(t) = x(t) + jy(t)\in\left[\mathbb{R}\to\mathbb{C}\right]$ where $x,y\in\left[\mathbb{R}\to\mathbb{R}\right]$, 

\begin{equation} \mathbf{D}_{\mathbb{C}}\left[z(t)\right] = \mathbf{D}_{\mathbb{R}}\left[x(t)\right] + j\mathbf{D}_{\mathbb{R}}\left[y(t)\right] .\end{equation}

	Then

\begin{equation} \dfrac{d}{dt}\left(\rho\left[\mathbf{x}\right]\right) = \mathbf{D}_{\mathbb{C}}\left[\rho\left[\mathbf{x}\right]\right] = \left(\mathbf{D}_{\mathbb{C}}\circ \rho\right)\left[\mathbf{x}\right] .\end{equation}

	We first show the proposition for $k=1$, that is, that the differentiation operator $D$ and the complexification operator $\rho$ commute, that is,

\begin{equation} \left(\rho\circ \mathbf{D}_{\mathbb{R}^2}\right)\left[\mathbf{x}\right]\equiv \left(\mathbf{D}_{\mathbb{C}}\circ \rho\right)\left[\mathbf{x}\right]\end{equation}

	And this can be done by a direct calculation:

\begin{equation} \left(\rho\circ \mathbf{D}_{\mathbb{R}^2}\right)\left[\mathbf{x}\right] = \left[1,j\right]\left[\begin{array}{c} \mathbf{D}_{\mathbb{R}}\left[x_d(t)\right] \\[5mm] \mathbf{D}_{\mathbb{R}}\left[x_q(t)\right] \end{array}\right] = \mathbf{D}_{\mathbb{R}}\left[x_d(t)\right] + j\mathbf{D}_{\mathbb{R}}\left[x_q(t)\right]\end{equation}

	\noindent but at the same time

\begin{equation} \left( \mathbf{D}_{\mathbb{C}}\circ\rho\right)\left[\mathbf{x}\right] = \mathbf{D}_{\mathbb{C}}\left[x_d(t) + jx_q(t)\right]  = \mathbf{D}_{\mathbb{R}}\left[x_d(t)\right] + j\mathbf{D}_{\mathbb{R}}\left[x_q(t)\right] \end{equation}

	\noindent proving both operators are equivalent. The next step is proving that the commutation of $D$ and $\rho$ is maintained throughout the differentiation orders, that is,

\begin{equation} \left(\rho\circ \mathbf{D}^k_{\mathbb{R}^2}\right)\left[\mathbf{x}\right]\equiv \left(\mathbf{D}^k_{\mathbb{C}}\circ \rho\right)\left[\mathbf{x}\right]\end{equation}

	\noindent for any natural $k$. This can be done by induction. The base case $k=1$ has been proven. For the inductive hypothesis, suppose the statement holds for a $k$. Then

\begin{equation} \left(\rho\circ \mathbf{D}^{\left(k+1\right)}_{\mathbb{R}^2}\right) = \left(\rho\circ \left(\mathbf{D}^k_{\mathbb{R}^2}\circ \mathbf{D}_{\mathbb{R}^2}\right)\right)  .\end{equation} 

	But since function composition is associative,

\begin{equation} \left(\rho\circ \left(\mathbf{D}^k_{\mathbb{R}^2}\circ \mathbf{D}_{\mathbb{R}^2}\right)\right) = \left(\left(\rho\circ \mathbf{D}^k_{\mathbb{R}^2}\right)\circ \mathbf{D}_{\mathbb{R}^2}\right)  \end{equation}

	\noindent and using the inductive hypothesis,

\begin{equation} \left(\left(\rho\circ \mathbf{D}^k_{\mathbb{R}^2}\right)\circ \mathbf{D}_{\mathbb{R}^2}\right) = \left(\left( \mathbf{D}^k_\mathbb{C}\circ\rho\right)\circ \mathbf{D}_{\mathbb{R}^2}\right) \end{equation}

	\noindent and again using association,

\begin{equation} \left(\left( \mathbf{D}^k_\mathbb{C}\circ\rho\right)\circ \mathbf{D}_{\mathbb{R}^2}\right) = \left( \mathbf{D}^k_\mathbb{C}\circ \left(\rho\circ \mathbf{D}_{\mathbb{R}^2}\right)\right) .\end{equation}

	Now using that the property is knowingly true for $k=1$,

\begin{equation} \left( \mathbf{D}^k_\mathbb{C}\circ \left(\rho\circ \mathbf{D}_{\mathbb{R}^2}\right)\right) = \left( \mathbf{D}^k_\mathbb{C}\circ \left(\mathbf{D}_\mathbb{C}\circ\rho\right)\right) = \left(\left( \mathbf{D}^k_\mathbb{C}\circ \mathbf{D}_\mathbb{C}\right)\circ\rho\right) = \left( \mathbf{D}^{\left(k+1\right)}_\mathbb{C}\circ\rho\right) \end{equation}

	\noindent which then proves that

\begin{equation} \left(\rho\circ \mathbf{D}^{\left(k+1\right)}_{\mathbb{R}^2}\right) = \left( \mathbf{D}^{\left(k+1\right)}_\mathbb{C}\circ\rho\right) \end{equation} 

	\noindent and the proposition is proven by induction. In shorter equivalence notation,

\begin{equation} \mathbf{x} \simeq X \Rightarrow \dfrac{d^k\mathbf{x}}{dt^k} \simeq \dfrac{d^kX}{dt^k},\ k\in\mathbb{N}. \end{equation}

	The converse implication is immediate once one notices that $X(t)$ necessarily reconstructs $x(t)$, including its initial conditions; therefore, if $d^kX/dt^k \simeq d^k\mathbf{x}/dt^k$ then one can integrate $d^kX/dt^k$ a number of $k$ times, using those initial conditions, to obtain $\mathbf{x}$ directly. Alternatively, one can repeat this theorem proof for $\rho^{-1}$, and the proof is identical. \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm} %>>>

	Therefore, the diffeomorphic nature of the complexification operator means that it makes possible to transform the real differential equation of an electrical grid on the variable $\mathbf{x}$, the $dq$ transform of a phasorial quantity, onto a complex differential equation on its complex version $X$. Theorem \eqref{theo:1p_ode_solution} proves that the time differential equation of the electrical grid of the form \eqref{eq:theo_1p_ode_solution_original_ode} can be transformed into an ODE for the $dq$ transformed version \eqref{eq:theo_1p_ode_solution_dq_equiv}. The objective is to use the complexification operator and its properties to show that the dq-equivalent equation is also equivalent to a differential equation in the Dynamic Phasor complex space.

\begin{theorem}[Complex equivalence of phasorially excited LTI ODEs]\label{corollary:complex_equivalence_phasorialodes} %<<<

	Take the LTI ODE \eqref{eq:theo_1p_ode_solution_original_ode} of theorem \ref{theo:1p_ode_solution}, the same apparent frequency $\omega(t)$ signal, and the dq-equivalent ODE to the complex differential equation \eqref{eq:theo_1p_ode_solution_dq_equiv}. Consider the complex differential equation

\begin{equation} \sum\limits_{i=0}^n \beta_i^n(t) Z^{(i)} - F = 0,\ Z(t) = z_d(t) + jz_q(t), \label{eq:theo_1p_ode_equivalent_complex_ode} \end{equation}

	\noindent equipped with	initial conditions $Z_0,Z'_0,Z''_0,...,Z^{(n-1)}_0$ calculated from the initial conditions of the dq system as

\begin{equation} Z_0 = z_{d0} + jz_{q0},\ Z'_0 = z'_{d0} + jz'_{q0},\ ...\ ,Z^{(n-1)}_0 = z^{(n-1)}_{d0} + jz^{(n-1)}_{q0}. \end{equation}
	
	\noindent where $F = \mathbf{P_D^\omega}\left[f\right]$ is the Dynamic Phasor Transform of the forcing $f(t)$, and the $\beta_i^n(t)$ are time-varying complex coefficients given by

\begin{equation} \beta_i^n(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} j^cB_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right].  \end{equation}

	\noindent Then $z(t) = \mathbf{P_D^{\left(-\omega\right)}}\left[Z\right]$ and there exist $a,b\in\mathbb{R}^+$ such that

\begin{equation} \left\lVert x(t) - \mathbf{P_D^{\left(-\omega\right)}}\left[Z\right] \right\rVert \leq ae^{-bt} ,\end{equation} 

	\noindent or, in other words, the solution $z(t)$ reconstructed by $\mathbf{P_D^{\left(-\omega\right)}}\left[Z\right]$ is the globally steady state exponentially stable solution of the original LTI ODE. Particularly, if the initial conditions of $Z(t)$ reconstruct the initial conditions of $x(t)$ at initial time, that is,

\begin{equation} Z_0 = x_{d0} + jx_{q0},\ Z'_0 = x'_{d0} + jx'_{q0},\ ...\ ,Z^{(n-1)}_0 = x^{(n-1)}_{d0} + jx^{(n-1)}_{q0}. \end{equation}

	\noindent then $Z(t) = X(t)$, that is, \eqref{eq:theo_1p_ode_equivalent_complex_ode} reconstructs $x(t)$ loslessly.
\end{theorem}
\textbf{Proof:} continuing from theorem \ref{theo:1p_ode_solution}, pick the dq equivalent system
\begin{equation} \sum\limits_{i=0}^n \mathbf{K}_i(t) \left(\dfrac{d^i \mathbf{z}_{dq}}{dt^i }\right) - \mathbf{f}_{dq} = 0 , \label{theo:1p_ode_solution_2}\end{equation}

	\noindent where

\begin{equation} \mathbf{K}_i(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right] .\end{equation}

	We first note that the $\mathbf{K}_i(t)$ matrices are countersymmetric, because they are composed of compositions of the $\mathbf{G}_k$, which are countersymmetric, multiplied by the $\alpha_k$ and the $B_{\left(k-i,c\right)}$ — which are one-dimensional numbers. As such, we can use theorem \ref{theo:complex_space_operations}; applying $\rho$ to \eqref{theo:1p_ode_solution_2} and using the linearity of $\rho$,

\begin{equation} \rho\left[\sum\limits_{i=0}^n \mathbf{K}_i\mathbf{z}^{(i)}_{dq} - \mathbf{f}_{dq}\right] = 0 \Leftrightarrow \sum\limits_{i=0}^n \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] - \rho\left[\mathbf{f}_{dq}\right] = 0 . \label{eq:complex_equiv_odes_4}\end{equation}

	Denote  $\rho\left[\mathbf{f}_{dq}\right] = F(t)$ and the theorem resumes to calculating $\rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right]$. Direct computation yields

\begin{equation} \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] = \rho\left[\left\{\sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right]\right\}\mathbf{z}^{(i)}_{dq}\right] .\end{equation}

	Now using the linearity of matrix and scalar multiplications,

\begin{equation} \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] = \rho\left[\sum\limits_{k=i}^{n} \left[ \sum\limits_{c=0}^{k-i} \alpha_k{k\choose i} B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \mathbf{G}_c\mathbf{z}^{(i)}_{dq}\right] \right].\end{equation}

	Again using the linearity of $\rho$, this functional can act inside the sums and the scalar portion can be noted outside its application:

\begin{equation} \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] = \sum\limits_{k=i}^{n} \left[ \sum\limits_{c=0}^{k-i} \alpha_k{k\choose i} B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right)  \rho\left[\mathbf{G}_c\mathbf{z}^{(i)}_{dq}\right] \right]. \label{eq:complex_equiv_odes_1}\end{equation}

	Now we use theorem \ref{theo:complex_space_operations} to yield that

\begin{equation} \rho\left[\mathbf{G}_c\mathbf{z}^{(i)}_{dq}\right] = j^c \rho\left[\mathbf{z}^{(i)}_{dq}\right] \label{eq:complex_equiv_odes_5}\end{equation}

	\noindent and by theorem \ref{corollary:invariance_differentiation}, $\rho\left[\mathbf{z}^{(i)}_{dq}\right] = Z^{(i)}$ and \eqref{eq:complex_equiv_odes_5} is equal to

\begin{equation} \rho\left[\mathbf{G}_c\mathbf{z}^{(i)}_{dq}\right] = j^c \rho\left[\mathbf{z}^{(i)}_{dq}\right] = j^c Z^{(i)}.\end{equation}

	Substituting this into \eqref{eq:complex_equiv_odes_1},

\begin{equation} \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] = \sum\limits_{k=i}^{n} \left[ \sum\limits_{c=0}^{k-i} \alpha_k{k\choose i} B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right)  j^c Z^{(i)} \right], \label{eq:complex_equiv_odes_2}\end{equation}

	\noindent and now because both $\alpha_k$ and $Z^{(i)}$ are not indexed by $c$, they can transcend the inner sum:

\begin{equation} \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] = \sum\limits_{k=i}^{n} \alpha_k{k\choose i}\left[ \sum\limits_{c=0}^{k-i} j^c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right)\right] Z^{(i)}. \label{eq:complex_equiv_odes_3}\end{equation}

	Let

\begin{equation} \beta_n^k(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i}\left[ \sum\limits_{c=0}^{k-i} j^c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right)\right] \label{eq:complex_equiv_odes_6}\end{equation}

	\noindent thus 

\begin{equation} \rho\left[\mathbf{K}_i\mathbf{z}^{(i)}_{dq}\right] = \beta_i^n(t) Z^{(i)}\end{equation}

	\noindent substituting this into \eqref{eq:complex_equiv_odes_4},

\begin{equation} \sum\limits_{i=0}^n \beta_i^n(t) Z^{(i)} - F(t) = 0 \end{equation}

	\noindent and, from the main result \eqref{eq:theo_1p_ode_solution_exp} of theorem \ref{theo:1p_ode_solution}, since $z(t) = \mathbf{P_D^{\left(-\omega\right)}}\left[Z\right]$,

\begin{equation} \left\lVert x(t) - \mathbf{P_D^{\left(-\omega\right)}}\left[Z\right] \right\rVert \leq ae^{-bt} .\end{equation} 

	Finally, consider that the initial conditions of $Z(t)$ reconstruct $x(0)$, that is,

\begin{equation} Z_0 = x_{d0} + jx_{q0},\ Z'_0 = x'_{d0} + jx'_{q0},\ ...\ ,Z^{(n-1)}_0 = x^{(n-1)}_{d0} + jx^{(n-1)}_{q0}. \end{equation}

	Then, at time $t = 0$, $\lVert x^{(k)}(t) - \mathbf{P_D^{\left(-\omega\right)}}\left[Z^{(k)}\right] \rVert = 0$. But because in a Hurwitz-stable linear system the distance from the general solution $x(t)$ and the particular solution $\mathbf{P_D^{\left(-\omega\right)}}\left[Z\right] $ can only decrease, because it is exponentially assymptotic, this yields

\begin{equation} \left\lVert x(t) - \mathbf{P_D^{\left(-\omega\right)}}\left[Z\right] \right\rVert  = 0 \end{equation} 

	\noindent for all time instants $t\geq 0$; therefore, $x(t) = \mathbf{P_D^{\left(-\omega\right)}}\left[Z\right]$ at all time instants. Because $\mathbf{P_D}$ is bijective, this yields $Z(t) = X(t)$. \hfill$\blacksquare$

\vspace{3mm}
\hrule
\vspace{3mm}

%>>>

% "COMPLEXIFIED" SYSTEM <<<
\begin{figure} 
\centering
\tikzexternaldisable
\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]

\node[stewartpink] at (0,0) (signalinput) {$f(t)$};
\node [draw, stewartpink, minimum width=35mm, very thick, minimum height=2cm, below=15mm of signalinput] (system_block) {$\displaystyle\sum_{k=0}^n \alpha_k x^{(k)} - f(t) = 0$};
\draw[->, stewartpink] (signalinput.south) -- ([shift=({0,1mm})]system_block.north);
\draw[->, stewartpink] (system_block.south) -- ([shift=({0, -15mm})]system_block.south) node[below] (signaloutput) {$x(t)$};

%------------------ DQ SYSTEM
\node [draw, stewartgreen, minimum width=35mm, very thick, minimum height=2cm, right=15mm of system_block] (system_dq_block) {$\displaystyle\sum_{i=0}^n \mathbf{K}_i \mathbf{x}^{(i)}_{dq} - \mathbf{f}_{dq} = \mathbf{0}$};

\node [draw, stewartgreen, minimum width=10mm, very thick, minimum height=10mm, above=10mm of system_dq_block.north] (tpsi_input) {$\mathbf{T}_\psi$};
\draw[->, stewartgreen] ([shift=({-5mm,0})]tpsi_input.west) node[left] {$\omega(t)$} -- ([shift=({-1mm,0})]tpsi_input.west) ;

\node [draw, stewartgreen, minimum width=10mm, very thick, minimum height=10mm, above=10mm of tpsi_input.north] (ab_input) {$\alpha\beta$};

\node [above, stewartpink, minimum width=10mm, very thick, minimum height=10mm, above=10mm of ab_input.north] (signal_dq_input) {$f(t)$};

\draw[->, stewartpink] ([shift=({0,0})]signal_dq_input.south) -- ([shift=({0,1mm})]ab_input.north);
\draw[->, stewartgreen] (ab_input.south) -- ([shift=({0,1mm})]tpsi_input.north) node[midway,right] {$\mathbf{f}_{\alpha\beta}(t)$}; ;
\draw[->, stewartgreen] (tpsi_input.south) -- ([shift=({0,1mm})]system_dq_block.north) node[midway,right] {$\mathbf{f}_{dq}(t)$};

\node [draw, stewartgreen, minimum width=10mm, very thick, minimum height=10mm, below=10mm of system_dq_block.south] (tpsi_out) {$\mathbf{T}^{-1}_\psi$};
\draw[->, stewartgreen] ([shift=({-5mm,0})]tpsi_out.west) node[left] {$\omega(t)$} -- ([shift=({-1mm,0})]tpsi_out.west) ;

\node [draw, stewartgreen, minimum width=10mm, very thick, minimum height=10mm, below=10mm of tpsi_out.south] (ab_out) {$\left(\alpha\beta\right)^{-1}$};

\node [above, stewartpink, minimum width=10mm, very thick, minimum height=10mm, below=10mm of ab_out.south] (signal_dq_output) {$x(t)$};

\draw[->, stewartgreen] ([shift=({0,0})]system_dq_block.south) -- ([shift=({0,1mm})]tpsi_out.north) node[midway,right] {$\mathbf{x}_{dq}(t)$};
\draw[->, stewartgreen] (tpsi_out.south) -- ([shift=({0,1mm})]ab_out.north) node[midway,right] {$\mathbf{x}_{\alpha\beta}(t)$};
\draw[->, stewartpink] (ab_out.south) -- ([shift=({0,1mm})]signal_dq_output.north) node[midway,right] {};

%------------------ COMPLEXIFIED SYSTEM
\node [draw, stewartblue, minimum width=35mm, very thick, minimum height=2cm, right=15mm of system_dq_block] (system_complex_block) {$\displaystyle\sum_{i=0}^n \beta_i^n (t) X^{(i)} - F = 0$};

\node [draw, stewartblue, minimum width=10mm, very thick, minimum height=10mm, above=10mm of system_complex_block.north] (rho_complex_input) {$\rho$};

\node [draw,stewartgreen, minimum width=10mm, very thick, minimum height=10mm, above=10mm of rho_complex_input.north] (tpsi_complex_input) {$\mathbf{T}_\psi$};

\node [draw,stewartgreen, minimum width=10mm, very thick, minimum height=10mm, above=10mm of tpsi_complex_input.north] (ab_complex_input) {$\alpha\beta$};

\node [above, stewartpink, minimum width=10mm, very thick, minimum height=10mm, above=10mm of ab_complex_input.north] (signal_complex_input) {$f(t)$};

\draw[->, stewartpink] ([shift=({0,0})]signal_complex_input.south) -- ([shift=({0,1mm})]ab_complex_input.north);
\draw[->, stewartgreen] (ab_complex_input.south) -- ([shift=({0,1mm})]tpsi_complex_input.north) node[midway,right] {$\mathbf{f}_{\alpha\beta}(t)$}; ;
\draw[->, stewartgreen] (tpsi_complex_input.south) -- ([shift=({0,1mm})]rho_complex_input.north) node[midway,right] {$\mathbf{f}_{dq}(t)$};
\draw[->, stewartblue] (rho_complex_input.south) -- ([shift=({0,1mm})]system_complex_block.north) node[midway,right] {$F(t)$};

\draw[->,stewartgreen] ([shift=({-5mm,0})]tpsi_complex_input.west) node[left] {$\omega(t)$} -- ([shift=({-1mm,0})]tpsi_complex_input.west) ;

\node [draw, stewartblue, minimum width=10mm, very thick, minimum height=10mm, below=10mm of system_complex_block.south] (rho_complex_output) {$\rho^{-1}$};

\node [draw, stewartgreen, minimum width=10mm, very thick, minimum height=10mm, below=10mm of rho_complex_output.south] (tpsi_complex_output) {$\mathbf{T}^{-1}_\psi$};

\node [draw, stewartgreen, minimum width=10mm, very thick, minimum height=10mm, below=10mm of tpsi_complex_output.south] (ab_complex_output) {$\left(\alpha\beta\right)^{-1}$};

\node [below, stewartpink, minimum width=10mm, very thick, minimum height=10mm, below=10mm of ab_complex_output.south] (signal_complex_output) {$x(t)$};

\draw[->, stewartgreen] ([shift=({-5mm,0})]tpsi_complex_output.west) node[left] {$\omega(t)$} -- ([shift=({-1mm,0})]tpsi_complex_output.west) ;

\draw[->, stewartblue] ([shift=({0,0})]system_complex_block.south) -- ([shift=({0,1mm})]rho_complex_output.north) node[midway,right] {$X(t)$};
\draw[->, stewartgreen] (rho_complex_output.south) -- ([shift=({0,1mm})]tpsi_complex_output.north) node[midway,right] {$\mathbf{x}_{dq}(t)$};
\draw[->, stewartgreen] (tpsi_complex_output.south) -- ([shift=({0,1mm})]ab_complex_output.north) node[midway,right] {$\mathbf{x}_{\alpha\beta}(t)$};
\draw[->, stewartpink] (ab_complex_output.south) -- ([shift=({0,1mm})]signal_complex_output.north) node[midway,right] {};

%------------------- BRACES
\path[draw,stewartblue,decorate,decoration=brace] ([shift=({8mm,0})]ab_complex_input.north east) -- ([shift=({8mm,0})]rho_complex_input.south east) node[right,midway]{\hspace{1mm} $\mathbf{P_D^{\omega}}$};
\path[draw,stewartblue,decorate,decoration=brace] ([shift=({8mm,0})] rho_complex_output.north east) -- ([shift=({8mm,0})]ab_complex_output.south east -| rho_complex_output.north east) node[right,midway]{\hspace{1mm} $\mathbf{P_D^{\left(-\omega\right)}}$};

\draw[-{Stealth[inset=0mm,length=5mm,angle'=50]},stewartpink,line width=2mm] (system_block.east) -- ([shift=({-1mm,0})]system_dq_block.west) node[midway,above,yshift=2mm] {Th. \ref{theo:1p_ode_solution}};
\draw[-{Stealth[inset=0mm,length=5mm,angle'=50]},stewartgreen,line width=2mm] (system_dq_block.east) -- ([shift=({-1mm,0})]system_complex_block.west) node[midway,above,yshift=2mm] {Th. \ref{corollary:complex_equivalence_phasorialodes}};

\draw[dashed,stewartpink] (system_block.north east) -- (ab_input.north west);
\draw[dashed,stewartpink] (system_block.south east) -- (ab_out.south west);

\draw[dashed,stewartpink] (ab_input.north east) -- (ab_complex_input.north west);
\draw[dashed,stewartpink] (ab_out.south east) -- (ab_complex_output.south west);

\draw[dashed,stewartgreen] (system_dq_block.north east) -- (rho_complex_input.north west);
\draw[dashed,stewartgreen] (system_dq_block.south east) -- (rho_complex_output.south west);

\end{tikzpicture}
\tikzexternalenable

\caption
[Schematic of a linear system being transformed into a ``dq'' version and then into a Dynamic Phasor version]
{Schematic of a linear system being transformed into a ``dq'' version and then into a Dynamic Phasor version as per theorems \ref{theo:1p_ode_solution} and \ref{corollary:complex_equivalence_phasorialodes}. In {\color{stewartpink} pink} the original time-domain system which is transformed into the dq-equivalent version by theorem \ref{theo:1p_ode_solution}; in {\color{stewartgreen} green} the ``dq apparatus'' comprised of the $\alpha\beta$ and dq transforms needed. Through theorem \ref{corollary:complex_equivalence_phasorialodes} the dq system is converted into the complex ODE by means of the complexification functional $\rho$, and this process is noted in {\color{stewartblue} blue.} The tandem operations $\alpha\beta$-dq-$\rho$ are, by definition, the Dynamic Phasor Transform $\mathbf{P_D}$, and the inverse operations comprise the inverse transform. }
\label{fig:dqification}
\end{figure}
%>>>

	Figure \ref{fig:dqification} shows a schematization of theorems \ref{theo:1p_ode_solution} and \ref{corollary:complex_equivalence_phasorialodes}. In the figure, the original time-domain system in red is translated as a dq-equivalent system; this translation is offered by theorem \ref{theo:1p_ode_solution}, such that the time signal of the forcing $f(t)$ is transported to its dq version and then processed by the system, yielding the dq version of the output, which is then reversed to time domain. Following this, theorem \ref{corollary:complex_equivalence_phasorialodes} shows that this dq system is equivalent to a complexified system by further transforming $\mathbf{f}_{dq}$ into its complexification $F(t)$, processing this signal through the complex version of the system, yielding the Dynamic Phasor quantity of the output $X(t)$, which is then de-complexified to yield $x(t)$.

%-------------------------------------------------
\subsection{Discussion on theorem \ref{corollary:complex_equivalence_phasorialodes}}\label{subsec:discussion_complexification} %<<<2

	Several topics arise from theorem \ref{corollary:complex_equivalence_phasorialodes}. We start by again discuss the initial conditions of the equivalent complex system \eqref{eq:theo_1p_ode_equivalent_complex_ode} and why it is interesting to have arbitrary initial conditions. 

	In general the signals reconstructed from the phasorial equivalent ODE are not representative of the original signal $x(t)$ at initial time. This is true even for classical phasors: a revisitation of theorem \ref{theo:phasors_solutions} shows that the sinusoidal solution \eqref{eq:linear_ode_phasor_solution_1} is exponentially stable because the sinusoidal solution does not necessatily reconstruct the solution $x(t)$ of the original linear system \eqref{eq:linear_ode_phasor_solution_1}. If the sinusoidal solution and the original solution have the same initial conditions, then they are one and the same.

	In this regard, it is useful to consider that the phasorial system does not start at the same initial conditions than the original time-domain system. In Power Systems this is especially useful because, in general, the initial conditions of the electrical grid are calculated by Power Flow algorithms that calculates these initial conditions from active and reactive power balances in the grid; the initial conditions of the differential equations of the agents are calculated ``backwards'', that is, the initial conditions of the phasorial equations are do not reconstruct the initial conditions of the time-domain equations because the phasorial ones are set so as to comply with the Power Flow algorithms. If, however, the initial conditions of the phasorial system are the same than that of the original system, then the phasorial system reconstructs the original solution without any approximations or losses in time.

	Another question raised by theorem \ref{corollary:complex_equivalence_phasorialodes} is if this theorem generalizes classical phasors; at a first glance, if $\omega(t) = \omega_0$ constant then the theorem should fall back into its static counterpart. Indeed, if $\omega_0$ is constant and the forcing $F(t)$ is a constant phasor at $\omega_0$, then the complexified version \eqref{eq:theo_1p_ode_equivalent_complex_ode} becomes

\begin{equation} \sum\limits_{i=0}^n \beta_i^n(t) Z^{(i)} - F = 0,\ Z(t) = z_d(t) + jz_q(t), \beta_i^n = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} j^cB_{\left(k-i,c\right)}\left(\omega_0,0,0,\cdots,0\right) \right]  \end{equation}

	\noindent and immediately one notices that the $\beta_i^n$ are constant and not time-variant anymore. By the properties of the Bell Polynomials,

\begin{equation} B_{\left(k-i,c\right)}\left(\omega_0,0,...0\right) = \left\{\begin{array}{l} \omega_0^{k} \text{, if } k-i=c \\[2mm] 0 \text{, if otherwise} \end{array}\right. \end{equation}

	\noindent and the $\beta_i^n$ become

\begin{equation} \beta_i^n = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left(j\omega_0\right)^{(k-i)} .\end{equation}

	Particularly,

\begin{equation} \beta_0^n = \sum\limits_{k=0}^{n} \alpha_k {k\choose 0} \left(j\omega_0\right)^{k} = \sum\limits_{k=0}^{n} \alpha_k \left(j\omega_0\right)^{k} ,\end{equation}

	\noindent and the model becomes

\begin{equation} \overbrace{\sum\limits_{i=1}^n \beta_i^n Z^{(i)}}^{\text{Transient sum}} + \overbrace{\beta_0^n Z - F}^{\text{Static behavior}} = 0 .\end{equation}

	Quickly one notices that the portion $\beta_0^n Z - F$ is the ``static'' equation that would be obtained by static phasors theory, while the summation on the left is a transient behavior pertaining to initial conditions. This transient term certainly fades exponentially over time so that the equation

\begin{equation} \beta_0^n Z_\infty - F = 0 \Leftrightarrow \lim\limits_{t\to\infty} \left\lvert Z(t) - Z_\infty\right\rvert = 0 \text{ (exp.)},\end{equation}

	\noindent where ``(exp.)'' means exponential tendency, describes the assymptotic behavior of $Z$ — that is, $Z$ tends to a constant static phasor $Z_\infty$ which naturally reconstructs a static sinusoid. Particularly, if we assume $Z$ is a static phasor, then the transient sum is identically null and $Z(t) = Z_\infty$. Thus, theorem \ref{corollary:complex_equivalence_phasorialodes} is a generalization of the classical phasors theorem \ref{theo:phasors_solutions_reproof}.

	These results also beg the question that if $\omega(t)$ is not exactly constant but ``almost constant'', then the static behavior still approximates the steady-state solution of the phasorial equivalent system. The answer is yes: chapter \ref{chapter:choice_apparent_frequency} proves in section \ref{sec:qsh_proof} that if the circuit is much quicker than $\omega_0$, then the behavior of the phasorial equivalent model is sufficiently approximated by the static approximation, that is,

\begin{equation} Z_a = \dfrac{F}{\beta_0^n} = \dfrac{F}{\sum\limits_{k=0}^{n} \alpha_k \left(j\omega_0\right)^{k}} \end{equation}

	\noindent sufficiently approximates $Z(t)$ with a precision that gets better as the circuit gets ``quicker'' and/or the frequency $\omega(t)$ is ``slower''. In this context, a ``slow'' frequency means that it is close to a constant $\omega_0$ that is sufficiently small, and a ``fast'' circuit means that the Hurwitz Polynomial of the original time-domain circuit

\begin{equation} H(x) = \sum\limits_{k=0}^n \alpha_k x^k \end{equation}

	\noindent is such that its roots have negative yet large real parts.	

\begin{example}[Application of theorem \ref{corollary:complex_equivalence_phasorialodes}] \label{example:rlc_dpt} %<<<

	Consider the RLC circuit of figure \ref{fig:complexification_example}, comprised of a RLC circuit fed by a voltage $v(t)$. Suppose that the circuit is excited by a nonstationary voltage

% MODELLING EXAMPLE: RLC CIRCUIT <<<
\begin{figure}[htb!]
\centering
        \begin{tikzpicture}[american,scale=1,transform shape,line width=0.75, cute inductors, voltage shift = 1]
	\ctikzset{/tikz/circuitikz/voltage/distance from node=10mm}
		\draw (0,0)
			to[vsource,sources/scale=1.25, v>=$v(t)$,invert] (0,4)
			to[L,l=$L$,f>^=$i_{L}$,v>=$v_{L}$,-*] (4,4) 
			to[C,l=$C$,f>^=$i_{C}$,v>=$v_{C}$,-*] (4,0) 
			to[short] (0,0); 
		\draw (4,4)
			to[short,f>^=$i_{R}$] (7,4) 
			to[R,l=$R$,v>=$v_{R}$] (7,0) 
			to[short]  (4,0);
        \end{tikzpicture}
	\caption{Second-order circuit for example application of theorem \ref{corollary:complex_equivalence_phasorialodes}.}
	\label{fig:complexification_example}
\end{figure} %>>>

\begin{equation} v(t) = m_v(t)\cos\left(\psi(t)\right) \text{, with } \psi = \int_0^a \omega(a)da \text{, where } \omega(t) = \omega_0\left[1 + Me^{-\alpha t}\sin\left(\beta t\right)\right], \label{eq:example_voltage_freq_def}\end{equation}

 	\noindent modelling a base frequency $\omega_0$ that is transiently disturbed and stabilizes after some time. The frequency $\omega(t)$ of \eqref{eq:example_voltage_freq_def} yields an angle displacement

\begin{equation} \psi(t) = \omega_0\left(t + \dfrac{M\left\{\beta - e^{-\alpha t}\left[\alpha\sin\left(\beta t\right) + \beta\cos\left(\beta t\right)\right]\right\}}{\alpha^2 + \beta^2} \right) .\end{equation}

	This frequency signal was specifically chosen because its Fourier Series is given in terms of Bessel Functions of the first kind; more precisely,

\begin{equation} \mathbf{F}\left[e^{ja\sin\left(\omega_0 t\right)}\right] = \sum_{n\in\mathbb{Z}} J_n\left(a\right)e^{jn\omega_0 t} ,\end{equation}

	\noindent where $J_n$ represents the Bessel Function of first kind, n-th order. This means that constructing a Dynamic Phasor representation for $v(t)$ using either both Hilbert Transform and STFT techniques would be inexorably difficult; due to the inherent complex nature of Bessel Functions and the infinite terms, operationalizing this specific signal using these techniques would need some sort of approximation. This example shows that the Dynamic Phasor technique proposed in this thesis can easily deal with signals complicated as these without need for such approximations.

	The numerical values adopted are $\omega_0=120\pi$ rad/s, $R = 100\Omega,C = 1mF,L = 4mH, \alpha=5s^{-1},\beta=10\pi\ \text{rad}.s^{-1},M=0.1 $. The objective is to find the time signal $v_R(t)$ of the voltage over the resistive load $R$. First, apply Kirchoff's Voltage Law to the left loop and Kirchoff's Current Law to the center top node

\begin{equation} \left\{\begin{array}{l} -v(t) + v_L(t) + v_C(t) = 0 \\[3mm] v_C(t) - v_R(t) = 0 \\[3mm] i_L(t) - i_C(t) - i_R(t) = 0 \end{array}\right. .\end{equation}

	Using the current-voltage relationships of the components,

\begin{equation} \left\{\begin{array}{l} -v(t) + L\dot{i}_L(t) + v_C(t) = 0 \\[3mm] v_C(t) - v_R(t) = 0 \\[2mm] i_L(t) - C\dot{v}_C(t) - \dfrac{1}{R}v_R(t) = 0 \end{array}\right. .\end{equation}

	Substituting the second and third equations into the first,

\begin{equation} - \dfrac{1}{LC} v(t) + \ddot{v}_R(t) + \dfrac{1}{RC}\dot{v}_R(t) + \dfrac{1}{LC} v_R(t) = 0 . \label{eq:rlc_time_diffeq}\end{equation}

	Now we apply theorem \ref{corollary:complex_equivalence_phasorialodes}. Adopt the frequency signal of \eqref{eq:example_voltage_freq_def} and \eqref{eq:rlc_time_diffeq} is equivalent to

\begin{equation} \ddot{V}_R(t) + \dot{V}_R(t)\left(\dfrac{1}{RC} + 2j\omega(t)\right) + V_R\left\{ \dfrac{1}{LC}  -\omega^2(t) + j \left[ \dot{\omega}(t) + \dfrac{1}{RC}\omega(t)\right]\right\} -\dfrac{1}{LC} V(t) = 0, \label{eq:rlc_complex_diffeq}\end{equation}

	\noindent where $V(t) = \mathbf{P_D^{\omega}}\left[v\right]$, which is naturally $m(t)e^{j0}$. This differential equation was integrated in time and the resulting complex signal $V_R(t)$ is shown in Figure \ref{fig:amp_phase_voltage_signals}. To show the capability of the DPT to reconstruct time signals, Figure \ref{fig:voltage_signals} shows in blue the time signal obtained by integrating the original time ODE \eqref{eq:rlc_time_diffeq}, and in red the signal reconstructed from the Dynamic Phasor $V_R(t)$ obtained by integrating the complex equation \eqref{eq:rlc_complex_diffeq}, such that both time signal and complex signal have the same initial conditions. It is immediate to see that both signals are identical, highlighting that the DPT from $V_R(t)$ reconstructs the time signal $v_R(t)$ without losses..

	On the other hand, Figure \ref{fig:voltage_signals_perturbed} shows the same time signal from the original ODE in blue; in red, the signal reconstructed from the complex differential equation \eqref{eq:rlc_complex_diffeq}. In this case, however, the initial conditions are perturbed and do not match, as shown by the zoomed-in version of the simulation start period. The figure shows a zoomed-in version of the simulation final period, allowing to observer that, as per the theorem statement, the signal reconstructed from $V_R(t)$ indeed approaches $v(t)$ as time grows, even though the initial conditions are not the same. The

% AMPLITUDE PHASE TIME CURVES <<<
\begin{figure}
        \begin{center}
                \begin{tikzpicture}
                        \begin{axis}[
                                width = \columnwidth,
                                height = 1/1.618*\columnwidth,
                                title={Amplitude and phase signals from time and DP simulations with matching initial conditions},
                                xlabel={Time (s)},
                                %ylabel={$\left\lvert V_R(t)\right\rvert$ (V)},
				y axis line style = {red, thick},
				every y tick label/.append style ={red},
				every y tick/.append style ={thick, red},
                                xmin=0, xmax=1,
                                ymin=0, ymax=42,
                                xtick={0,0.1,...,1},
				ylabel style = {align=center},
				axis y line*=left,
                                every axis plot/.append style={thick},
				legend pos = north east
                        ]
                                \addplot[red,smooth] table[col sep=comma,header=false,x index=0,y index=3]{data/rlc_sim/data_rlc_sim_dps.csv};
				\addlegendentry{$\left\lvert V_R(t)\right\rvert$ (V)}
                        \end{axis}
%
                        \begin{axis}[
                                width = \columnwidth,
                                height = 1/1.618*\columnwidth,
                                xmin=0, xmax=1,
		        	axis y line*=right,
                               %ylabel={$\text{arg}\left(V_R(t)\right)$ ($\times \pi$ rad)},
				y axis line style = {blue, thick},
				every y tick label/.append style ={blue},
				every y tick/.append style ={thick, blue},
				ylabel near ticks,
                                ymin=-0.25, ymax=0.25,
                                xtick={0,0.1,...,1},
                                ytick={-0.2,-0.1,...,0.2},
				axis x line=none,
		        	ylabel style = {align=center},
                                every axis plot/.append style={thick},
				legend pos = south east
                        ]
                                \addplot[blue ,smooth] table[col sep=comma,header=false,x index=0,y expr = \thisrowno{4}/3.141596]{data/rlc_sim/data_rlc_sim_dps.csv};
				\addlegendentry{$\text{arg}\left(V_R(t)\right)$ ($\times\ \pi$ rad)}
                        \end{axis}
                \end{tikzpicture}
        \caption
	[Amplitude and phase signals of the Dynamic Phasor obtained by integrating the complex differential equation.]
	{Amplitude (red) and phase (blue) signals of the Dynamic Phasor $V_R(t)$ obtained by integrating the complex differential equation \eqref{eq:rlc_complex_diffeq}.}
        \label{fig:amp_phase_voltage_signals}
        \end{center}
\end{figure}
% >>>

% VOLTAGE TIME CURVES <<<
\begin{figure}
        \begin{center}
                \begin{tikzpicture}
                        \begin{axis}[
				name = ax_main,
                                width = 0.9*\columnwidth,
                                height = 0.9*1/1.618*\columnwidth,
                                title={Voltage signals from time and DP simulations with exact initial conditions},
                                xlabel={Time (s)},
                                ylabel={$v_R(t)$ and $\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$ (V)},
                                xmin=0, xmax=1,
                                ymin=-42, ymax=42,
                                xtick={0,0.1,...,1},
                                ytick={-40,-30,...,40}, 
                                legend pos=south east,
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                colormap name=hsv2,
                                cycle list={[ colors of colormap={100,200,300,400,500,600,700,800,900,1000} ]},
                                every axis plot/.append style={thick},
                        ]
                                \addplot[blue ,smooth] table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_rlc_sim_dps.csv};
                        \coordinate (c1) at (axis cs:0,-42);
                        \coordinate (c2) at (axis cs:0.1,-42);
                        \end{axis}
%
                        \begin{axis}[
                                name = ax_zoomed,
                                at={($(ax_main.north east)-(0.9\columnwidth,1.75/1.618*\columnwidth)$)},
                                width = 0.9*1\columnwidth,
                                height = 0.9*1/1.618*\columnwidth,
                                xmin=0, xmax=0.1,
                                ymin=-42, ymax=42,
                                xtick={0,0.01,...,0.1},
				xlabel={Time (ms)},
				xticklabels={$0$,$10$,$20$,$30$,$40$,$50$,$60$,$70$,$80$,$90$,$100$},
                                ytick={-40,-30,...,40},
				tick label style={/pgf/number format/fixed},
				legend columns=2,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}},
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                every axis plot/.append style={thick},
                                axis background/.style = {
                                        preaction = {
                                        path picture = {
                                        \draw[fill=white,line width=0mm] (axis cs:0,400) rectangle (axis cs:0.1,-40);
                                                }
                                        }
                                }
                        ]
				\addplot[blue, smooth]         table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_rlc_sim_dps.csv};
				\addlegendentry{$v_R(t)$}
				\addplot[red,  smooth, dashed, dash pattern=on 2pt off 4pt, line cap=round] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_rlc_sim_dps.csv};
				\addlegendentry{$\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$}
                        \end{axis}
                        % draw dashed lines from rectangle in first axis to corners of second
                        \draw [gray,dashed] (c1) -- (ax_zoomed.north west);
                        \draw [gray,dashed] (c2) -- (ax_zoomed.north east);
                \end{tikzpicture}
        \caption
[Voltage across the resistor of the circuit of Figure \ref{fig:complexification_example} using exact initial conditions.]
{Voltage across the resistor of the circuit of Figure \ref{fig:complexification_example} using exact initial conditions. In blue, the signal $v_R(t)$ obtained by integrating the time differential equation \eqref{eq:rlc_time_diffeq}. In red, the signal obtained by the inverse transform of the Dynamic Phasor $V_R(t)$ of the solution of the complex differential equation \eqref{eq:rlc_complex_diffeq}. Top plot shows only the blue line; bottom plot shows a zoomed-in version with both lines juxtaposed for comparison.}
        \label{fig:voltage_signals}
        \end{center}
\end{figure}
% >>>

% VOLTAGE TIME CURVES (PERTURBED) <<<
\begin{figure}
        \begin{center}
                \begin{tikzpicture}
                        \begin{axis}[
				name = ax_main,
                                width = 0.9*\columnwidth,
                                height = 0.9*1/1.618*\columnwidth,
                                title={Voltage signals from time and DP simulations with perturbed initial conditions},
                                xlabel={Time (s)},
                                ylabel={$v_R(t)$ and $\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$ (V)},
                                xmin=0, xmax=1,
                                ymin=-42, ymax=42,
                                xtick={0,0.1,...,1},
                                ytick={-40,-30,...,40}, 
                                legend pos=south east,
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                colormap name=hsv2,
                                cycle list={[ colors of colormap={100,200,300,400,500,600,700,800,900,1000} ]},
                                every axis plot/.append style={thick},
				legend columns=2,
                        ]
				\addplot[blue, smooth]         table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_rlc_sim_perturbed.csv};
				\addlegendentry{$v_R(t)$}
				\addplot[red,  smooth, dashed, dash pattern=on 4pt off 2pt, line cap=round] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_rlc_sim_perturbed.csv};
				\addlegendentry{$\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$}
                        \coordinate (c1) at (axis cs:0,-42);
                        \coordinate (c2) at (axis cs:0.1,-42);
%
                        \coordinate (c3) at (axis cs:0.9,-42);
                        \coordinate (c4) at (axis cs:1.0,-42);
                        \end{axis}
%
                        \begin{axis}[
                                name = ax_zoomed_start,
                                at={($(ax_main.north east)-(0.9\columnwidth,1.3/1.618*\columnwidth)$)},
                                width = 0.6*1\columnwidth,
                                height = 0.6*1/1.618*\columnwidth,
                                xmin=0, xmax=0.1,
                                ymin=-42, ymax=42,
                                xtick={0,0.01,...,0.1},
				xlabel={Time (ms)},
				xticklabels={$0$,$10$,$20$,$30$,$40$,$50$,$60$,$70$,$80$,$90$,$100$},
                                ytick={-40,-30,...,40},
				tick label style={/pgf/number format/fixed},
				legend columns=2,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}},
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                every axis plot/.append style={thick},
                                axis background/.style = {
                                        preaction = {
                                        path picture = {
                                        \draw[fill=white,line width=0mm] (axis cs:0,400) rectangle (axis cs:0.1,-40);
                                                }
                                        }
                                }
                        ]
				\addplot[blue, smooth]         table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_rlc_sim_perturbed.csv};
				\addplot[red,  smooth, dashed, dash pattern=on 4pt off 2pt, line cap=round] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_rlc_sim_perturbed.csv};
                        \end{axis}
                        % draw dashed lines from rectangle in first axis to corners of second
                        \draw [gray,dashed] (c1) -- (ax_zoomed_start.north west);
                        \draw [gray,dashed] (c2) -- (ax_zoomed_start.north east);
%
                        \begin{axis}[
                                name = ax_zoomed_final,
                                at={($(ax_main.north east)-(0.4\columnwidth,1.9/1.618*\columnwidth)$)},
                                width = 0.6*1\columnwidth,
                                height = 0.6*1/1.618*\columnwidth,
                                xmin=0.9, xmax=1,
                                ymin=-42, ymax=42,
                                xtick={0.90,0.91,...,1},
				xlabel={Time (ms)},
				xticklabels={$900$,$910$,$920$,$930$,$940$,$950$,$960$,$970$,$980$,$990$,$1000$},
                                ytick={-40,-30,...,40},
				tick label style={/pgf/number format/fixed},
				legend columns=2,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}},
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                every axis plot/.append style={thick},
                                axis background/.style = {
                                        preaction = {
                                        path picture = {
                                        \draw[fill=white,line width=0mm] (axis cs:0,400) rectangle (axis cs:0.1,-40);
                                                }
                                        }
                                }
                        ]
				\addplot[blue, smooth]         table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_rlc_sim_perturbed.csv};
				\addplot[red,  smooth, dashed, dash pattern=on 4pt off 2pt, line cap=round] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_rlc_sim_perturbed.csv};
                        \end{axis}
                        % draw dashed lines from rectangle in first axis to corners of second
                        \draw [gray,dashed] (c3) -- (ax_zoomed_final.north west);
                        \draw [gray,dashed] (c4) -- (ax_zoomed_final.north east);
                \end{tikzpicture}
        \caption
[Voltage across the resistor of the circuit of Figure \ref{fig:complexification_example} using perturbed initial conditions.]
{Voltage across the resistor of the circuit of Figure \ref{fig:complexification_example} using perturbed initial conditions. In blue, the signal $v_R(t)$ obtained by integrating the time differential equation \eqref{eq:rlc_time_diffeq}. In red, the signal obtained by the inverse transform of the Dynamic Phasor $V_R(t)$ of the solution of the complex differential equation \eqref{eq:rlc_complex_diffeq} which initial conditions are different that those of the time-domain equation.}
        \label{fig:voltage_signals_perturbed}
        \end{center}
\end{figure}
% >>>

	As a contrast, we can model the system using the Short-Time Fourier Transform. Using the differential property \eqref{sys:fdp_sys_fundamental_harmonic} onto the differential equation \eqref{eq:rlc_complex_diffeq_stft}, where the ``F'' subscript stands for ``Fourier'', highlighting that this equation uses the STFT. In this equation, as discussed in subsection \ref{subsec:infinite_complex_systems}, the capital letters denote the first harmonic of the signals they represent. Integrating this equation, and then using the inversion formula \eqref{eq:fourierSeries} yields a time signal that is plotted on figure \ref{fig:voltage_signals_stft} against the signal $\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$ reconstructed from the proposed DPT.

\begin{equation} \ddot{V}_{RF}(t) + \dot{V}_{RF}(t)\left(\dfrac{1}{RC} + 2j\omega(t)\right) + V_{RF}\left\{ \dfrac{1}{LC}  -\omega^2(t) + j \left[ \dot{\omega}(t) + \dfrac{1}{RC}\omega(t)\right]\right\} -\dfrac{1}{LC} V_F(t) = 0, \label{eq:rlc_complex_diffeq_stft}\end{equation}

% VOLTAGE TIME CURVES VS STFT <<<
\begin{figure}
        \begin{center}
                \begin{tikzpicture}
                        \begin{axis}[
				name = ax_main,
                                width = 0.9*\columnwidth,
                                height = 0.9*1/1.618*\columnwidth,
                                title={Voltage signals reconstructed from STFT and from $\mathbf{P_D}$},
                                xlabel={Time (s)},
                                ylabel={$\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$, $\mathbf{P_D^{\left(-\omega\right)}}\left[V_{RF}\right]$},
                                xmin=0, xmax=1,
                                ymin=-42, ymax=42,
                                xtick={0,0.1,...,1},
                                ytick={-40,-30,...,40}, 
                                legend pos=south east,
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                colormap name=hsv2,
                                cycle list={[ colors of colormap={100,200,300,400,500,600,700,800,900,1000} ]},
                                every axis plot/.append style={thick},
				legend columns=2,
                        ]
				\addplot[blue, smooth] table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_stft.csv};
				\addlegendentry{$\mathbf{P_D^{\left(-\omega\right)}}\left[V_R\right]$}
				\addplot[red, smooth, dashed, dash pattern=on 1pt off 1pt, line cap=round] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_stft.csv};
				\addlegendentry{$\mathbf{STFT^{-1}}\left[V_{RF}\right]$}
                        \coordinate (c1) at (axis cs:0,-42);
                        \coordinate (c2) at (axis cs:0.1,-42);
%
                        \coordinate (c3) at (axis cs:0.9,-42);
                        \coordinate (c4) at (axis cs:1.0,-42);
                        \end{axis}
%
                        \begin{axis}[
                                name = ax_zoomed_start,
                                at={($(ax_main.north east)-(0.9\columnwidth,1.3/1.618*\columnwidth)$)},
                                width = 0.6*1\columnwidth,
                                height = 0.6*1/1.618*\columnwidth,
                                xmin=0, xmax=0.1,
                                ymin=-42, ymax=42,
                                xtick={0,0.01,...,0.1},
				xlabel={Time (ms)},
				xticklabels={$0$,$10$,$20$,$30$,$40$,$50$,$60$,$70$,$80$,$90$,$100$},
                                ytick={-40,-30,...,40},
				tick label style={/pgf/number format/fixed},
				legend columns=2,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}},
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                every axis plot/.append style={thick},
                                axis background/.style = {
                                        preaction = {
                                        path picture = {
                                        \draw[fill=white,line width=0mm] (axis cs:0,400) rectangle (axis cs:0.1,-40);
                                                }
                                        }
                                }
                        ]
				\addplot[blue, smooth] table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_stft.csv};
				\addplot[red, smooth] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_stft.csv};
                        \end{axis}
                        % draw dashed lines from rectangle in first axis to corners of second
                        \draw [gray,dashed] (c1) -- (ax_zoomed_start.north west);
                        \draw [gray,dashed] (c2) -- (ax_zoomed_start.north east);
%
                        \begin{axis}[
                                name = ax_zoomed_final,
                                at={($(ax_main.north east)-(0.4\columnwidth,1.9/1.618*\columnwidth)$)},
                                width = 0.6*1\columnwidth,
                                height = 0.6*1/1.618*\columnwidth,
                                xmin=0.9, xmax=1,
                                ymin=-42, ymax=42,
                                xtick={0.90,0.91,...,1},
				xlabel={Time (ms)},
				xticklabels={$900$,$910$,$920$,$930$,$940$,$950$,$960$,$970$,$980$,$990$,$1000$},
                                ytick={-40,-30,...,40},
				tick label style={/pgf/number format/fixed},
				legend columns=2,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}},
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                every axis plot/.append style={thick},
                                axis background/.style = {
                                        preaction = {
                                        path picture = {
                                        \draw[fill=white,line width=0mm] (axis cs:0,400) rectangle (axis cs:0.1,-40);
                                                }
                                        }
                                }
                        ]
				\addplot[blue, smooth] table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/data_stft.csv};
				\addplot[red, smooth] table[col sep=comma,header=false,x index=0,y index=2]{data/rlc_sim/data_stft.csv};
                        \end{axis}
                        % draw dashed lines from rectangle in first axis to corners of second
                        \draw [gray,dashed] (c3) -- (ax_zoomed_final.north west);
                        \draw [gray,dashed] (c4) -- (ax_zoomed_final.north east);
                \end{tikzpicture}
        \caption
[Comparison of signals as reconstructed by the Dynamic Phasor Transform and as reconstructed by the STFT]
{Comparison of the voltage signals as reconstructed by the proposed Dynamic Phasor Transform (in blue) and as reconstructed by the Short-Time Fourier Transform (in red) by integrating \ref{eq:rlc_complex_diffeq_stft} and using the inversion formula \eqref{eq:fourierSeries}.}
        \label{fig:voltage_signals_stft}
        \end{center}
\end{figure}
% >>>

	Figure \ref{fig:voltage_signals_stft} then illustrates how the STFT is unable to deal with this system and the signals involved. Not only it clearly does not reconstruct the signal in the first time instants, as shown by the zoomed subplot, but it also fails to reconstruct the signal even at steady-state, showing a considerable angle difference.

\examplebar
\end{example} %>>>

%-------------------------------------------------
\section{Nonstationary Complex Power} %<<<1

	Finally, we now want to show that the proposed Dynamic Phasor Transform is able to beget the notion of complex power under nonstationary regimens. The idea is to show that the induced notion of complex power is nigh-identical to that found in static phasors, as per theorem \ref{theo:sfp_complex_apparent_power}.

\begin{theorem}[Generalized Complex Power]\label{theo:activereactivepower} %<<<
	Let $V = m_v(t)e^{j\phi_v(t)}$ and $I = m_i(t)e^{j\phi_i(t)}$ represent the dynamical phasors of the voltage across and current through a bipole and consider the quantity 

\begin{equation} S(t) = \frac{1}{2}\left<V(t),I(t)\right> = P(t) + jQ(t)\ \left\{\begin{array}{l} P(t) = \dfrac{m_v(t)m_i(t)}{2}\cos\left[\phi_v(t) - \phi_i(t)\right] \\[3mm] Q(t) = \dfrac{m_v(t)m_i(t)}{2}\sin\left[\phi_v(t) - \phi_i(t)\right] \end{array}\right. \label{eq:dynamical_complex_power_def}\end{equation}

	\noindent called \textbf{complex power}. Then $S(t)$ is such that the instantaneous power performed by the bipole can be calculated as

\begin{equation} p(t) = P(t) \left[1 + \cos\left(2\psi + 2\phi_v\right) \right] + Q(t) \sin\left(2\psi + 2\phi_v\right). \label{eq:dynamical_complex_instpower}\end{equation}

\end{theorem}
\textbf{Proof:} for \eqref{eq:dynamical_complex_instpower} and \eqref{eq:dynamical_complex_power_def}, write $p(t) = v(t)i(t)$:

\begin{equation} p(t) = m_v\cos\left(\psi(t) + \phi_v(t)\right)m_i\cos\left(\psi(t) + \phi_i(t)\right) \end{equation} 

	\noindent and denote $\Delta\phi(t) = \phi_v(t) - \phi_i(t)$. Then $\phi_v(t) + \phi_i(t) = 2\phi_v(t) - \Delta\phi(t)$; therefore using

\begin{equation} \cos(a)\cos(b) = \dfrac{1}{2}\left[\cos(a+b) + \cos(a-b)\right],\end{equation}

	\noindent one obtains

\begin{align} p(t) &= m_i(t) m_v(t) \dfrac{1}{2} \left[ \cos\left(2\psi(t) + \phi_v(t) + \phi_i(t)\right) + \cos\left(\phi_v(t) - \phi_i(t)\right)\right] \nonumber\\[3mm] &= \dfrac{m_i(t) m_v(t)}{2} \left\{\cos\left[2\left(\psi(t) + \phi_v(t)\right) - \Delta\phi(t)\right] + \cos\left[\Delta\phi(t)\right]\right\} \end{align}

	Using $\cos(a-b) = \cos(a)\cos(b) + \sin(a)\sin(b)$,

\begin{equation} p(t) = \dfrac{m_i(t) m_v(t)}{2} \left\{\raisebox{5mm}{} \cos\left(\Delta\phi(t)\right)\left\{\raisebox{3mm}{} 1 + \cos\left[2\left(\psi(t) + \phi_v(t)\right)\right]\right\} + \sin\left(\Delta\phi(t)\right)\sin\left[2\left(\psi(t) + \phi_v(t)\right)\right] \right\} . \label{eq:nonst_complex_apparent_power_eq1} \end{equation}

	Let

\begin{equation} P = \dfrac{m_i m_v}{2} \cos\left(\Delta\phi(t)\right),\ Q = \dfrac{m_i m_v}{2} \sin\left(\Delta\phi(t)\right) \end{equation}

	\noindent then

\begin{equation} p(t) = P\left\{1 + \cos\left[2\left(\psi(t) + \phi_v(t)\right) \right]\right\} + Q\sin\left[2\left(\psi(t) + \phi_v(t) \right)\right] . \label{eq:nononst_complex_apparent_power_eq2} \end{equation}
\hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm} %>>>

	Having now proven that the notions of active and reactive power induced by the Dynamic Phasor Transform are the same as the ones of static phasors, we must now prove that these new notions have the same physical meaning. We first prove that, also identically to static active power, this generalized notion of active power is such that $P(t)$ is the average power over some interval.

\begin{theorem}[Active Power as average power in some period]\label{theo:activepowerperiod} %<<<
	Let $V = m_v(t)e^{j\phi_v(t)}$ and $I = m_i(t)e^{j\phi_i(t)}$ represent the dynamical phasors of the voltage across and current through a bipole. Consider the equation on $T(t)$:

\begin{equation} \dfrac{1}{T(t)}\int_{t}^{t+T(t)} p(s)ds = P(t) \label{eq:theo_cp_power_avg_power_period_eq} \end{equation}

	Let $\left[t_0,t_f\right)$ be such that $m_v,m_i,\phi_v,\phi_i$ are defined, bounded with bounded derivatives, and there exists a positive solution $T_0 = T\left(t_0\right)$ for \eqref{eq:theo_cp_power_avg_power_period_eq}.	Then there exists a unique function $T(t)$ defined in $\left[t_0,t_f\right)$ that satisfies \eqref{eq:theo_cp_power_avg_power_period_eq} and $T\left(t_0\right) = T_0$, meaning there exists a $T(t)$ such that $P(t)$ is the average value of the instantaneous power $p(t)$ over $\left[t,t+T\left(t\right)\right)$.
\end{theorem}
\textbf{Proof:} start with the average power equation \eqref{eq:theo_cp_power_avg_power_period_eq}; differentiating it with respect to time and using Leibniz's Integral rule yields

\begin{equation}
\dot{T}\left(t\right)\left[p\left(t+T\left(t\right)\right) - P\left(t\right)\right] = T\left(t\right)\dot{P}\left(t\right) + p\left(t\right)- p\left(t+T\left(t\right)\right) \label{eq:period_dot_1}
\end{equation}

	Now let us analyze the term of \eqref{eq:period_dot_1} in brackets that multiplies $\dot{T}$; call this term $u\left(t,T(t)\right)$. Clearly, $u$ can only be zero for isolated and distinct values of $t$, not being null in a continuum of values unless $m_v,m_i,\phi_v,\phi_i,\psi$ have very particular qualities — making reasonable the genericity argument that this will most likely not happen for arbitrary signals. Then, let $t_1 < t_2 < ... < t_k$ be the roots of $u$, $t_0 < t_1$, $t_k \leq t_f$, and denote $I_j = \left(t_j,t_{j+1}\right)$; in each interval $I_j$, \eqref{eq:period_dot_1} can be written as $\dot{T}(t) = f\left(t,T(t)\right)$, thus if $f\in C^1$ for each $I_j$ and an initial solution $T(t_j)$ can be found for each $t_j$, then by the Picard-Lindelöf Existence and Uniqueness Theorem \cite[p. 188]{Perko2001} a unique solution $T_j$ exists for each $I_j$. Continuous differentiability of $f$ can be obtained by requiring $m_v,m_i,\phi_v,\phi_i$ continuously differentiable; then all that is left to prove is that an initial condition can be found for each $I_j$, achievable by using the continuation of the solutions of \eqref{eq:period_dot_1}. Suppose $T_0 = T\left(t_0\right)$ is known and exists for some $t_0$. Then the IVP has a unique solution in the entire $I_0$, and this solution exists until $t_1$ is reached. However, by \eqref{eq:theo_cp_power_avg_power_period_eq}, $T(t)$ must be continuous — by definition $P(t)$ is continuous and so is the integral — therefore define

\begin{equation} T_1 = T\left(t_1\right) = \lim\limits_{t\to t_1^-} T\left(t\right) \label{eq:period_dot_3} \end{equation}

	 and if this limit exists and is finite, adopt $t_1,T\left(t_1\right)$ as a new IVP. The solution will exist on $I_1$ until $t_2$ is reached; if the limit \eqref{eq:period_dot_3} exists for $t_2$, adopt $t_2,T_2$ as the initial value for $I_2$, and so on. This process can be continued until such time $t_f$ when either $u$ is zero, the limit \eqref{eq:period_dot_3} does not exist or diverges for a certain $t_j$, or one of the amplitude or phase signals is not defined or they break the conditions needed to make $f$ compliant with the requirements of the Picard theorem. Thus, the IVP \eqref{eq:period_dot_1} with $T\left(t_0\right) = T_0$ has a unique solution on $\left[t_0,t_f\right)$. \hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm} %>>>

	Naturally one wonders whether theorems \ref{theo:activereactivepower} and \ref{theo:activepowerperiod} are suitable to represent the common active and reactive powers in static phasors. The customary definitions of $P$ and $Q$ are immediate from the theorem; as for the period $T$, it is simple to prove that $T_0 = T(0) = 2\pi/\omega$ is a solution to \eqref{eq:theo_cp_power_avg_power_period_eq} (so is any multiple of $\pi/\omega$). Then \eqref{eq:period_dot_1} becomes

\begin{equation} \dot{T}\left(t\right)\left[p\left(t+T\left(t\right)\right) - P\left(t\right)\right] = 0 \label{eq:period_dot_4} \end{equation}

	Analyzing $u\left(t,T\left(t\right)\right)$, through simple algebra one obtains

\begin{equation} u\left(t,T\left(t\right)\right) = \dfrac{m_vm_i}{2} \cos\left(2\omega t + \phi_v + \phi_i\right) \end{equation}

	therefore one can obtain the roots of $u$:

\begin{equation} t_i = \dfrac{1}{2\omega}\left(i\pi + \dfrac{\pi}{2} - \phi_v - \phi_i\right), i\in\mathbb{Z} \end{equation}

	At a first glance, in each of the $t_i$ \eqref{eq:period_dot_4} becomes $0\times \dot{T}\left(t_i\right) = 0$ and $\dot{T}$ is undefined. For any other time instants however $\dot{T}\left(t\right) = 0$ by \eqref{eq:period_dot_4} and by the continuity of $T$ the limit \eqref{eq:period_dot_3} exists for all $t_i$ and is $T_0$, therefore $T(t) = T_0$ for all $t$.

	Theorem \ref{theo:activereactivepower} establishes the same active and reactive power quantities that static phasors enjoy, and with exactly the same interpretation: $P(t)$ is the power performed by the bipole over some interval $\left[t,t+T(t)\right]$, while $Q(t)$ vanishes over the same interval; moreover, it is simple to prove the generalized counterpart of equation \eqref{eq:active_reactive_current} and theorem \ref{corollary:direct_quad_current}, stating that the (generalized) sinusoidal current can be decomposed into one component in phase with the voltage and another in quadrature with voltage.

\begin{theorem}[Direct and quadrature components of sinusoidal currents]\label{theo:direct_quad_current_nonst} %<<<
	Let $v,i,P,Q$ as defined in theorem \ref{theo:activereactivepower}. Then $i$ can be written as

\begin{equation} i(t) = \dfrac{2P(t)}{m_v(t)}\cos\left(\psi(t) + \phi_v(t)\right) + \dfrac{2Q(t)}{m_v(t)}\sin\left(\psi(t) + \phi_v(t)\right) .\end{equation}
\end{theorem}
\textbf{Proof.} By simple algebraic manipulation:

\begin{align}
	i(t)
	&= m_i(t)\cos\left(\psi(t) + \phi_v(t)\right) \nonumber\\[3mm]
	&= m_i(t)\cos\left(\psi(t) + \phi_v(t) - \Delta\phi\right) \nonumber\\[3mm]
	&= m_i(t)\left[\cos\left(\psi(t) + \phi_v(t)\right)\cos\left(\Delta\phi(t)\right) + \sin\left(\psi(t) + \phi_v(t)\right)\sin\left(\Delta\phi(t)\right) \right] \nonumber\\[3mm]
	&= \dfrac{2P(t)}{m_v(t)}\cos\left(\psi(t) + \phi_v(t)\right) + \dfrac{2Q(t)}{m_v(t)}\sin\left(\psi(t) + \phi_v(t)\right)
\end{align} \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm} %>>>

\begin{example}[Application of theorem \ref{theo:activereactivepower}] \label{example:rlc_dpt_power} %<<<

	Continuing from example \ref{corollary:complex_equivalence_phasorialodes}, consider the RLC circuit of figure with the same voltage excitation and the same adopted values. Figure \ref{fig:amp_phase_voltage_signals_power} shows the active $P(t)$ and reactive power $Q(t)$ supplied by the excitation source $V(t)$, as calculated through equation \eqref{eq:dynamical_complex_power_def}. At the same time, figure \ref{fig:period_signals} shows the many period signals $T(t)$ calculated through equation \eqref{eq:theo_cp_power_avg_power_period_eq}. This figure shows that in the proposed framework, much like in static phasors, equation \eqref{eq:theo_cp_power_avg_power_period_eq} may have several (or infinite) solutions for $T(t)$; in general, the period adopted is the smallest positive one. To this extent, the figure shows a zoomed-in version showing the first solution. Finally, all solutions tend to a multiple of $\pi/\omega_0$, which is expected as the excitation signal $v(t)$ ``tends'' to a static sinusoid, then $T(t)$ tends to the period of a static phasor.

% POWER CURVES %<<<
\begin{figure}
        \begin{center}
                \begin{tikzpicture}
                        \begin{axis}[
                                width = 1\columnwidth,
                                height = 1/1.618*\columnwidth,
                                title={Active and reactive power supplied by $V(t)$},
                                xlabel={Time (s)},
				ylabel style = red,
 %                              ylabel={$P(t)$ (W)},
				y axis line style = {red, thick},
				every y tick label/.append style ={red},
				every y tick/.append style ={thick, red},
                                xmin=0, xmax=1,
                                ymin=40, ymax=220,
                                xtick={0,0.2,...,1},
                                ytick={40,80,...,200},
				ylabel style = {align=center},
				axis y line*=left,
                                %ymajorgrids=true,
                                %xmajorgrids=true,
                                every axis plot/.append style={thick},
				legend pos = south east
                        ]
                                \addplot[red,smooth] table[col sep=comma,header=false,x index=0,y index=6]{data/rlc_sim/data_rlc_sim_dps.csv};
				\addlegendentry{$P$ (W)}
                        \end{axis}
%
                        \begin{axis}[
                                width = 1\columnwidth,
                                height = 1/1.618*\columnwidth,
                                xmin=0, xmax=1,
		        	axis y line*=right,
                               % ylabel={$Q(t)$ (VAR) },
				y axis line style = {blue, thick},
				every tick label/.append style ={blue},
				every y tick/.append style ={thick, blue},
				ylabel near ticks,
                                ymin=-91, ymax=0,
                                xtick={-90,-80,...,0},
				axis x line=none,
                                %ytick={-0.2,-0.1,...,1},
		        	ylabel style = {align=center},
                                %ymajorgrids=true,
                                %xmajorgrids=true,
                                every axis plot/.append style={thick},
				legend pos = north east
                        ]
                                \addplot[blue ,smooth] table[col sep=comma,header=false,x index=0,y index = 7]{data/rlc_sim/data_rlc_sim_dps.csv};
				\addlegendentry{$Q$ (VAR)}
                        \end{axis}
                \end{tikzpicture}
        \caption
[Dynamic Phasor simulation of active and reactive power output.]
{Dynamic Phasor simulation of active P (red, left axis) and reactive Q (blue, right axis) power output by the voltage source $V(t)$ of the circuit of Figure \ref{fig:complexification_example} as calculated by theorem \ref{theo:activereactivepower}.}
        \label{fig:amp_phase_voltage_signals_power}
        \end{center}
\end{figure}
% >>>

% PERIOD TIME CURVES <<<
\begin{figure}
        \begin{center}
                \begin{tikzpicture}
                        \begin{axis}[
				name = ax_main,
                                width = 0.9\columnwidth,
                                height = 0.9/1.618*\columnwidth,
                                title={Period signals from DP simulations},
                                xlabel={Time (s)},
                                ylabel={Period $\left(\times \pi/\omega_0\right)$},
                                xmin=0, xmax=1,
                                ymin=0, ymax=10.5,
                                xtick={0,0.1,...,1},
                                ytick={0,2,...,10}, 
                                legend pos=south east,
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                colormap name=hsv,
                                cycle list={[ colors of colormap={0,100,200,300,400,500,600,700,800,900} ]},
                                every axis plot/.append style={thick},
                        ]
                        \addplot table[col sep=comma,header=false,x index=0,y index=1] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=2] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=3] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=4] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=5] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=6] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=7] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=8] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=9] {data/rlc_sim/period.csv};
                        \addplot table[col sep=comma,header=false,x index=0,y index=10]{data/rlc_sim/period.csv};
                        \coordinate (c1) at (axis cs:0,0.92);
                        \coordinate (c2) at (axis cs:1,0.92);
                        \end{axis}
%
                        \begin{axis}[
                                name = ax_zoomed,
                                at={($(ax_main.north east)-(0.75\columnwidth,1.75/1.618*\columnwidth)$)},
                                width = 0.9\columnwidth,
                                height = 0.9/1.618*\columnwidth,
                                xmin=0, xmax=1,
                                ymin=0.92, ymax=1.055,
                                xtick={0,0.1,...,1},
                                ytick={0.925,0.95,...,1.05},
				xlabel=\empty,
				ylabel=\empty,
				tick label style={/pgf/number format/fixed},
				legend columns=2,
				legend style={/tikz/every even column/.append style={column sep=0.5cm}},
                                ymajorgrids=true,
                                xmajorgrids=true,
                                %grid style=dashed,
                                every axis plot/.append style={thick},
                                axis background/.style = {
                                        preaction = {
                                        path picture = {
                                        \draw[fill=white,line width=0mm] (axis cs:0,1.055) rectangle (axis cs:1,0.92);
                                                }
                                        }
                                }    
                        ]
				\addplot[red, smooth]         table[col sep=comma,header=false,x index=0,y index=1]{data/rlc_sim/period.csv};
                        \end{axis}
                        % draw dashed lines from rectangle in first axis to corners of second
                        \draw [gray,dashed] (c1) -- (ax_zoomed.north west);
                        \draw [gray,dashed] (c2) -- (ax_zoomed.north east);
                \end{tikzpicture}
        \caption
[Period signals as defined in theorem \ref{theo:activereactivepower} calculated for the circuit of figure \ref{fig:complexification_example}.]
{Period signals $T(t)$ as defined in theorem \ref{theo:activereactivepower} calculated for the circuit of figure \ref{fig:complexification_example}. Each curve belongs to a period signal $T(t)$ corresponding to a distinct initial period $T_0$ obtained by numerically solving \eqref{eq:theo_cp_power_avg_power_period_eq} at $t=0$ and using this value as a initial condition for integrating the differential equation \eqref{eq:period_dot_1}. The bottom plot shows a zoom-in detailing the smallest positive solution which is generally the one adopted as period.}
	\label{fig:period_signals}
        \end{center}
\end{figure}
% >>>

\examplebar
\end{example} %>>>

%-------------------------------------------------
\section{Some circuit analysis in Dynamic Phasor domain} %<<<1

	Following the developments of Dynamic Phasor Theory, we now want to prove that this theory proposed begets some network analysis results that one can use to make circuit resolution easier. We first prove the Dynamic Phasor equivalents of Kirchoff's Laws as direct consequences of the linearity of the Dynamic Phasor Transform.

\begin{theorem}[Kirchoff's Current Law in the Dynamic Phasor domain] \label{theo:kirchoff_current_1p} %<<<
Let $i_p(t)$, $p = 1,...,q$ be the nonstationary sinusoidal currents of a certain network meeting at a node, $I_p(t)$ their dynamic phasors. Then

\begin{equation} \sum\limits_{p=1}^q I_p(t) = 0 \end{equation}

\end{theorem}
\textbf{Proof.} By Kirchoff's Current Law in time domain, $\sum i_p(t) = 0$. Applying the dynamic phasor transform and using its linearity yields $\sum I_p(t) = 0$. \hfill$\blacksquare$ 
\vspace{5mm}
\hrule
\vspace{5mm} %>>>
	
\begin{theorem}[Kirchoff's Voltage Law in the Dynamic Phasor domain] \label{theo:kirchoff_voltage_1p} %<<<
Let $v_p(t)$, $p = 1,...,q$ be the nonstationary sinusoidal voltages of a certain network around a certain closed loop, $V_p(t)$ their dynamic phasors. Then

\begin{equation} \sum\limits_{p=1}^q V_p(t) = 0 \end{equation}

\end{theorem}
\textbf{Proof:} akin to theorem \ref{theo:kirchoff_current}. \hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm} %>>>

	Further, we want to know what are the voltage-current relationships of linear elements in the Dynamic Phasor Domain.

\begin{theorem}[Dynamic Phasor Capacitive Relationship]\label{theo:1p_capacitive_conductance} % <<<
Let $v(t)$ be the voltage across a capacitor like in the figure below. Denote $V =\mathbf{P_D^{\omega}} \left[v\right] = v_d\left(t\right) + jv_q\left(t\right)$ as the corresponding phasor of $v(t)$, $\omega$ as its apparent frequency and $\psi = \int_{0}^{t} \omega(x)dx$. Also let $\mathbf{T}_\psi$ be the $dq$ transform matrix where $\omega = \dot{\psi}$ exists and is continuous.

\begin{center}
        \begin{tikzpicture}[american,scale=1.2,transform shape,line width=0.75]
		\draw(0,0)   to[short,o-o]       (3,0);
		\draw(0,3)   to[short,o-o]       (3,3);
		\draw(1.5,3) to[C,l=$C$,f>^=$i$] (1.5,0);
                \draw(3,0) to[open,european,voltage/distance from node=0.5mm, voltage/bump b=1, v=$v$] (3,3);
        \end{tikzpicture}
\end{center}

	Then the current through the capacitor is such that

\begin{equation}
\left\{\begin{array}{l}
        i_d = C\dfrac{dv_d}{dt} - \omega C v_q \\[5mm]
        i_q = C\dfrac{dv_q}{dt} + \omega C v_d
\end{array}\right.
\end{equation}

	\noindent and the complex phasor $I$ obtained through the equation

\begin{equation} I = C\dfrac{dV}{dt} + j\omega C V \end{equation}

	is equal to the phasor corresponding to $i(t)$, $\mathbf{P_D^{\omega}}\left[i\right] = i_d\left(t\right) + ji_q\left(t\right)$.

\end{theorem}
\textbf{Proof:} writing the time differential equations,

\begin{equation} \mathbf{i}_{\alpha\beta} = 
\left[\begin{array}{c} i_\alpha \\ i_\beta \end{array}\right] = 
\left[\begin{array}{c} C\dfrac{dv_\alpha}{dt} \\[5mm] C\dfrac{dv_\beta}{dt} \end{array}\right] \Leftrightarrow
 \mathbf{i}_{\alpha\beta} = C\dfrac{d\mathbf{v}_{\alpha\beta}}{dt} \end{equation}

	Multiplying both sides by $\mathbf{T}_{\psi}$,

\begin{align}
	\mathbf{i}_{dq} &= \mathbf{T}_\psi \mathbf{i}_{\alpha\beta} \nonumber\\[3mm]
	=& \mathbf{T}_\psi C\dfrac{d\mathbf{v}_{\alpha\beta}}{dt} \nonumber\\[3mm] 
	\substack{\text{(Lemma \ref{theo:dq_1p_diff})} \\ =}\hspace{2mm}& \mathbf{T}_\psi C\left[ \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{v}_{dq}\right) + \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{v}_{dq} \right] \nonumber\\[3mm] 
	=& C\left[ \mathbf{T}_\psi \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{v}_{dq}\right) + \mathbf{T}_\psi \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{v}_{dq} \right] \nonumber\\[3mm] 
	=& C\left[ \dfrac{d}{dt}\left(\mathbf{v}_{dq}\right) + \mathbf{T}_{\psi}\dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{v}_{dq} \right] \nonumber\\[3mm]
	\substack{\text{(Lemma \ref{lemma:1p_t_ndifftminus_product})} \\ =}\hspace{2mm}& C\left\{ \dfrac{d}{dt}\left(\mathbf{v}_{dq}\right) + \dfrac{d\psi}{dt} \left[\begin{array}{cc}    0 & -1 \\[5mm] 1 & 0 \end{array}\right] \mathbf{v}_{dq0} \right\} \nonumber\\[3mm]
%
%
&= \left[\begin{array}{l}
        C\dfrac{dv_d}{dt} - \omega C v_q \\[5mm]
        C\dfrac{dv_q}{dt} + \omega C v_d
\end{array}\right]
\end{align}

	Applying the complexification operator,

\begin{equation} I = i_d + ji_q = C\dfrac{dv_d}{dt} - \omega C v_q + j\left(C\dfrac{dv_q}{dt} + \omega C v_d\right) = C\dfrac{d}{dt}\left(v_d + jv_q\right) + j\omega C\left(v_d + jv_q\right) = C\dfrac{dV}{dt} + j\omega C V \end{equation}

\hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

\begin{theorem}[Dynamic Phasor Inductive Relationship]\label{theo:1p_inductive_impedance} %<<< 
Let $i(t)$ be the current through an inductor like in the figure below. Denote $I =\mathbf{P_D^{\omega}} \left[i\right] = i_d\left(t\right) + ji_q\left(t\right)$ as the corresponding phasor of $i(t)$, $\omega$ as its apparent frequency and $\psi = \int_{0}^{t} \omega(x)dx$. Also let $\mathbf{T}_\psi$ be the $dq$ transform matrix where $\omega = \dot{\psi}$ exists and is continuous.

\begin{center}
        \begin{tikzpicture}[american,scale=1.2,transform shape,line width=0.75, cute inductors]
		\draw(0,0)	to[short,o-]		(1,0)
				to[L,l=$L$,f>^=$i$]	(4,0)
				to[short,-o]		(5,0);
                \draw(0,-0.2) to[open,european,voltage/distance from node=0.5mm, voltage/bump b=2, v<=$v$] (5,-0.2);
        \end{tikzpicture}
\end{center}

	Then the current through the capacitor is such that

\begin{align}
\left\{\begin{array}{l}
        v_d = L\dfrac{di_d}{dt} - \omega L i_q \\[5mm]
        v_q = L\dfrac{di_q}{dt} + \omega L i_d \\[5mm]
\end{array}\right.
\end{align}

	\noindent and the complex phasor $V$ obtained through the equation

\begin{equation} V = L\dfrac{dI}{dt} + j\omega L I \end{equation}

	is equal to the phasor corresponding to $v(t)$, $\mathbf{P_D^{\omega}}\left[v\right] = v_d\left(t\right) + jv_q\left(t\right)$.

\end{theorem}
\textbf{Proof:} a repetition of theorem \ref{theo:1p_capacitive_conductance}. Writing the time differential equations,

\begin{equation} \mathbf{v}_{\alpha\beta} = 
\left[\begin{array}{c} v_\alpha \\ v_\beta \end{array}\right] = 
\left[\begin{array}{c} L\dfrac{di_\alpha}{dt} \\[5mm] L\dfrac{di_\beta}{dt} \end{array}\right] \Leftrightarrow
\mathbf{v}_{\alpha\beta} = L\dfrac{d\mathbf{i}_{\alpha\beta}}{dt} \end{equation}

	Multiplying both sides by $\mathbf{T}_{\psi}$,

\begin{align}
	\mathbf{v}_{dq} &= \mathbf{T}_\psi \mathbf{v}_{\alpha\beta} \nonumber\\[3mm]
	&= \mathbf{T}_\psi L\dfrac{d\mathbf{i}_{\alpha\beta}}{dt} \nonumber\\[3mm] 
	&\substack{\text{(Lemma \ref{theo:dq_1p_diff})} \\ =}\hspace{2mm} \mathbf{T}_\psi L\left[ \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{i}_{dq}\right) + \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{i}_{dq} \right] \nonumber\\[3mm] 
	&= L\left[ \mathbf{T}_\psi \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{i}_{dq}\right) + \mathbf{T}_\psi \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{i}_{dq} \right] \nonumber\\[3mm] 
	&= L\left[ \dfrac{d}{dt}\left(\mathbf{i}_{dq}\right) + \mathbf{T}_{\psi}\dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{i}_{dq} \right] \nonumber\\[3mm]
	&\substack{\text{(Lemma \ref{lemma:1p_t_ndifftminus_product})} \\ =}\hspace{2mm} L\left\{ \dfrac{d}{dt}\left(\mathbf{i}_{dq}\right) + \dfrac{d\psi}{dt} \left[\begin{array}{cc}    0 & -1 \\[5mm] 1 & 0 \end{array}\right] \mathbf{i}_{dq} \right\} \nonumber\\[3mm]
%
%
&= \left[\begin{array}{l}
        L\dfrac{di_d}{dt} - \omega L i_q \\[5mm]
        L\dfrac{di_q}{dt} + \omega L i_d
\end{array}\right]
\end{align}

	Applying the complexification operator,

\begin{equation} V = v_d + jv_q = C\dfrac{di_d}{dt} - \omega L i_q + j\left(L\dfrac{di_q}{dt} + \omega L i_d\right) = L\dfrac{d}{dt}\left(i_d + ji_q\right) + j\omega L\left(i_d + ji_q\right) = L\dfrac{dI}{dt} + j\omega L I \end{equation}

\hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
% >>>

	Theorems \ref{theo:1p_capacitive_conductance} and \ref{theo:1p_inductive_impedance} are essentially the application of theorems \ref{theo:1p_ode_solution} and \ref{corollary:complex_equivalence_phasorialodes} to the equations $i = C\dot{v}$ and $v = L\dot{i}$. It is not difficult to see that the current-voltage relationship of a resistor is also linear, that is, $v(t) = Ri(t) \Rightarrow V(t) = RI(t)$. Theorems \ref{theo:1p_capacitive_conductance} and \ref{theo:1p_inductive_impedance} determine that inductors and capacitors have phasorial relationships of the form, completing the relationships as in \eqref{eq:Linear_relationships_1p}.

\begin{equation} \left\{\begin{array}{l} \text{Linear inductor: } v(t) = L \dot{i}(t) \Rightarrow V(t) = L\dot{I} + j\omega L I \\[3mm] \text{Linear capacitor: } i(t) = C \dot{v}(t) \Rightarrow I(t) = C\dot{V} + j\omega C V \\[3mm] \text{Linear resistor: } v(t) = Ri(t) \Rightarrow V(t) = RI(t) \end{array}\right. \label{eq:Linear_relationships_1p} \end{equation}

	In essence, these relationships stem from the fact that

\begin{equation} y(t) = \dot{x}(t) \Rightarrow Y(t) = \dot{X} + j\omega X, \label{eq:dpt_diff_eq}\end{equation}

	\noindent which can be asserted by applying theorem \ref{corollary:complex_equivalence_phasorialodes} to the differential equation $\dot{x}(t) - y(t) = 0$. Interestingly, \eqref{eq:dpt_diff_eq} is strikingly similar to the differentiation property \eqref{eq:stft_derivative_3} of the Short Time Fourier Transform, and exactly identical to the single-harmonic-approximated equations \eqref{sys:fdp_sys_fundamental_harmonic}. This is a fortunate result because, since most of the Dynamic Phasor literature, as well as Power Systems modelling using Dynamic Phasors, is based on the STFT, the Dynamic Phasor framework proposed here preserves the modelling procedures and techniques of the current literature.

	Formally, the Dynamic Phasor Transform proposed transforms derivatives in time domain to a particular operation in the Dynamic Phasor domain, and this will be explored later in this thesis. It will be proven that the combination of the complex operations form algebraic structures, which make modelling very simple and intuitive. For now, we use theorems \ref{theo:kirchoff_current_1p} through \ref{theo:1p_inductive_impedance} to prove that circuit analysis in the Dynamic Phasor domain is very close to that of static phasors, in the sense that these theorems make it possible to undertake the entire analysis in the complex domain instead of obtaining equations from the time domain.

\begin{example}[Circuit analysis in the DP domain]\label{example:dpdomain_secondorder} %<<<

	Consider the second-order circuit of figure \ref{fig:complexification_example_network_1p} where the same second-order circuit of example \ref{example:rlc_dpt} is shown, but in the Dynamic Phasor domain.

% MODELLING EXAMPLE: RLC CIRCUIT <<<
\begin{figure}[htb!]
\centering
        \begin{tikzpicture}[american,scale=1,transform shape,line width=0.75, cute inductors, voltage shift = 1]
	\ctikzset{/tikz/circuitikz/voltage/distance from node=10mm}
		\draw (0,0)
			to[vsource,sources/scale=1.25, v>=$V(t)$,invert] (0,4)
			to[L,l=$L$,f>^=$I_{L}$,v>=$V_{L}$,-*] (4,4) 
			to[C,l=$C$,f>^=$I_{C}$,v>=$V_{C}$,-*] (4,0) 
			to[short] (0,0); 
		\draw (4,4)
			to[short,f>^=$I_{R}$] (8,4) 
			to[R,l=$R$,v>=$V_{R}$] (8,0) 
			to[short]  (4,0);

		% DRAWING VOLTAGE LOOPS
		\draw[rounded corners=10,loop, draw opacity=0.3,->, color=blue] (0.5,0.5) -- (0.5,3.5) -- (3.5,3.5) -- (3.5,0.5) -- (1,0.5) ;
		\draw[rounded corners=10,loop, draw opacity=0.3,->, color=red] (4.5,0.5) -- (4.5,3.5) -- (7.5,3.5) -- (7.5,0.5) -- (5,0.5) ;
		% DRAWING NODE LABELS
		\node[shape=circle,draw,inner sep=1pt] at (  4,4.5) {$1$};
		
		% DRAWING LOOP LABELS

		\node[color=blue] at (2,2) {$L1$} ;
		\node[color=red ] at (6,2) {$L2$} ;
        \end{tikzpicture}
	\caption{Second-order circuit for example application of circuit analysis in the DP domain.}
	\label{fig:complexification_example_network_1p}
\end{figure} %>>>

	Applying Kirchoff's Current Law in the DP domain (theorem \ref{theo:kirchoff_current_1p}) in node 1 one obtains

\begin{equation} (KCL):\  I_L - I_C - I_R = 0\end{equation}

	\noindent and using Kirchoff's Voltage Law in the DP domain (theorem \ref{theo:kirchoff_voltage_1p}) in the voltage nodes yields

\begin{equation}\left\{\begin{array}{l} (L1):\ V_C - V + V_L = 0 \\[3mm] (L2):\ V_R = V_C \end{array}\right. .\end{equation}

	Finally, using the voltage-current relationships of the elements,

\begin{equation}\left\{\begin{array}{rl} (KCL):&\ I_L - \left(C\dot{V}_C + j\omega C V_C\right) - \dfrac{V_R}{R} = 0 \\[3mm] (L1):&\ V_C - V + L\dot{I}_L + j\omega LI_L = 0 \\[3mm] (L2):&\ V_R = V_C \end{array}\right. .\end{equation}

	Applying the third equation to the other two,

\begin{equation}\left\{\begin{array}{l} I_L - \left(C\dot{V}_R + j\omega C V_R\right) - \dfrac{V_R}{R} = 0 \\[3mm] V_R - V + L\dot{I}_L + j\omega LI_L = 0 \end{array}\right. .\end{equation}

	Now, differentiating the first equation, 

\begin{equation}\left\{\begin{array}{l} \dot{I}_L - \left(C\ddot{V}_R + j\dot{\omega} C V_R + j\omega C \dot{V}_R\right) - \dfrac{\dot{V}_R}{R} = 0 \\[3mm] V_R - V + L\dot{I}_L + j\omega LI_L = 0 \end{array}\right. .\end{equation}

	\noindent and substituting $\dot{I}_L$ from the second equation into the differentiated first equation:

\begin{equation}\dfrac{-V_R + V}{L} + j\omega I_L - \left(C\ddot{V}_R + j\dot{\omega} C V_R + j\omega C \dot{V}_R\right) - \dfrac{\dot{V}_R}{R} = 0 .\end{equation}

	Finally, one can isolate $I_L$ from the (KCL) equation and

\begin{equation}\dfrac{-V_R + V}{L} + j\omega \left[\left(C\dot{V}_R + j\omega C V_R\right) + \dfrac{V_R}{R}\right] - \left(C\ddot{V}_R + j\dot{\omega} C V_R + j\omega C \dot{V}_R\right) - \dfrac{\dot{V}_R}{R} = 0 .\end{equation}

	Dividing the equation by $C$ and grouping the terms, 

\begin{equation} \ddot{V}_R(t) + \dot{V}_R(t)\left(\dfrac{1}{RC} + 2j\omega(t)\right) + V_R\left\{ \dfrac{1}{LC}  -\omega^2(t) + j \left[ \dot{\omega}(t) + \dfrac{1}{RC}\omega(t)\right]\right\} -\dfrac{1}{LC} V(t) = 0, \label{eq:rlc_complex_diffeq_dpt}\end{equation}

	\noindent which is the exact same equation as \eqref{eq:rlc_complex_diffeq} of example \ref{example:rlc_dpt}.

\examplebar
\end{example} %>>>

	In example \ref{example:rlc_dpt}, the final equation \eqref{eq:rlc_complex_diffeq} that models $V_R(t)$ is obtained by first obtaining the model in the time domain, and then using theorem \ref{corollary:complex_equivalence_phasorialodes} to transport the time-domain differential equation to the equivalent Dynamic Phasor differential equation. In contrast, example \ref{example:dpdomain_secondorder} shows that the circuit analysis can be carried entirely in the complex domain, by virtue of theorems \ref{theo:kirchoff_current_1p} through \ref{theo:1p_inductive_impedance}.

%-------------------------------------------------
\section{Three-Phase Dynamic Phasors} %<<<1

	We now want to import all the results of the Dynamic Phasor Transform to three-phase signals. It will be shown that with minimal adaptations, the Dynamic Phasor Transform can be constructed for three-phase signals with high resemblance to the DPT for single-phase signals. Further, it will be shown that a counterpart to theorems \ref{theo:1p_ode_solution} and \ref{corollary:complex_equivalence_phasorialodes} can be proven for three-phase signals, with the added challenge of dealing with an extra dimension — the zero-sequence component. Finally, it will be shown that the notions of complex, active and reactive power of theorem \ref{theo:activereactivepower} are also maintained.

%------------------------------------------------
\subsection{Synchronization basics: the $\alpha\beta\gamma$ and $dq0$ transforms} %<<<2

	We first show that in three-phase systems we can easily define counterparts to the $\alpha\beta$ and the $dq$ transform. Naturally, in three-phases a dimension is added; however, these transformations in three-phase systems are simpler because they are very known linear transformations based on particular matrices. The historical developments of these transforms can be found in  \cite{orourkeGeometricInterpretationReference2019,Park1929,Krause1965} and \cite{clarke1938problems}.

	First the definition of a three-phase signal is presented as a three-dimensional signal. The definition of a poly-phase quantity dates back to the initial developments in polyphase analysis by \cite{Fortescue1918}, who proved that any set of $N$ unbalanced phasors (therefore a polyphase quantity) can be written as the linear combination of $N$ symmetrical sets of balanced phasors; the set as a whole manifests in a single frequency. Since this thesis is based on single and three-phase systems, the definitions and theorems below focus on $N=3$.

	Then the $\alpha\beta\gamma$ or Clarke Transform is presented as the power-invariant variation of the original transform conceived by Emily Clarke in the 1930s. Clarke proposed this transform as a means to simplify the analysis of three-phase systems, in particular unbalanced systems, using the tools available at the time, which were largely based on the positive and negative sequence transform \pcite{clarke1938problems}. The innovation of Clarke's method was that three-phase quantities, when projected onto a stationary axis at a particular angle, were transformed into orthogonal quantities called $\alpha$, $\beta$ and $\gamma$ components and, if the original quantity was a balanced three-phase signal, the resulting transformed quantities would yield a null $\gamma$ component — effectively transforming three components into two orthogonal ones, rendering the analysis much easier.

	Finally, the Park Transform is presented as a linear transform equivalent to the rotation of the three-phase quantity of an arbitrary angle. This transform translates a time-varying three-phase system into a set of two axes, direct and quadrature, and a ``zero-sequence''component. The composition of the Park and Clarke transforms forms the $dq0$ transform.

	The definitions and theorems given are made to be as general as possible to avoid the natural terminology that comes with the historic fact that the transformations were built upon the analyses of polyphase systems heavily influenced by synchronous machinery; most of the jargon involved in the literature refers to elements of electrical machines such as stators, rotors and flux linkages. In recent years, there has been a push in the literature to expand these transforms and analyses to embrace modern switched systems; to this extent, \cite{orourkeGeometricInterpretationReference2019} presents a thorough development of the Clarke-Park or $dq0$ transform as a generalized geometric transform on the $\mathbb{R}^3$, offering a more formal approach that allows for the understanding of these transformations in the context of three-phase analyses not bound by a particular technology.

%--------------------------------------------------------------------------------------------------
\subsubsection{The Clarke Transform} %<<<3

\begin{definition}[Three-phase signal]\label{def:three_phase_signal} %<<<
	A three phase signal $\mathbf{x}$ is a three-dimensional quantity comprised of three sinusoidal signals, that is, there exist three positive functions $m_a$, $m_b$, $m_c$ called moduli (modulus in the singular) and three functions $\theta_a$, $\theta_b$ and $\theta_c$ called absolute angles such that

\begin{equation} \mathbf{x} = \left[\begin{array}{c} x_a\left(t\right)\\ x_b\left(t\right) \\ x_c\left(t\right) \end{array}\right] = \left[\begin{array}{c} m_a\left(t\right)\cos\theta_a(t)\\ m_b\left(t\right)\cos\theta_b(t) \\ m_c\left(t\right)\cos\theta_c(t) \end{array}\right], \end{equation}

	which is known as the $abc$ representation; the components are named phases $a$, $b$ and $c$. A three-phase quantity is called \textbf{balanced} if the three phases are:

\begin{itemize}
	\item \textbf{Symmetric}: they have the same amplitude, that is, $m_1 = m_2 = m_3$;
	\item \textbf{Direct}: they are delayed copies of one single function;
	\item \textbf{Sequential}: phases are delayed by the same quantity.
\end{itemize}

	In other words, there exist two functions: a $m\left(t\right)$, called modulus and a $\theta\left(t\right)$, called absolute angle, such that

\begin{equation} \mathbf{x} = m(t)\left[
	\begin{array}{c}
		\cos\left(\theta\right) \\[5mm]
		\cos\left(\theta - \dfrac{2\pi}{3}\right) \\[5mm]
		\cos\left(\theta + \dfrac{2\pi}{3}\right)
	\end{array}\right] \label{eq:balanced_threephase_def}
\end{equation}
	
\end{definition} %>>>

	One of the main properties of balanced three-phase signals is that, albeit being three-dimensional quantities, they only need two dimensions to be described: a modulus $m(t)$ and an angle $\theta(t)$. This already gives an idea that this quantity can be described by a complex function $X(t)$. Here we also import all the definitions of generalized sinusoids from the single-phase case: a ``balanced three-phase sinusoid'' is that which angle $\theta(t)$ can be written as the combination

\begin{equation} \theta(t) = \psi(t) + \phi(t),\ \psi(t) = \int_0^t \omega(s)ds \end{equation}

	\noindent where $\omega(t)$ is a chosen apparent frequency and $\phi(t)$ the corresponding apparent phase.

	The notation for phases $a$, $b$ and $c$ are legacy notations for the three windings $a$, $b$ and $c$ of a three-phase synchronous machine upon which the definitions were built. Because of this, the $abc$ representation is largely defined as the time-signals pertaining to the three phases of a voltage or current in a certain system under study. For this reason, the three phases of an inverter are also denominated as such. Also for the sake of clarity, a three-phase signal will be also denoted as a ``3$\phi$ signal''.

	In the context of Electric Power System, a ``three-phase quantity'' will generally be either a three-phase voltage or current; because the first analyses were made for machinery, magnetic fluxes can also be depicted as such quantities in some researches.

	In the 1920s, \cite{Fortescue1918} presented a method whereby a polyphase network was decomposed into symmetrical components, allowing a much simpler analysis of such networks especially in the context of network unbalances. \cite{clarke1938problems} greatly improved and simplified over Fortescue's results, developing a method of transforming a three-phase system into a sequence of linearly independent complex phasors; finally, in 1943, Clarke published her transform, defined below.

\begin{definition}[Clarke or $\alpha\beta\gamma$ transform]\label{def:clarke_transform} %<<<
	The Clarke Transform is the linear transformation

\begin{equation}
\mathbf{T_{\alpha\beta\gamma}} = \sqrt{\dfrac{2}{3}}
\left[\begin{array}{ccc}
1 & -\dfrac{1}{2} & -\dfrac{1}{2} \\[5mm]
0 &  \dfrac{\sqrt{3}}{2} & -\dfrac{\sqrt{3}}{2} \\[5mm]
\dfrac{1}{2} & \dfrac{1}{2} & \dfrac{1}{2}
\end{array}\right]
\end{equation}

\end{definition} %>>>

	In the original paper by Clarke, the transform was presented as scaled not by $\sqrt{2/3}$ but $2/3$. Definition \ref{def:clarke_transform} makes it power invariant: due to the fact that $\mathbf{T}_{\alpha\beta\gamma}$ has a determinant of $\sqrt{3}/2$, the three-phase power on the $\alpha\beta\gamma$ space $p_{3\phi}^{\alpha\beta\gamma} = v_\alpha i_\alpha + v_\beta i_\beta + v_\gamma i_\gamma$ was not equal to the three-phase power on the $abc$ space $p_{3\phi}^{abc} = v_ai_a + v_bi_b + v_ci_c$, but proportional to it by a factor of $3/2$. This is due to the fact that, without this scaling factor, the $\mathbf{T_{\alpha\beta\gamma}}$ transform is not unitary (its inverse is not equal to its transpose). The unitarity becomes true when the rooted scaling is used; hence, such factor was later added \pcite{CHATTOPADHYAY2008}.

	Also notably, the Clarke Transform is invertible and linear. Further, the ingenuity of Clarke's transform is that a balanced three-phase quantity as in definition \eqref{eq:balanced_threephase_def}, when transformed through the matrix $\mathbf{T_{\alpha\beta\gamma}}$, yields

\begin{equation} \mathbf{T_C} \left( m\hspace{-0.6mm}\left(t\right)\left[\begin{array}{c} \cos\theta\hspace{-0.6mm}\left(t\right) \\[5mm] \cos\left(\theta\hspace{-0.6mm}\left(t\right) - \dfrac{2\pi}{3}\right) \\[5mm] \cos\left(\theta\hspace{-0.6mm}\left(t\right) + \dfrac{2\pi}{3}\right) \end{array}\right] \right) = \sqrt{3}m(t)\hspace{-0.6mm}\left[\begin{array}{c} \cos\theta\hspace{-0.6mm}\left(t\right) \\[5mm] \sin\theta\hspace{-0.6mm}\left(t\right)\\[5mm] 0\end{array}\right], \end{equation}

	\noindent and it is obvious that this quantity is diffeomorphic to $m(t)e^{j\theta(t)}$, thus representing the complex phasor of $x(t)$ with respect to the fixed time reference, the same way that the $\alpha\beta$ transform did with single-phase quantities.

%--------------------------------------------------------------------------------------------------
\subsubsection{The Park Transform} %<<<3

	Not concurrently with Clarke's analysis, \cite{Park1929} published a generalization of the Two-Reaction Theory of Synchronous Machines by Blondel, which was later expanded in \cite{Doherty1926}. The Park Transform was used to express the flux linkages in salient-pole synchronous machines by defining two axes of rotation: axis $d$ for ``direct'' and axis $q$ for ``quadrature'', the former being directly aligned with the machine rotor and the latter aligned in quadrature with the rotor. The flux linkages are then projected onto the $abc$ magnetic axes. Finally, a zero-sequence component ``$0$'' was added. Figure \ref{fig:park_transform_rotor} shows the diagram as drawn by Park, showing the synchronous machine $abc$ phases and the rotating $d$ and $q$ frames.



% SYNCHRONOUS MACHINE SCHEMATIC <<<Add commentMore actions
\begin{figure}[ht]
\scalebox{1}{
\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}, transform shape]
\ctikzset{color=stewartblue}Add commentMore actions

\node (O) at (0,0) {};

\pgfmathsetmacro{\rotorangle}{50}

\node[draw, circle, fill=gray!75!white, thick, minimum size=68mm] (graybg) at (0,0) {}; % SUM CIRCLE
\node[draw, circle, fill=white,         thick, minimum size=60mm] (whitefg) at (0,0) {}; % SUM CIRCLE

% rotor drawing <<<
\begin{scope}[rotate=\rotorangle]
	\path [name path=outercircle] (0,0) circle (28.5mm) ; % SUM CIRCLE
	\path [name path = leftline]  (-2.5mm,-30mm) -- (-2.5mm,30mm);
	\path [name path = rightline] ( 2.5mm,-30mm) -- ( 2.5mm,30mm);

	\path[name path = leftmostline]  (-10mm,10mm) -- (-10mm,30mm);
	\path[name path = rightmostline] ( 10mm,10mm) -- ( 10mm,30mm);

	\path [name intersections={of=outercircle and leftmostline, by={southwest_intersection}}];
	\path [name intersections={of=outercircle and rightmostline, by={southeast_intersection}}];

	\path[name path = leftmostline]  (-10mm, 10mm) -- (-10mm, 30mm);
	\path[name path = rightmostline] ( 10mm, 10mm) -- ( 10mm, 30mm);

	\path[name path = botsouthline] ( -12mm, 22mm) -- ( 12mm, 22mm);

	\path [name intersections={of=botsouthline and rightmostline, by={southeast_T}}];

	\path [name intersections={of=botsouthline and leftmostline, by={southwest_T}}];

	\path [name intersections={of=botsouthline and leftline, by={southwest_inner_T}}];
	\path [name intersections={of=botsouthline and rightline, by={southeast_inner_T}}];

		% Compute angles and radius using \path let
		\node (A) at (southwest_intersection) {};
		\node (B) at (southeast_intersection) {};
		\node (C) at (southwest_T) {};
		\node (D) at (southeast_T) {};
		\node (E) at (southwest_inner_T) {};
		\node (F) at (southeast_inner_T) {};

		% Get coordinates for calculations
		\path (O); \pgfgetlastxy{\Ox}{\Oy}
		\path (A); \pgfgetlastxy{\Ax}{\Ay}
		\path (B); \pgfgetlastxy{\Bx}{\By}

		% Compute angles
		\pgfmathsetmacro{\angleA}{atan2(\Ay - \Oy,\Ax - \Ox)}
		\pgfmathsetmacro{\angleB}{atan2(\By - \Oy,\Bx - \Ox)}

		% Compute radius (distance from O to A)
		\pgfmathsetmacro{\radius}{veclen(\Ax - \Ox,\Ay - \Oy)/28.45274}

		% Draw the arc centered at O from A to B
		\draw[thick] (E.center) -- (C.center) -- ([shift=(\angleA:\radius)]O.center) arc(\angleA:\angleB:\radius) -- (D.center) -- (F.center);

	% SAME PROCESS FOR BOTTOM T

	\path[name path = leftmostline]  (-10mm,-10mm) -- (-10mm,-30mm);
	\path[name path = rightmostline] ( 10mm,-10mm) -- ( 10mm,-30mm);

	\path [name intersections={of=outercircle and leftmostline, by={southwest_intersection}}];

	\path [name intersections={of=outercircle and rightmostline, by={southeast_intersection}}];

	\path[name path = leftmostline]  (-10mm,-10mm) -- (-10mm,-30mm);
	\path[name path = rightmostline] ( 10mm,-10mm) -- ( 10mm,-30mm);

	\path[name path = botsouthline] ( -12mm,-22mm) -- ( 12mm,-22mm);

	\path [name intersections={of=botsouthline and rightmostline, by={southeast_T}}];
	\path [name intersections={of=botsouthline and leftmostline, by={southwest_T}}];
	\path [name intersections={of=botsouthline and leftline, by={southwest_inner_T}}];
	\path [name intersections={of=botsouthline and rightline, by={southeast_inner_T}}];

	% Compute angles and radius using \path let
		\node (G) at (southwest_intersection) {};
		\node (H) at (southeast_intersection) {};
		\node (I) at (southwest_T) {};
		\node (J) at (southeast_T) {};
		\node (K) at (southwest_inner_T) {};
		\node (L) at (southeast_inner_T) {};

		% Get coordinates for calculations
		\path (O); \pgfgetlastxy{\Ox}{\Oy}
		\path (G); \pgfgetlastxy{\Ax}{\Ay}
		\path (H); \pgfgetlastxy{\Bx}{\By}

		% Compute angles
		\pgfmathsetmacro{\angleA}{atan2(\Ay - \Oy,\Ax - \Ox)}
		\pgfmathsetmacro{\angleB}{atan2(\By - \Oy,\Bx - \Ox)}

		% Compute radius (distance from O to A)
		\pgfmathsetmacro{\radius}{veclen(\Ax - \Ox,\Ay - \Oy)/28.45274}

		% Draw the arc centered at O from A to B
		\draw[thick] (F.center) -- (L.center) -- (J.center) -- ([shift=(\angleB:\radius)]O.center) arc(\angleB:\angleA:\radius) -- (I.center) -- (K.center) -- (E.center);

	\def\coil#1{ {4mm * cos(\t * pi r)}, {1mm * (2*#1 + \t) + 1mm*sin(\t * pi r)) - 15mm} }

	    % Draw the part of the coil behind the rectangle
	    \foreach \n in {1,...,15} { \draw[domain={0:1},stewartblue,smooth,thick, variable=\t,samples=15] plot (\coil{\n});  }

		\draw[domain={1:0.5},stewartblue,smooth,variable=\t, thick, samples=15, preaction={draw,white,line width=1pt}] plot (\coil{0}) node (coilstart) {};

	    \fill[fill=white] (-2.35mm,-20mm) rectangle (2.35mm,20mm);
		

	    % Draw the part of the coil in front of the rectangle
	    \foreach \n in {0,1,...,14} {
		\draw[domain={1:2},stewartblue,smooth,variable=\t, thick, ,samples=15, preaction={draw,white,line width=2pt}] plot (\coil{\n});
		}

		\draw[domain={1:1.4},stewartblue, thick, smooth,variable=\t,samples=15,] plot (\coil{15}) node (coilend) {}; 

		\draw[thick,stewartblue,preaction={draw,white,line width=2pt}] (coilend.center) -- ++ (20mm,0) node (battstart) {};

		\draw[thick,stewartblue] (battstart.center) to [vsourceAM, l=$v_F$, f^<=$i_F$] (battstart |- coilstart) -- ([shift=({3mm,0})]coilstart.center);

		\draw[thick,stewartpink] (-10mm,0) circle (1mm) node (dcoilright) {} ;
		\draw[thick,stewartpink] ($(dcoilright) + ({1mm*cos(45)},{1mm*sin(45)})$) -- ($(dcoilright) + ({-1mm*cos(45)},{-1mm*sin(45)})$);
		\draw[thick,stewartpink] ($(dcoilright) + ({1mm*cos(135)},{1mm*sin(135)})$) -- ($(dcoilright) + ({-1mm*cos(135)},{-1mm*sin(135)})$);

		\draw[thick,stewartpink] (10mm,0) circle (1mm) node (dcoilleft) {} ;
		\draw[thick,stewartpink, fill] (dcoilleft.center) circle (0.25mm) ;

		\draw[thick,stewartpink] (0,-25mm) circle (1mm) node (dcoilup) {} ;
		\draw[thick,stewartpink] ($(dcoilup) + ({1mm*cos(45)},{1mm*sin(45)})$) -- ($(dcoilup) + ({-1mm*cos(45)},{-1mm*sin(45)})$);
		\draw[thick,stewartpink] ($(dcoilup) + ({1mm*cos(135)},{1mm*sin(135)})$) -- ($(dcoilup) + ({-1mm*cos(135)},{-1mm*sin(135)})$);

		\draw[thick,stewartpink] (0,25mm) circle (1mm) node (dcoilsouth) {} ;
		\draw[thick,stewartpink, fill] (dcoilsouth.center) circle (0.25mm) ;
\end{scope} %>>>

	\node[stewartpink] at ([shift=({ 2.5mm, 2.5mm})]dcoilsouth) {$q$};
	\node[stewartpink] at ([shift=({-2.5mm,-2.5mm})]dcoilup) {$q'$};

	\node[stewartpink] at ([shift=({ 3.5mm, 0mm})]dcoilleft) {$d$};
	\node[stewartpink] at ([shift=({-2.5mm,-2.5mm})]dcoilright) {$d'$};

% A COIL
		\draw[thick] (32mm,0) circle (1mm) node (ain) {} ;
		\node at (37mm,0) {$a$} ;
		\draw[thick, fill] (ain.center) circle (0.25mm) ;

		\draw[thick] (-32mm,0) circle (1mm) node (aout) {} ;
		\draw[thick] ($(aout) + ({1mm*cos(45)},{1mm*sin(45)})$) --   ($(aout) + ({-1mm*cos(45)},{-1mm*sin(45)})$);
		\draw[thick] ($(aout) + ({1mm*cos(135)},{1mm*sin(135)})$) -- ($(aout) + ({-1mm*cos(135)},{-1mm*sin(135)})$);
		\node at (-37mm,0) {$a'$} ;

% B COIL
\begin{scope}[rotate=120]
		\draw[thick] (32mm,0) circle (1mm) node (ain) {} ;
		\draw[thick, fill] (ain.center) circle (0.25mm) ;

		\draw[thick] (-32mm,0) circle (1mm) node (aout) {} ;
		\draw[thick] ($(aout) + ({1mm*cos(45)},{1mm*sin(45)})$) --   ($(aout) + ({-1mm*cos(45)},{-1mm*sin(45)})$);
		\draw[thick] ($(aout) + ({1mm*cos(135)},{1mm*sin(135)})$) -- ($(aout) + ({-1mm*cos(135)},{-1mm*sin(135)})$);
\end{scope}

\node at ({37mm*cos(120)},{37mm*sin(120)}) {$b$} ;
\node at ({37mm*cos(300)},{37mm*sin(300)}) {$b'$} ;

% C COIL
\begin{scope}[rotate=-120]
		\draw[thick] (32mm,0) circle (1mm) node (ain) {} ;
		\draw[thick, fill] (ain.center) circle (0.25mm) ;

		\draw[thick] (-32mm,0) circle (1mm) node (aout) {} ;
		\draw[thick] ($(aout) + ({1mm*cos(45)},{1mm*sin(45)})$) --   ($(aout) + ({-1mm*cos(45)},{-1mm*sin(45)})$);
		\draw[thick] ($(aout) + ({1mm*cos(135)},{1mm*sin(135)})$) -- ($(aout) + ({-1mm*cos(135)},{-1mm*sin(135)})$);
\end{scope}

\node at ({37mm*cos(-120)},{37mm*sin(-120)}) {$c$} ;
\node at ({37mm*cos(60)},{37mm*sin(60)}) {$c'$} ;

\pgfmathsetmacro{\sizein}{35}
\pgfmathsetmacro{\sizeout}{50}
\pgfmathsetmacro{\sizemid}{(\sizein + \sizeout)/2}

\draw[thick] (0mm, \sizein*1mm) -- (0mm, \sizeout*1mm);

\pgfmathsetlengthmacro{\a}{\sizein*1mm*cos(\rotorangle + 90)}
\pgfmathsetlengthmacro{\b}{\sizein*1mm*sin(\rotorangle + 90)}
\pgfmathsetlengthmacro{\c}{\sizeout*1mm*cos(\rotorangle + 90)}
\pgfmathsetlengthmacro{\d}{\sizeout*1mm*sin(\rotorangle + 90)}

\draw[thick] (\a,\b) -- (\c,\d);

\draw[->,thick] (0mm,\sizemid*1mm) arc(90:{90+\rotorangle-2}:\sizemid*1mm);

\draw[->,thick] ({\sizemid*0.9mm*cos(90+\rotorangle-10)},{\sizemid*0.9mm*sin(90+\rotorangle-10)}) arc({90+\rotorangle-10}:{90+\rotorangle+10}:\sizemid*0.9mm);

\node at ({\sizemid*1.1mm*cos(90+\rotorangle/2)},{\sizemid*1.1mm*sin(90+\rotorangle/2)}) {$\theta$};

\node at ({\sizemid*1mm*cos(90+\rotorangle+10)},{\sizemid*1mm*sin(90+\rotorangle+10)}) {$\omega$};

% second rotor drawing <<<
\begin{scope}[xshift = 90mm]

\node (O) at (0,0) {};

\node[draw, circle, fill=gray!75!white, thick, minimum size=68mm] (graybg) at (0,0) {}; % SUM CIRCLE
\node[draw, circle, fill=white,         thick, minimum size=60mm] (whitefg) at (0,0) {}; % SUM CIRCLE

\draw[->, gray,thick] (0,-40mm) -- (0,40mm) ;
\draw[->, gray,thick] (-40mm,0) -- (40mm,0) ;

\pgfmathsetmacro{\sizein}{35}
\pgfmathsetmacro{\sizeout}{50}
\pgfmathsetmacro{\sizemid}{(\sizein + \sizeout)/2}

\draw[thick] (0mm, \sizein*1mm + 5mm) -- (0mm, \sizeout*1mm);

\pgfmathsetlengthmacro{\a}{(\sizein*1mm + 5mm)*cos(\rotorangle + 90)}
\pgfmathsetlengthmacro{\b}{(\sizein*1mm + 5mm)*sin(\rotorangle + 90)}
\pgfmathsetlengthmacro{\c}{\sizeout*1mm*cos(\rotorangle + 90)}
\pgfmathsetlengthmacro{\d}{\sizeout*1mm*sin(\rotorangle + 90)}

\draw[thick] (\a,\b) -- (\c,\d);

\draw[->,thick] (0mm,\sizemid*1mm) arc(90:{90+\rotorangle-2}:\sizemid*1mm);

\draw[->,thick] ({\sizemid*1.1mm*cos(90+\rotorangle-10)},{\sizemid*1.1mm*sin(90+\rotorangle-10)}) arc({90+\rotorangle-10}:{90+\rotorangle+10}:\sizemid*0.9mm);

\node at ({\sizemid*1.1mm*cos(90+\rotorangle/2)},{\sizemid*1.1mm*sin(90+\rotorangle/2)}) {$\theta$};

\node at ({\sizemid*1.1mm*cos(90+\rotorangle+10)},{\sizemid*1.1mm*sin(90+\rotorangle+10)}) {$\omega$};


% rotated DQ <<<
\begin{scope}[rotate=\rotorangle]

	\path [name path=outercircle] (0,0) circle (28.5mm) ; % SUM CIRCLE
	\path [name path = leftline]  (-2.5mm,-30mm) -- (-2.5mm,30mm);
	\path [name path = rightline] ( 2.5mm,-30mm) -- ( 2.5mm,30mm);

	\path[name path = leftmostline]  (-10mm,10mm) -- (-10mm,30mm);
	\path[name path = rightmostline] ( 10mm,10mm) -- ( 10mm,30mm);

	\path [name intersections={of=outercircle and leftmostline, by={southwest_intersection}}];
	\path [name intersections={of=outercircle and rightmostline, by={southeast_intersection}}];

	\path[name path = leftmostline]  (-10mm, 10mm) -- (-10mm, 30mm);
	\path[name path = rightmostline] ( 10mm, 10mm) -- ( 10mm, 30mm);

	\path[name path = botsouthline] ( -12mm, 22mm) -- ( 12mm, 22mm);

	\path [name intersections={of=botsouthline and rightmostline, by={southeast_T}}];

	\path [name intersections={of=botsouthline and leftmostline, by={southwest_T}}];

	\path [name intersections={of=botsouthline and leftline, by={southwest_inner_T}}];
	\path [name intersections={of=botsouthline and rightline, by={southeast_inner_T}}];

		% Compute angles and radius using \path let
		\node (A) at (southwest_intersection) {};
		\node (B) at (southeast_intersection) {};
		\node (C) at (southwest_T) {};
		\node (D) at (southeast_T) {};
		\node (E) at (southwest_inner_T) {};
		\node (F) at (southeast_inner_T) {};

		% Get coordinates for calculations
		\path (O); \pgfgetlastxy{\Ox}{\Oy}
		\path (A); \pgfgetlastxy{\Ax}{\Ay}
		\path (B); \pgfgetlastxy{\Bx}{\By}

		% Compute angles
		\pgfmathsetmacro{\angleA}{atan2(\Ay - \Oy,\Ax - \Ox)}
		\pgfmathsetmacro{\angleB}{atan2(\By - \Oy,\Bx - \Ox)}

		% Compute radius (distance from O to A)
		\pgfmathsetmacro{\radius}{veclen(\Ax - \Ox,\Ay - \Oy)/28.45274}

		% Draw the arc centered at O from A to B
		\draw[thick] (E.center) -- (C.center) -- ([shift=(\angleA:\radius)]O.center) arc(\angleA:\angleB:\radius) -- (D.center) -- (F.center);

	% SAME PROCESS FOR BOTTOM T

	\path[name path = leftmostline]  (-10mm,-10mm) -- (-10mm,-30mm);
	\path[name path = rightmostline] ( 10mm,-10mm) -- ( 10mm,-30mm);

	\path [name intersections={of=outercircle and leftmostline, by={southwest_intersection}}];

	\path [name intersections={of=outercircle and rightmostline, by={southeast_intersection}}];

	\path[name path = leftmostline]  (-10mm,-10mm) -- (-10mm,-30mm);
	\path[name path = rightmostline] ( 10mm,-10mm) -- ( 10mm,-30mm);

	\path[name path = botsouthline] ( -12mm,-22mm) -- ( 12mm,-22mm);

	\path [name intersections={of=botsouthline and rightmostline, by={southeast_T}}];
	\path [name intersections={of=botsouthline and leftmostline, by={southwest_T}}];
	\path [name intersections={of=botsouthline and leftline, by={southwest_inner_T}}];
	\path [name intersections={of=botsouthline and rightline, by={southeast_inner_T}}];

	% Compute angles and radius using \path let
		\node (G) at (southwest_intersection) {};
		\node (H) at (southeast_intersection) {};
		\node (I) at (southwest_T) {};
		\node (J) at (southeast_T) {};
		\node (K) at (southwest_inner_T) {};
		\node (L) at (southeast_inner_T) {};

		% Get coordinates for calculations
		\path (O); \pgfgetlastxy{\Ox}{\Oy}
		\path (G); \pgfgetlastxy{\Ax}{\Ay}
		\path (H); \pgfgetlastxy{\Bx}{\By}

		% Compute angles
		\pgfmathsetmacro{\angleA}{atan2(\Ay - \Oy,\Ax - \Ox)}
		\pgfmathsetmacro{\angleB}{atan2(\By - \Oy,\Bx - \Ox)}

		% Compute radius (distance from O to A)
		\pgfmathsetmacro{\radius}{veclen(\Ax - \Ox,\Ay - \Oy)/28.45274}

		% Draw the arc centered at O from A to B
		\draw[thick] (F.center) -- (L.center) -- (J.center) -- ([shift=(\angleB:\radius)]O.center) arc(\angleB:\angleA:\radius) -- (I.center) -- (K.center) -- (E.center);

	    \fill[fill=white] (-2.35mm,-20mm) rectangle (2.35mm,20mm);

\end{scope} %>>>

\pgfmathsetmacro{\axissize}{40}

\pgfmathsetlengthmacro{\a}{\axissize*1mm*cos(\rotorangle - 90)}
\pgfmathsetlengthmacro{\b}{\axissize*1mm*sin(\rotorangle - 90)}
\pgfmathsetlengthmacro{\c}{\axissize*1mm*cos(\rotorangle + 90)}
\pgfmathsetlengthmacro{\d}{\axissize*1mm*sin(\rotorangle + 90)}

\draw[->,thick,stewartpink] (\a,\b) -- (\c,\d);
\node[stewartpink] at ([shift=({4mm,0})]\c,\d) {$Q$};

\pgfmathsetlengthmacro{\a}{\axissize*1mm*cos(\rotorangle + 180)}
\pgfmathsetlengthmacro{\b}{\axissize*1mm*sin(\rotorangle + 180)}
\pgfmathsetlengthmacro{\c}{\axissize*1mm*cos(\rotorangle)}
\pgfmathsetlengthmacro{\d}{\axissize*1mm*sin(\rotorangle)}

\draw[->,thick,stewartpink] (\a,\b) -- (\c,\d);
\node[stewartpink] at ([shift=({2mm,2mm})]\c,\d) {$D$};

\end{scope} %>>>

\end{tikzpicture}
}
\caption
[Schematic of a salient-pole synchronous machine with the rotating $DQ$ frame as conceived in \cite{Park1929}.]
{Schematic of a salient-pole synchronous machine with the rotating $DQ$ frame as conceived in \cite{Park1929}. The left schematic shows the $a$, $b$ and $c$ stator wirings; in blue the rotor circuit with the field voltage $v_F$ and field current $v_D$. Park then creates two virtual coils $d$ and $q$ that translate the stator effect onto the rotor, generating the direct-quadrature $DQ$ rotating frame as in the right schematic.}
	\label{fig:park_transform_rotor}
\end{figure} %>>>

	A somewhat generalized definition of Park's transformation is shown in definition \ref{def:dq0_transform}, where the linear transformation is defined as a rotational transform as a function of an arbitrary angle $\theta$.

\begin{definition}[Park transform]\label{def:park_transform} The Park Transform takes an argument angle $\theta$ and delivers the rotating linear transformation

\begin{equation}
\mathbf{T_P}\left(\theta\right) = 
\left[\begin{array}{ccc}
\phantom{-}\cos\left(\theta\right) & \sin\left(\theta\right) & 0 \\[5mm]
-\sin\left(\theta\right) & \cos\left(\theta\right) & 0 \\[5mm]
0 & 0 & 1
\end{array}\right]
\end{equation}

\end{definition}

	Originally, Park's $dq$ axes referred to a set of orthogonal axes rotating at the rotor speed $\omega_s$ of a synchronous machine, that is, using a transformation angle of $\theta = \omega_s t$ (see figure \ref{fig:park_transform_rotor}); this eliminated the varying inductances arisen from the reluctances in synchronous machine analyses. Researchers then used Park's idea  and explored different reference frames for the $dq$ axes; \cite{Krause1965} later showed that all of the diference reference frames were particular cases of the Park Transform, using some arbitrary reference frame; this justifies defining the Park transform as a transformation of an arbitrary angle.

%-------------------------------------------------
\section{The $dq0$ Transform and the Three-Phase Dynamic Phasor} %<<<2

	It becomes now obvious that The composition of the $\alpha\beta\gamma$ and the Park transforms onto a balanced three-phase signal yields a very useful result: the quantity is transformed into a pair of continuous but not oscillating signals — one might see the literature define these as ``DC signals''. This composition was later named the ``$dq0$ Transform''.

\begin{definition}[Clarke-Park or $dq0$ transform]\label{def:dq0_transform} The Clarke-Park or $dq0$ Transform $\mathbf{T}$ is a rotating linear transformation, defined as the composition of the Clarke Transform followed by the Park Transform applied at an angle $\theta$:

\begin{equation} \mathbf{T}_\theta = \mathbf{T_C}\mathbf{T_P}\left(\theta\right) = \sqrt{\dfrac{2}{3}}
\left[\begin{array}{ccc}
\phantom{-}\cos\left(\theta\right) & \phantom{-}\cos\left(\theta - \dfrac{2\pi}{3}\right) & \phantom{-}\cos\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
-\sin\left(\theta\right) & -\sin\left(\theta - \dfrac{2\pi}{3}\right) & -\sin\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} 
\end{array}\right]
 \end{equation}

	\noindent and the inverse transform is given by

\begin{equation} \mathbf{T}^{-1}_\theta = \left(\mathbf{T}_\theta\right)^\transpose = 
\sqrt{\dfrac{2}{3}}\left[\begin{array}{ccc}
\cos\left(\theta\right)                   & -\sin\left(\theta\right)                   & \dfrac{1}{\sqrt{2}} \\[5mm]
\cos\left(\theta - \dfrac{2\pi}{3}\right) & -\sin\left(\theta - \dfrac{2\pi}{3}\right) & \dfrac{1}{\sqrt{2}} \\[5mm] 
\cos\left(\theta + \dfrac{2\pi}{3}\right) & -\sin\left(\theta + \dfrac{2\pi}{3}\right) & \dfrac{1}{\sqrt{2}}
\end{array}\right]
 \end{equation}
\end{definition}

	Similarly to figure \ref{fig:pll_example} one can also devise a block model of this transform In block schematics of control systems, the $dq0$ transform is generally represented as depicted in figure \ref{fig:dq0_block_modelling}. The block takes three arguments: phases $a$, $b$ and $c$ of an input quantity and an angle $\theta$, and performs the transformation as per definition \ref{def:dq0_transform} to yield the three $d$, $q$ and $0$ components. 

% PLL SUBSTYSTEM <<<
\begin{figure} 
\centering
\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]

\node [draw, minimum width=2cm, very thick, minimum height=2cm, left=0] (tab_block) {};

\draw (tab_block.south west) -- (tab_block.north east);

\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]tab_block.north west) {\large $abc$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]tab_block.south east) {\large $\alpha\beta\gamma$};


\node at ([shift=({-15mm, 7mm})]tab_block.west) (signalinputa) {$x_a(t)$};
\node at ([shift=({-15mm, 0})]tab_block.west) (signalinputb) {$x_b(t)$};
\node at ([shift=({-15mm,-7mm})]tab_block.west) (signalinputc) {$x_c(t)$};

\draw[->] (signalinputa.east) -- ([shift=({-1mm,0})] signalinputa -| tab_block.west);
\draw[->] (signalinputb.east) -- ([shift=({-1mm,0})] signalinputb -| tab_block.west);
\draw[->] (signalinputc.east) -- ([shift=({-1mm,0})] signalinputc -| tab_block.west);

\node [draw, minimum width=2cm, very thick, minimum height=2cm, right=2cm of tab_block] (abdq_block) {};
\draw (abdq_block.south west) -- (abdq_block.north east);
\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]abdq_block.north west) {\large $\alpha\beta\gamma$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]abdq_block.south east) {\large $dq$};

\draw[->] ([shift=({0, 7mm})]tab_block.east) -- ([shift=({-1mm, 7mm})]abdq_block.west) node[midway, above] {$x_\alpha(t)$};
\draw[->]                   (tab_block.east) -- ([shift=({-1mm, 0mm})]abdq_block.west) node[midway, above] {$x_\beta(t)$};
\draw[->] ([shift=({0,-7mm})]tab_block.east) -- ([shift=({-1mm,-7mm})]abdq_block.west) node[midway, above] {$x_\gamma(t)$};

\draw[->] ([shift=({0, 7mm})]abdq_block.east) -- ([shift=({10mm, 7mm})]abdq_block.east) node[right] {$x_d(t)$};
\draw[->] ([shift=({0, 0mm})]abdq_block.east) -- ([shift=({10mm, 0mm})]abdq_block.east) node[right] {$x_q(t)$};
\draw[->] ([shift=({0,-7mm})]abdq_block.east) -- ([shift=({10mm,-7mm})]abdq_block.east) node[right] {$x_0(t)$};

\node [draw, minimum width=1cm, very thick, minimum height=1cm, below=1cm of abdq_block] (omega_integrator) {$\int$};

\node [below=1cm of omega_integrator.south] (omega_signal) {$\omega(t)$};

\draw[->] (omega_signal.north) -- ([shift=({0,-1mm})]omega_integrator.south);
\draw[->] (omega_integrator.north) -- ([shift=({0,-1mm})]abdq_block.south) node [midway, right] {$\psi(t)$};

\end{tikzpicture}
\caption{Block model of the three-phase dq transform.}
\label{fig:dq0_block_modelling}
\end{figure}
%>>>

\begin{theorem}[$dq0$ Transform of balanced three-phase quantities]\label{theo:dq0_balanced_3p} %<<<
Let $\mathbf{x}$ be a balanced three-phase signal with modulus $m(t)$ and angle $\theta(t)$ such with apparent frequency $\omega(t)$ and apparent phase angle $\phi(t)$. Then adopt

\begin{equation} \psi(t) = \int_0^t \omega(x)dx \end{equation}

	as the angle of rotation of the $dq0$ transform $\mathbf{T}_\psi$. Then

\begin{equation} \mathbf{T}_\psi \mathbf{x} = \mathbf{T}_{\psi} \left(m\left[
	\begin{array}{c}
		\cos\left(\theta \right) \\[5mm]
		\cos\left(\theta  - \dfrac{2\pi}{3}\right) \\[5mm]
		\cos\left(\theta  + \dfrac{2\pi}{3}\right)
	\end{array}\right] \right) = 
m\sqrt{\dfrac{3}{2}}\left[
	\begin{array}{c}
		\cos\left(\phi\right) \\[5mm]
		\sin\left(\phi\right) \\[5mm]
		0
	\end{array}\right] 
\end{equation}

\end{theorem}

\textbf{Proof:} by direct calculation. First consider the Park transform of an arbitrary angle $\alpha$. Then
\begin{align}
\sqrt{\dfrac{3}{2}} x_d
	&= m\left[\raisebox{5mm}{}\cos\left(\theta\right)\cos\left(\alpha\right) + \right. \nonumber\\[3mm]
	&\hspace{1cm} +\cos\left(\theta - \dfrac{2\pi}{3}\right)\cos\left(\alpha - \dfrac{2\pi}{3}\right)  + \nonumber\\[3mm]
	&\left. \hspace{2cm} + \cos\left(\theta + \dfrac{2\pi}{3}\right)\cos\left(\alpha + \dfrac{2\pi}{3}\right) \right]
\end{align}

	From the Prostaph\ae resis Formulas, $\cos\left(a\right)\cos\left(b\right) = \frac{1}{2}\left[\cos\left(a-b\right) + \cos\left(a+b\right)\right]$:

\begin{align}
\sqrt{\dfrac{3}{2}} x_d
	=& \dfrac{m}{2}\left[\raisebox{5mm}{} \cos\left(\theta + \alpha\right) + \cos\left(\theta - \alpha\right) + \right. \nonumber\\[3mm]
	&\hspace{1cm} \cos\left(\theta + \alpha - \dfrac{4\pi}{3}\right) + \cos\left(\theta - \alpha\right) + \nonumber\\[3mm]
	&\hspace{2cm} \left. \cos\left(\theta + \alpha + \dfrac{4\pi}{3}\right) + \cos\left(\theta - \alpha\right)\right] = \nonumber\\[3mm]
	=& \dfrac{3}{2} m\cos\left(\theta - \alpha\right) \nonumber\\[3mm]
	x_d =& \sqrt{\dfrac{3}{2}}m\cos\left(\theta - \alpha\right)
\end{align}

	Much the same way, $\sin\left(a\right)\cos\left(b\right) = \frac{1}{2}\left[\sin\left(a+b\right) + \sin\left(a-b\right)\right]$:

\begin{align}
\sqrt{\dfrac{3}{2}}x_q &= \dfrac{1}{2}m \left[\raisebox{5mm}{} \cos\left(\theta\right)\sin\left(\alpha\right) \right.+ \nonumber\\[3mm]
	&\hspace{1cm} + \cos\left(\theta - \dfrac{2\pi}{3}\right)\sin\left(\alpha - \dfrac{2\pi}{3}\right) + \nonumber\\[3mm]
	&\hspace{2cm}\left. +\cos\left(\theta + \dfrac{2\pi}{3}\right)\sin\left(\alpha + \dfrac{2\pi}{3}\right)\right] =\nonumber\\[3mm]
	=& \dfrac{1}{2}m\left[\raisebox{5mm}{} \sin\left(\theta + \alpha\right) + \sin\left(\theta - \alpha\right) + \right.\nonumber\\[3mm]
	&\hspace{1cm} + \sin\left(\theta + \alpha - \dfrac{4\pi}{3}\right) + \sin\left(\theta - \alpha\right) + \nonumber\\[3mm]
	&\hspace{2cm} + \left. \sin\left(\theta + \alpha + \dfrac{4\pi}{3}\right) + \sin\left(\theta - \alpha\right) \right] = \nonumber\\[3mm]
	=& \dfrac{3}{2} m\sin\left(\theta - \alpha\right) \nonumber\\[3mm]
	x_q =& \sqrt{\dfrac{3}{2}}m\sin\left(\theta - \alpha\right)
\end{align}

	And 

\begin{equation} \sqrt{\dfrac{3}{2}}\mathbf{x}^{0} = m\left[\dfrac{1}{\sqrt{2}}\cos\left(\theta + \phi\right) + \dfrac{1}{\sqrt{2}}\cos\left(\omega t + \phi - \dfrac{2\pi}{3}\right) + \dfrac{1}{\sqrt{2}}\cos\left(\omega t + \phi + \dfrac{2\pi}{3}\right)\right] = 0 \end{equation}

	Finally, adopting the arbitrary angle $\alpha$ as  $\psi(t) = \displaystyle \int_{t_0}^t \omega(x)dx$ yields

\begin{align}
\left\{ \begin{array}{rl}
	x_d &= \sqrt{\dfrac{3}{2}}m(t)\cos\left[\theta(t) - \psi(t)\right] = \sqrt{\dfrac{3}{2}}m(t)\cos\left[\phi(t)\right] \\[5mm]
	x_d &= \sqrt{\dfrac{3}{2}}m(t)\sin\left[\theta(t) - \psi(t)\right] = \sqrt{\dfrac{3}{2}}m(t)\sin\left[\phi(t)\right] \\[5mm]
	x_0 &= 0,
\end{array}\right.
\end{align}


	which is the result wanted. \hfill $\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	Theorem \ref{theo:dq0_balanced_3p} then shows that a balanced three-phase quantity is equivalent to a two-dimensional quantity $x_d,x_q$ plus a indentically null zero-sequence component. Therefore, if we use the same complexification operator $\rho$ as theorem \ref{theo:rho_diff_inf}, we can disregard the zero-sequence component without loss of information to yield a complex number $X(t) = x_d + jx_q$.

\begin{definition}[Three-phase Dynamic Phasor Transform] \label{theo:clarke_park_phasor_transform}%<<<
	Let $\mathbf{x}$ be a three-phase signal with modulus $m(t)$, apparent frequency $\omega(t)$ and apparent phase $\phi(t)$. Then define the Three-Phase Dynamic Phasor Transform as 

\begin{equation}
	\mathbf{P_{3\phi}^{\omega}}\left[\cdot\right] \vcentcolon \left\{\begin{array}{rcl}
	\left[\mathbb{R}\to \mathbb{R}^3\right] &\to& \left[\mathbb{R}\to \mathbb{C}\right]\\[3mm]
	\mathbf{x}(t) &\mapsto& X\left(t\right)
\end{array}\right. ,
\end{equation}
\end{definition}
%>>>

	It is important to note that the definition of $\mathbf{P_{3\phi}}$ states that the domain is the set of three-dimensional real functions, and not necessarily the balanced ones. Naturally, the matrix $\mathbf{T}_{\psi}$ can be applied to any three-dimensional vectors; in the case of balanced ones, the zero-sequence component will be null, hence $\mathbf{P_{3\phi}}\left[\mathbf{x}\right] = X(t)$ completely reconstructs $\mathbf{x}(t)$. In other words, if $x_0(t)$ is ignored, $\mathbf{P_{3\phi}}$ is invertible.

	Here, ``ignored'' means ``understood''. In practice, if a certain signal $x_0(t)$ is picked, then the image of $\mathbf{P_{3\phi}}$ through the entire $\left[\mathbb{R}\to\mathbb{C}\right]$ forms an equivalence class. More specifically, the image of $\mathbf{T}_\psi$ is homeomorphic to the quotient group $\left[\mathbb{R}\to\mathbb{C}\right]/\left[\mathbb{R}\to\mathbb{R}\right]$, that is, any signal produced by the transformation can be described by a complex function (its Dynamic Phasor) and a real function (its zero-sequence component). This means that if two signals have the same zero-sequence signal, then their corresponding Dynamic Phasors are unique; hence the idea of a ``ignored'' zero-sequence signal. Most of the times, this ``understood'' signal is the null function, because the most studied subgroup of three-phase signals is perhaps the one that contains balanced signals.

	Also importantly, both the transform and its inverse are linear due to the linearity and invertibility of the operations involved; therefore, combining the Clarke Transform, the Park Transform and the complexification functional we obtain $\mathbf{P_{3\phi}}$, as shown in Figure \ref{fig:3p_dq0_block_modelling}.

% PLL SUBSTYSTEM <<<
\begin{figure} 
\centering
\scalebox{0.75}{
\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]

\node [draw, minimum width=2cm, very thick, minimum height=2cm, left=0] (tab_block) {};

\draw (tab_block.south west) -- (tab_block.north east);

\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]tab_block.north west) {\large $abc$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]tab_block.south east) {\large $\alpha\beta\gamma$};


\node at ([shift=({-15mm, 7mm})]tab_block.west) (signalinputa) {$x_a(t)$};
\node at ([shift=({-15mm, 0})]tab_block.west) (signalinputb) {$x_b(t)$};
\node at ([shift=({-15mm,-7mm})]tab_block.west) (signalinputc) {$x_c(t)$};

\draw[->] (signalinputa.east) -- ([shift=({-1mm,0})] signalinputa -| tab_block.west);
\draw[->] (signalinputb.east) -- ([shift=({-1mm,0})] signalinputb -| tab_block.west);
\draw[->] (signalinputc.east) -- ([shift=({-1mm,0})] signalinputc -| tab_block.west);

\node [draw, minimum width=2cm, very thick, minimum height=2cm, right=2cm of tab_block] (abdq_block) {};

\node [draw, rounded corners, stewartblue, very  thick, dashed, minimum width=11cm, minimum height=3cm] at ([shift=({-1mm,0})]abdq_block.center)  (invol) {};

\draw (abdq_block.south west) -- (abdq_block.north east);
\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]abdq_block.north west) {\large $\alpha\beta\gamma$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]abdq_block.south east) {\large $dq$};

\draw[->] ([shift=({0, 7mm})]tab_block.east) -- ([shift=({-1mm, 7mm})]abdq_block.west) node[midway, above] {$x_\alpha(t)$};
\draw[->]                   (tab_block.east) -- ([shift=({-1mm, 0mm})]abdq_block.west) node[midway, above] {$x_\beta(t)$};
\draw[->] ([shift=({0,-7mm})]tab_block.east) -- ([shift=({-1mm,-7mm})]abdq_block.west) node[midway, above] {$x_\gamma(t)$};

\node [draw, minimum width=1cm, very thick, minimum height=1cm, below=1cm of abdq_block] (omega_integrator) {$\int$};

\node [below=1cm of omega_integrator.south] (omega_signal) {$\omega(t)$};
\draw[->] (omega_signal.north) -- ([shift=({0,-1mm})]omega_integrator.south);
\draw[->] (omega_integrator.north) -- ([shift=({0,-1mm})]abdq_block.south) node [midway, right] {$\psi(t)$};

\node [draw, minimum width=15mm, very thick, minimum height=15mm] (rho_block) at ([shift=({30mm,3.5mm})]abdq_block.east) {$\rho$};

\node at ([shift=({0, 7mm})] abdq_block.east) (xdout) {};
\node at ([shift=({0, 0mm})] abdq_block.east) (xqout) {};

\draw[->] (xdout.center) -- ([shift=({-1mm, 0mm})] xdout -| rho_block.west) node[above,midway] {$x_d(t)$};
\draw[->] (xqout.center) -- ([shift=({-1mm, 0mm})] xqout -| rho_block.west) node[above,midway] {$x_q(t)$};

\draw[->] (rho_block.east) -- ([shift=({15mm,0mm})]rho_block.east) node[right] (xphasorout) {$X(t)$};

\node at ([shift=({0,-7mm})]abdq_block.east) (x0out) {};
\draw[->] (x0out.center) -- (x0out -| xphasorout.west) node[right] {$x_0(t)$};

\node [draw, stewartblue, minimum width=2cm, very thick, minimum height=2cm, below=5cm of abdq_block] (p3p_block) {$\mathbf{P_{3\phi}}$};
\node at ([shift=({-15mm, 7mm})]p3p_block.west) (signalinputa_p3p) {$x_a(t)$};
\node at ([shift=({-15mm, 0})]  p3p_block.west) (signalinputb_p3p) {$x_b(t)$};
\node at ([shift=({-15mm,-7mm})]p3p_block.west) (signalinputc_p3p) {$x_c(t)$};

\draw[->] (signalinputa_p3p.east) -- ([shift=({-1mm,0})] signalinputa_p3p -| p3p_block.west);
\draw[->] (signalinputb_p3p.east) -- ([shift=({-1mm,0})] signalinputb_p3p -| p3p_block.west);
\draw[->] (signalinputc_p3p.east) -- ([shift=({-1mm,0})] signalinputc_p3p -| p3p_block.west);

\node [draw, minimum width=1cm, very thick, minimum height=1cm, below=1cm of p3p_block] (omega_integrator_p3p) {$\int$};
\node [below=1cm of omega_integrator_p3p.south] (omega_signal_p3p) {$\omega(t)$};
\draw[->] (omega_signal_p3p.north) -- ([shift=({0,-1mm})]omega_integrator_p3p.south);
\draw[->] (omega_integrator_p3p.north) -- ([shift=({0,-1mm})]p3p_block.south) node [midway, right] {$\psi(t)$};

\draw[->] ([shift=({0, 3.33mm})]p3p_block.east) -- ([shift=({10mm, 3.33mm})]p3p_block.east) node[right] {$X(t)$};
\draw[->] ([shift=({0,-3.33mm})]p3p_block.east) -- ([shift=({10mm,-3.33mm})]p3p_block.east) node[right] {$x_0(t)$};

\draw[dashed,stewartblue] (invol.south east) -- (p3p_block.north east);
\draw[dashed,stewartblue] (invol.south west) -- (p3p_block.north west);

\end{tikzpicture}
}
\caption{Three-phase Dynamic Phasor Transform block model.}
\label{fig:3p_dq0_block_modelling}
\end{figure}
%>>>

%------------------------------------------------
\subsection{Three-Phase Dynamic Phasors as representations of solutions of ODEs} %<<<2

	Having constructed the Three-Phase Dynamic Phasor Transform, we now want to prove that this transform is able to translate linear systems in time to phasorial systems in the complex space, that is, we want to prove the three-phase converse of theorem \ref{theo:1p_ode_solution}. In order to do this, we use lemmas \ref{theo:dq_1p_diff} and \ref{lemma:1p_t_ndifftminus_product} and adapt them to a three-phase scenario. These two lemmas are then applied to theorem \ref{theo:3p_ode_solution} to yield the required result.

\begin{lemma}[n-th order time differentiation of $dq0$ transformed 3$\phi$ quantities]\label{theo:dq0_3p_diff}%<<<
Let $n\in\mathbb{N}^*$, $\mathbf{x}$ be a 3$\phi$ quantity, $\mathbf{T}_\theta$ the $dq0$ Transform operator where $\theta(t)$ is $C^n$-class, and $\mathbf{y} = \mathbf{T}_\theta\mathbf{x}$. Then

\begin{equation} \dfrac{d^n \mathbf{y}}{dt^n} = \dfrac{d^n\left(\mathbf{T}_\theta \mathbf{x}\right)}{dt^n} = \sum\limits_{k=0}^{n} {n\choose k} \left(\dfrac{d^{k} \mathbf{T}_\theta}{dt^k}\right) \left(\dfrac{d^{\left(n-k\right)} \mathbf{x}}{dt^{\left(n-k\right)}}\right), \end{equation}

	and

\begin{equation} \dfrac{d^n\mathbf{x}}{dt^n} = \dfrac{d^n\left(\mathbf{T}^{-1}_\theta \mathbf{x}\right)}{dt^n} = \sum\limits_{k=0}^{n} {n\choose k} \left(\dfrac{d^{k} \mathbf{T}^{-1}_\theta}{dt^k}\right) \left(\dfrac{d^{\left(n-k\right)} \mathbf{y}}{dt^{\left(n-k\right)}}\right) \end{equation}

	Particularly for $n=1$,

\begin{equation} \dfrac{d\mathbf{x}}{dt} = \dfrac{d}{dt} \left(\mathbf{T}^{-1}_\theta \mathbf{y}\right) = \mathbf{T}^{-1}_\theta \dfrac{d\mathbf{y}}{dt} + \dfrac{d\mathbf{T}^{-1}_\theta}{dt} \mathbf{y}, \end{equation}

and

\begin{equation} \dfrac{d\mathbf{y}}{dt} = \dfrac{d}{dt} \left(\mathbf{T}_\theta \mathbf{x}\right) = \mathbf{T}_\theta \dfrac{d\mathbf{x}}{dt} + \dfrac{d\mathbf{T}_\theta}{dt} \mathbf{x}, \end{equation}
 
	where

\begin{equation}
        \dfrac{d\mathbf{T}^{-1}_{\theta} }{dt} =
\sqrt{\dfrac{3}{2}}\dfrac{d\theta}{dt}
\left[\begin{array}{ccc}
         -\sin\left(\theta\right)                    & \cos\left(\theta\right)                    & 0 \\[5mm]
         -\sin\left(\theta - \dfrac{2\pi}{3} \right) & \cos\left(\theta - \dfrac{2\pi}{3} \right) & 0 \\[5mm]
         -\sin\left(\theta + \dfrac{2\pi}{3} \right) & \cos\left(\theta + \dfrac{2\pi}{3} \right) & 0 
\end{array}\right]
\end{equation}

	and

\begin{equation}
        \dfrac{d\mathbf{T}_\theta }{dt} =
\sqrt{\dfrac{2}{3}}\dfrac{d\theta}{dt}
\left[\begin{array}{ccc}
         -\sin\left(\theta\right)                    & -\sin\left(\theta - \dfrac{2\pi}{3} \right) & -\sin\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
          \cos\left(\theta\right)                    &  \cos\left(\theta - \dfrac{2\pi}{3} \right) &  \cos\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
          0                                          &  0                                          & 0
\end{array}\right]
\end{equation}
\end{lemma}
\textbf{Proof:} identical to lemma \ref{theo:dq0_3p_diff}.
\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

\begin{lemma} \label{lemma:t_ndifftminus_product}%<<<

	Let $n\geq 1$ be a natural and let $\mathbf{T}_\theta$ and $\mathbf{T}_\theta$ denote the Clarke-Park Transform and its inverse of an angle $\theta$, where $\theta$ is n-th order differentiable. Then

\begin{equation} \mathbf{T}_\theta \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt^n} = \sum\limits_{k=1}^n \mathbf{S}_k B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right), \end{equation}

	where $B_{\left(n,k\right)}$ is the incomplete exponential Bell Polynomial and

\begin{equation}
\mathbf{S}_k = 
\left[\begin{array}{ccc}
 \cos\left( \dfrac{k\pi}{2}\right) &  -\sin\left(\dfrac{k\pi}{2}\right) & 0\\[5mm]
 \sin\left( \dfrac{k\pi}{2}\right) &  \phantom{-}\cos\left(\dfrac{k\pi}{2}\right) & 0\\[5mm]
0 & 0 & 0
\end{array}\right] \text{, for } k\geq 0 \text{ and } \mathbf{S}_0 = \mathbf{I_3}
\end{equation}

	Notably, $\mathbf{S}_k$ can be written in terms of the matrices $\mathbf{G}_k$ of lemma \ref{lemma:1p_t_ndifftminus_product} as

\begin{equation}
\mathbf{S}_k = 
\left[\begin{array}{ccc} \\[-1mm]
\multicolumn{2}{c}{\left[\mathbf{G}_k\right]} & 0\\[5mm]
0 & 0 & 0
\end{array}\right], \text{ for } k\geq 1 \text{ and }
%
\mathbf{S}_0 = 
\left[\begin{array}{ccc} \\[-1mm]
\multicolumn{2}{c}{\left[\mathbf{G}_0\right]} & 0\\[5mm]
0 & 0 & 1
\end{array}\right]. \label{eq:sk_gk_equiv}
\end{equation}

	Particularly for $n=1$,

\begin{equation} \mathbf{T}_\theta\dfrac{d\mathbf{T}_\theta^{-1}}{dt} = \dfrac{d\theta}{dt} \left[\begin{array}{ccc}    0 & -1 & 0 \\[5mm] 1 & 0  & 0 \\[5mm]  0 & 0  & 0 \end{array}\right] \end{equation}

\end{lemma}
\textbf{Proof:} the first-order case can be obtained by direct computation as

\begin{align}
\mathbf{T}_\theta\dfrac{d\mathbf{T}^{-1}_\theta}{dt} &= \dot{\theta}
\left[\begin{array}{ccc}
\phantom{-}\cos\left(\theta\right) & \phantom{-}\cos\left(\theta - \dfrac{2\pi}{3}\right) & \phantom{-}\cos\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
-\sin\left(\theta\right) & -\sin\left(\theta - \dfrac{2\pi}{3}\right) & -\sin\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} 
\end{array}\right]
\left[\begin{array}{ccc}
-\sin\left(\theta\right)                   & -\cos\left(\theta\right)                   & 0 \\[5mm]
-\sin\left(\theta - \dfrac{2\pi}{3}\right) & -\cos\left(\theta - \dfrac{2\pi}{3}\right) & 0 \\[5mm] 
-\sin\left(\theta + \dfrac{2\pi}{3}\right) & -\cos\left(\theta + \dfrac{2\pi}{3}\right) & 0 
\end{array}\right] \nonumber\\[5mm]
%
&=\dot{\theta}\left[\begin{array}{ccc}
0 & -1 & 0 \\[5mm]
1 &  0 & 0 \\[5mm]
0 &  0 & 0
\end{array}\right]
\end{align}

	For an arbitrary order $n \geq 1$, one needs to use the Faà Di Bruno's formula \pcite{DiBruno1855} for the n-th order Chain Rule. The formula states that, for two single-variable n-th order differentiable functions $f$ and $g$, the chain rule is given by

\begin{equation} \dfrac{d^n}{dx^n} f\left(g\left(x\right)\right)= \sum\limits_{k=0}^n f^{\left(k\right)}\left(g\left(x\right)\right) B_{\left(n,k\right)}\left(g'\left(x\right),g''\left(x\right),...,g^{\left(n-k+1\right)}\left(x\right)\right), \end{equation}

	where the $B_{\left(n,k\right)}$ are the incomplete exponential Bell Polynomials. Consider $t^{-1}_{\left(i,j\right)}$ as the $i,j$ element of $\mathbf{T}^{-1}$. Then	

\begin{equation} \dfrac{d^n}{dt^n} t^{-1}_{\left(i,j\right)} \left(\theta\left(t\right)\right)= \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(i,j\right)}\left(\theta\right)}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right). \end{equation}

	But because the indexes $n$ and $k$ are not related to $i$ and $j$, 

\footnotesize
\begin{gather} \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt^n} = \nonumber\\[5mm]
	\left[\begin{array}{ccc}
\sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(1,1\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(1,2\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(1,3\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) \\[5mm]
\sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(2,1\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(2,2\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(2,3\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) \\[5mm]
\sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(3,1\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(3,2\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) & \sum\limits_{k=0}^n \dfrac{d^k t^{-1}_{\left(3,3\right)}}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) 
\end{array}\right] = \nonumber\\[5mm]
%
	= \sum\limits_{k=0}^n  B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right)\left[\begin{array}{ccc}
\dfrac{d^k t^{-1}_{\left(1,1\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(1,2\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(1,3\right)}}{d\theta^k} \\[5mm]
\dfrac{d^k t^{-1}_{\left(2,1\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(2,2\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(2,3\right)}}{d\theta^k} \\[5mm]
\dfrac{d^k t^{-1}_{\left(3,1\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(3,2\right)}}{d\theta^k} & \dfrac{d^k t^{-1}_{\left(3,3\right)}}{d\theta^k} 
\end{array}\right]
\end{gather}
\normalsize

	Which in matrix form means

\begin{equation} \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt^n} = \sum\limits_{k=0}^n \dfrac{d^k\mathbf{T}^{-1}_\theta}{d\theta^k} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right). \end{equation}

	But knowing that

\begin{equation}
\left\{\begin{array}{l}
\dfrac{d^n \cos\left(\theta\right)}{d\theta^n} = \cos\left(\theta + \dfrac{n\pi}{2}\right) \\[5mm]
\dfrac{d^n \sin\left(\theta\right)}{d\theta^n} = \sin\left(\theta + \dfrac{n\pi}{2}\right)
\end{array}\right. .
\end{equation}

	Here we must remove the case $k=0$ because, for $k\geq 1$, the third column of $d^k\mathbf{T}^{-1}_\theta/d\theta^k$ is zero due to the differentiated constants, but this does not happen at $k = 0$. In this case, $d^0\mathbf{T}^{-1}_\theta/d\theta^0 = \mathbf{T}^{-1}$ and $B_{(n,0)} = 1$ for $n=0$ and $B_{(n,0)} = 0$ if else. For $n\geq 1$,

\begin{equation} \dfrac{d^k \mathbf{T}^{-1}_\theta}{d\theta^k} =  \mathbf{K}_{\left(\theta + \frac{k\pi}{2}\right)}.\end{equation}

	where $\mathbf{K}$ is equal to $\mathbf{T}^{-1}$ but with a null third column because for $n\geq 1$ the third column is composed of differentiated constants. Therefore

\begin{equation} \dfrac{d^n\mathbf{T}^{-1}_\theta}{dt} = \sum\limits_{k=0}^n \mathbf{K}_{\left(\theta + \frac{k\pi}{2}\right)} B_{\left(n,k\right)}\left(\dot{\theta},\ddot{\theta},...,\theta^{(n-k+1)}\right) \end{equation}

	Now calculate the matrix multiplication:

\small
\begin{align}
\mathbf{T}\mathbf{K}_{\left(\theta + \frac{k\pi}{2}\right)} = 
\left[\begin{array}{ccc}
\cos\left(\theta\right) & \cos\left(\theta - \dfrac{2\pi}{3}\right) & \cos\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
\sin\left(\theta\right) & \sin\left(\theta - \dfrac{2\pi}{3}\right) & \sin\left(\theta + \dfrac{2\pi}{3}\right) \\[5mm]
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} 
\end{array}\right]
\left[\begin{array}{ccc}
\cos\left(\theta + \dfrac{k\pi}{2}\right)                  & \sin\left(\theta + \dfrac{k\pi}{2}\right)                   & 0 \\[5mm]
\cos\left(\theta + \dfrac{k\pi}{2}- \dfrac{2\pi}{3}\right) & \sin\left(\theta + \dfrac{k\pi}{2} - \dfrac{2\pi}{3}\right) & 0 \\[5mm] 
\cos\left(\theta + \dfrac{k\pi}{2}+ \dfrac{2\pi}{3}\right) & \sin\left(\theta + \dfrac{k\pi}{2} + \dfrac{2\pi}{3}\right) & 0 
\end{array}\right] \nonumber\\[5mm]
\end{align}
\normalsize

	Computing the elements of $\mathbf{TK}$ is done through simple calculations  repeat the ones of the proof for theorem \ref{theo:dq0_balanced_3p}. For an arbitrary $\alpha$,

\begin{align}
\mathbf{T}\mathbf{K}_{\left(\alpha\right)} = 
\left[\begin{array}{ccc}
 \cos\left(\theta-\alpha\right) &  \sin\left(\theta - \alpha\right) & 0\\[5mm]
-\sin\left(\theta-\alpha\right) & -\cos\left(\theta - \alpha\right) & 0\\[5mm]
0 & 0 & 0
\end{array}\right]
\end{align}

	Therefore for $\alpha = \theta + k\pi/2$,

\begin{align}
\mathbf{T}\mathbf{K}_{\left(\theta + \frac{k\pi}{2}\right)} = 
\left[\begin{array}{ccc}
 \cos\left(-\dfrac{k\pi}{2}\right) &  \sin\left(-\dfrac{k\pi}{2}\right) & 0\\[5mm]
-\sin\left(-\dfrac{k\pi}{2}\right) & -\cos\left(-\dfrac{k\pi}{2}\right) & 0\\[5mm]
0 & 0 & 0
\end{array}\right]
= 
\left[\begin{array}{ccc}
 \cos\left( \dfrac{k\pi}{2}\right) &  -\sin\left(\dfrac{k\pi}{2}\right) & 0\\[5mm]
 \sin\left( \dfrac{k\pi}{2}\right) &   \cos\left(\dfrac{k\pi}{2}\right) & 0\\[5mm]
0 & 0 & 0
\end{array}\right] .
\end{align}

	Call this matrix $\mathbf{S_k}$ for $k\geq 1$. For the case $k = 0,\ \mathbf{K}_{\left(\theta + \frac{0\pi}{2}\right)} = \mathbf{T}^{-1}_{\theta}$, meaning $\mathbf{S}_0 = \mathbf{I}_3$ the identity matrix. \hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

\begin{theorem}[Solutions to LTI ODEs with three-phase forcing] \label{theo:3p_ode_solution}%<<<

	Let $m\left(t\right),\theta\left(t\right)\in\left[\mathbb{R}\to\mathbb{R}\right]$ and consider the Hurwitz stable linear ODE with a three-phase phasorial forcing:

\begin{equation} \sum\limits_{k=0}^{n} \alpha_k \mathbf{x}^{\left(k\right)} - \mathbf{f_3}(t) = 0, \label{eq:theo_3p_ode_solution_original_ode}\end{equation}

	\noindent where $\mathbf{x},\mathbf{f_3}\in\left[\mathbb{R}\to\mathbb{R}^3\right]$ with a set of initial conditions $x_0,x'_0,...,x^{(n-1)}_0$. Let $\omega(t)$ be a $C^{\left(n-1\right)}$-class real function, and consider the set of decoupled ODEs of the ``dq equivalent'' and system with a zero-sequence

\begin{equation}
\left\{\begin{array}{l}
	\displaystyle \sum\limits_{i=0}^n \mathbf{K}_i (t) \dfrac{d^i \mathbf{z}_{dq}}{dt^i} - \mathbf{f}_{dq}  = \left[\begin{array}{c} 0 \\[3mm] 0 \end{array}\right]\\[5mm]
	\displaystyle \sum\limits_{i=0}^n \eta_i(t) \dfrac{d^i z_0}{dt^i} - f_0 = 0
\end{array}\right. , \label{eq:theo_3p_ode_solution_dq_equiv}
\end{equation}

	\noindent with a set of initial conditions $(\mathbf{z}_{dq})_0,(\mathbf{z}'_{dq})_0,...,(\mathbf{z}^{(n-1)}_{dq})_0$ , where $\mathbf{f}_{dq}$ is the $dq$ transform of the forcing at the frequency $\omega(t)$,

\begin{gather}
	\mathbf{K}_i(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right] \\[3mm]
	\eta_i(t) = \sum\limits_{k=i}^{k} \alpha_k {k\choose p} \left[\sum\limits_{c=0}^{k-i} B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right)\right] 
\end{gather}

	\noindent where the $B_{\left(i,j\right)}$ are the incomplete exponential Bell Polynomials and and $\mathbf{G}_k$ are calculated as

\begin{equation}
\mathbf{G}_k = 
\left[\begin{array}{ccc}
 \cos\left(\dfrac{k\pi}{2}\right) &  -\sin\left(\dfrac{k\pi}{2}\right) \\[5mm]
 \sin\left(\dfrac{k\pi}{2}\right) & \phantom{-} \cos\left(\dfrac{k\pi}{2}\right)
\end{array}\right]
\end{equation}

	Then there exist two positive reals $a,b$ such that the solution $x$ to the original ODE \eqref{eq:theo_3p_ode_solution_original_ode} satisfies

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq0}\right\rVert \leq ae^{-bt}, \label{eq:theo_3p_ode_solution_exp}\end{equation}

	\noindent with $\mathbf{z}_{dq0}$ is the unique solution to the dq system \eqref{eq:theo_3p_ode_solution_dq_equiv}. Reestated, the solution $\mathbf{z}_{\alpha\beta\gamma}$ reconstructed by \eqref{eq:theo_3p_ode_solution_dq_equiv} is the globally steady-state stable solution of \eqref{eq:theo_3p_ode_solution_original_ode}.

\end{theorem}
\textbf{Proof:} consider the original LTI ODE

\begin{equation} \sum\limits_{k=0}^n \alpha_k \mathbf{x}^{\left(k\right)} - \mathbf{f_3}(t) = 0.\end{equation}

	By hypothesis this system is Hurwitz stable, that is, the solution $x(t)$ tends exponentially to a particular solution: $\left\lVert x(t) - x_p(t)\right\rVert \leq ae^{-bt}$ for some two reals $a$ and $b$. Finding a particular solution $z(t)$, let $z_0,z'_0,...,z^{(n-1)}_0$ the initial conditions of the particular solution. Using $\mathbf{T}_\psi$ transform to generate an equivalent dq0 ODE:

\begin{equation} \sum\limits_{k=0}^n \alpha_k \mathbf{T}_\psi\left(\mathbf{T}_\psi^{-1}\mathbf{z}_{dq0}\right)^{\left(k\right)} - \mathbf{f}_{dq0} = 0 \end{equation}

	Apply lemma \ref{theo:dq0_3p_diff}:

\begin{equation} \sum\limits_{k=0}^n \alpha_k\left\{\mathbf{T}_\psi\left[ \sum\limits_{p=0}^{k} {k\choose p} \left(\dfrac{d^{p} \mathbf{T}^{-1}_\psi}{dt^p}\right) \left(\dfrac{d^{\left(k-p\right)} \mathbf{z}_{dq0}}{dt^{\left(k-p\right)}}\right) \right]\right\} - \mathbf{f}_{dq0} = 0 \end{equation}

	And because both $\mathbf{T}$ and $\mathbf{T}^{-1}$ are linear,

\begin{equation} \sum\limits_{k=0}^n \alpha_k \sum\limits_{p=0}^{k} {k\choose p} \mathbf{T}_\psi\left[\left(\dfrac{d^{\left(k-p\right)} \mathbf{T}^{-1}_\psi}{dt^{\left(k-p\right)}}\right) \left(\dfrac{d^p \mathbf{z}_{dq0}}{dt^p}\right) \right] - \mathbf{f}_{dq0} = 0 \end{equation}

	Now apply lemma \ref{lemma:t_ndifftminus_product}:

\begin{equation} \sum\limits_{k=0}^n \alpha_k \left\{\sum\limits_{p=0}^{k} {k\choose p} \left[\sum\limits_{c=0}^{k-p} \mathbf{S}_c B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right) \right] \left(\dfrac{d^p \mathbf{z}_{dq0}}{dt^p }\right)\right\} - \mathbf{f}_{dq0} = 0 \end{equation}

	To isolate the derivatives of $\mathbf{z}_{dq}$, one must solve the triangular sum of this equation. The 0-th derivatives are present at all $k$ indexes; the first, for the $k$ indexes $1$ through $n$; the second for $2$ to $n$. In general, the i-th derivative is present for indexes $k$ from $i$ to $n$.

\begin{equation} \sum\limits_{i=0}^n \left\{\sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{S}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right]\right\} \left(\dfrac{d^i \mathbf{z}_{dq0}}{dt^i }\right) - \mathbf{f}_{dq0} = 0 . \label{eq:3p_original_ode_complete}\end{equation}

	We now note that the matrices $\mathbf{S}_c$ have null third row and column, except for $c = 0$ as per \eqref{eq:sk_gk_equiv}, and can be expressed as a block composition of the $\mathbf{G}_c$. Thus we use \eqref{eq:sk_gk_equiv} and separate the case $c=0$ to yield

\small
\begin{gather}
	\sum\limits_{c=0}^{k-1} \mathbf{S}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right) = \nonumber\\[3mm]
	\left[\begin{array}{ccc} \\[-1mm] \multicolumn{2}{c}{\left[\mathbf{G}_0\right]} & 0\\[5mm] 0 & 0 & 1 \end{array}\right] B_{\left(k-i,0\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p)}\right) + \sum\limits_{c=1}^{k-i} \left[\begin{array}{ccc} \\[-1mm] \multicolumn{2}{c}{\left[\mathbf{G}_c\right]} & 0\\[5mm] 0 & 0 & 0 \end{array}\right] B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right)
\end{gather}
\normalsize

	\noindent and one notes that the fact that $\mathbf{G}_c$ is isolated in a block and that $\mathbf{G}_0$ has the single unit element on the bottom right makes this equation equivalent to two de-coupled equations, one bi-dimensional in the dq frame and another single-dimensional in the zero-sequence:

\begin{equation}
	\sum\limits_{c=0}^{k-i} \mathbf{S}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right) = 
\left[\begin{array}{c}
\displaystyle\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right) \\[5mm]
\displaystyle\sum\limits_{c=0}^{k-i} B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right)
\end{array}\right] .
\end{equation}

	Thus \eqref{eq:3p_original_ode_complete} is equivalent to two de-coupled equations:

\begin{equation}
\left\{\begin{array}{l}
	\displaystyle \sum\limits_{i=0}^n \left\{\sum\limits_{k=i}^{n} \alpha_k {k\choose p} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right) \right]\right\} \left(\dfrac{d^i \mathbf{z}_{dq}}{dt^i}\right) - \mathbf{f}_{dq}  = \left[\begin{array}{c} 0 \\[3mm] 0 \end{array}\right]\\[5mm]
	\displaystyle \sum\limits_{i=0}^n \left\{\sum\limits_{k=i}^{k} \alpha_k {k\choose p} \left[\sum\limits_{c=0}^{k-i} B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right)\right] \right\} \dfrac{d^i z_0}{dt^i} - f_0 = 0
\end{array}\right.
\end{equation}

	Finally, we group the terms inside the sums as

\begin{gather}
	\mathbf{K}_i(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} \mathbf{G}_c B_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right] \\[3mm]
	\eta_i(t) = \sum\limits_{k=i}^{k} \alpha_k {k\choose p} \left[\sum\limits_{c=0}^{k-i} B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right)\right] 
\end{gather}

	yielding

\begin{equation}
\left\{\begin{array}{l}
	\displaystyle \sum\limits_{i=0}^n \mathbf{K}_i (t) \dfrac{d^i \mathbf{z}_{dq}}{dt^i} - \mathbf{f}_{dq}  = \left[\begin{array}{c} 0 \\[3mm] 0 \end{array}\right]\\[5mm]
	\displaystyle \sum\limits_{i=0}^n \eta_i(t) \dfrac{d^i z_0}{dt^i} - f_0 = 0
\end{array}\right. .
\end{equation}

	Therefore, $z(t) = \mathbf{T}^{-1}_{\psi(t)}\mathbf{z}_{dq0}$ is a particular solution to the original system, and \eqref{eq:theo_3p_ode_solution_exp} follows. \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	Furthermore, it is simple to see that we can apply the results of subsection \ref{subsec:complexification} to transform this theorem into a complex version:

\begin{theorem}[Complex equivalence of three-phase phasorially excited LTI ODEs]\label{corollary:3p_complex_equivalence_phasorialodes} %<<<

	Take the three-phase LTI ODE \eqref{eq:theo_3p_ode_solution_original_ode} of theorem \ref{theo:3p_ode_solution}, the same apparent frequency $\omega(t)$ signal, and the dq-equivalent ODE to the complex differential equation \eqref{eq:theo_3p_ode_solution_dq_equiv}. Consider the set of differential equations

\begin{equation}
\left\{\begin{array}{l}
	\displaystyle \sum\limits_{i=0}^n \beta_i^n(t) Z^{(i)} - F = 0 ,\\[5mm]
	\displaystyle \sum\limits_{i=0}^n \eta_i(t) \dfrac{d^i z_0}{dt^i} - f_0 = 0
\end{array}\right. , \label{eq:theo_3p_ode_solution_complex_equiv}
\end{equation}

	\noindent with $Z(t) = z_d(t) + jz_q(t)$, equipped with	initial conditions $Z_0,Z'_0,Z''_0,...,Z^{(n-1)}_0$ calculated from the initial conditions of the dq system as

\begin{equation} Z_0 = z_{d0} + jz_{q0},\ Z'_0 = z'_{d0} + jz'_{q0},\ ...\ ,Z^{(n-1)}_0 = z^{(n-1)}_{d0} + jz^{(n-1)}_{q0}. \end{equation}
	
	\noindent where $F = \rho\left[f_d + jf_q\right]$ is the Dynamic Phasor Transform of the forcing $\mathbf{f}_3(t)$, and the $\beta_i^n(t)$ are time-varying complex coefficients given by

\begin{equation} \beta_i^n(t) = \sum\limits_{k=i}^{n} \alpha_k{k\choose i} \left[\sum\limits_{c=0}^{k-i} j^cB_{\left(k-i,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-i-c)}\right) \right].  \end{equation}

	\noindent and the $\eta(t)$ as defined in Theorem \ref{theo:3p_ode_solution}. Then $\mathbf{z}_{dq}(t) = \rho^{-1}\left[Z\right]$ is such that there exist $a,b\in\mathbb{R}^+$ such that

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi\left[\begin{array}{c} z_d(t) \\[3mm] z_q(t) \\[3mm] z_0(t) \end{array}\right]\right\rVert \leq ae^{-bt}. \label{eq:theo_3p_ode_complex_solution_exp}\end{equation}

	Particularly, if the initial conditions of $Z(t)$ and of $z_0(t)$ reconstruct the initial conditions of $\mathbf{x}(t)$ at initial time, then $z_d,z_q,z_0$ reconstructs $\mathbf{x}(t)$ loslessly.
\end{theorem}
\textbf{Proof:} identical to theorem \ref{corollary:complex_equivalence_phasorialodes}, by using the complexification operator onto the dq portion of \eqref{eq:theo_3p_ode_solution_dq_equiv}.
\vspace{3mm}
\hrule
\vspace{3mm}
%>>>

%-------------------------------------------------
\subsection{On the zero-sequence component}\label{subsec:zeroseq_comp} %<<<2

	A discussion on theorem \ref{corollary:3p_complex_equivalence_phasorialodes} can be made regarding the zero-sequence component $z_0(t)$. Naturally, if $z_0(t) = 0$ then $\left[z_d,z_q\right] = \rho^{-1}\left[Z\right]$ reconstructs $\mathbf{x}$ with fading exponential precision in time, that is, $Z(t)$ is sufficient to describe $\mathbf{x}$ in time, which is to say that $\mathbf{x}$ becomes balanced exponentially. Particularly, if the initial conditions of $Z(t)$ reconstruct the initial conditions of $\mathbf{x}(t)$, then $Z(t)$ reconstructs $\mathbf{x}$ perfectly, meaning $\mathbf{x}(t)$ is balanced.

	Being able to reconstruct the three-phase $\mathbf{x}(t)$ with only the Dynamic Phasor $Z(t)$ is certainly an easement, but it requires that $z_0(t) = 0$ at all times which is a rather hard requirement. We now explore more general conditions on $z_0(t)$ so that the phasor $Z(t)$ can still be used almost exclusively.

\begin{corollary}[Bounds of the solutions of LTI ODEs with three-phase forcing] \label{corollary:bounds_solution_3p_ode}%<<<

	Let $\mathbf{x}(t)$ the solution of the original time-domain LTI ODE \eqref{eq:theo_3p_ode_solution_original_ode}. Let $\mathbf{z}_{dq}^B = \left[z_d(t),z_q(t),0\right]$, the subscript ``B'' for ``balanced'', where $z_d,z_q,z_0$ are the solutions to the dq0-equivalent system \eqref{eq:theo_3p_ode_solution_dq_equiv}. Then there exist $a,b\in\mathbb{R}_+$ such that

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq}^B \right\rVert_2 \leq ae^{-bt} + \left\lvert z_0(t) \right\rvert .\end{equation}

	Particularly, if $\mathbf{z}_{dq}$ reconstructs $\mathbf{x}$ perfectly (like if they have the same initial conditions) then

\begin{equation} \left\lVert \mathbf{x} - \mathbf{x}_B \right\rVert_2 \leq \left\lvert x_0(t)\right\rvert ,\end{equation}

	\noindent where $\left\lVert\cdot\right\rVert_2$ is the Euclidean norm  and $\mathbf{x}_B = \mathbf{T}^{-1}_\psi\left[x_d(t),x_q(t),0\right]$.
\end{corollary}
\textbf{Proof:} let $\mathbf{z}_{dq} = \left[z_d,z_q,z_0\right]^\transpose,\ \mathbf{z}_{dq}^B = \left[z_d, z_q,0\right]^\transpose$. Calculating the distance between $\mathbf{z}$ and $\mathbf{z}_\infty$ yields

\begin{equation} \left\lVert \mathbf{z}_{dq} - \mathbf{z}_{dq}^B \right\rVert =  \left\lVert \left[\begin{array}{c} z_d(t) \\[3mm] z_q(t) \\[3mm] z_0(t) \end{array}\right] - \left[\begin{array}{c} z_d(t) \\[3mm] z_q(t) \\[3mm] 0 \end{array}\right]\right\rVert = \left\lvert z_0(t)\right\rvert . \label{eq:balanced_3p_ode_solution_eq2}\end{equation}

	Now note that

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi \mathbf{z}_{dq}^B \right\rVert = \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi \mathbf{z}_{dq} - \left(\mathbf{T}^{-1}_\psi \mathbf{z}_{dq}^B - \mathbf{T}^{-1}_\psi \mathbf{z}_{dq}\right) \right\rVert \leq  \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq} \right\rVert + \left\lVert\mathbf{T}^{-1}_\psi \mathbf{z}_{dq} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq}^B \right\rVert . \label{eq:balanced_3p_ode_solution_eq4}\end{equation}

	Now use the result \eqref{eq:theo_3p_ode_solution_exp} of theorem \ref{theo:3p_ode_solution}, and that

\begin{equation} \left\lVert\mathbf{T}^{-1}_\psi \mathbf{z}_{dq} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq}^B \right\rVert = \left\lVert\mathbf{T}^{-1}_\psi\left(\mathbf{z}_{dq} - \mathbf{z}_{dq}^B\right) \right\rVert \leq \left\lVert\mathbf{T}^{-1}_\psi \right\rVert \left\lVert\mathbf{z}_{dq} - \mathbf{z}_{dq}^B \right\rVert \end{equation}

	\noindent yields

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi \mathbf{z}_{dq}^B \right\rVert \leq ae^{-bt} + \left\lVert\mathbf{T}^{-1}_\psi \right\rVert\left\lvert z_0(t)\right\rvert . \label{q:balanced_3p_ode_solution_eq4}\end{equation}

	Now we estimate the norm of the operator. Using the Euclidean norm $\left\lVert \left(\cdot\right)\right\rVert_2$, by theorem \ref{theo:spectral_norm}, the Euclidean norm of a matrix $\mathbf{A}$ is given by its singular value, that is, the square root of the largest eigenvalue of the adjoint matrix $\mathbf{A}^\hermitian \mathbf{A}$. But since $\mathbf{T}^{-1}_\psi$ is orthonormal (its transpose equals its inverse), then 

\begin{equation} \left(\mathbf{T}^{-1}_\psi\right)^\hermitian \mathbf{T}^{-1}_\psi = \overline{\mathbf{T}_\psi} \mathbf{T}^{-1}_\psi , \end{equation}

	\noindent where the overline $\overline{\left(\cdot\right)}$ denotes the complex conjugate. Because both matrices are real, their conjugates are equal to themselves and

\begin{equation} \overline{\mathbf{T}_\psi} \mathbf{T}^{-1}_\psi = \mathbf{T}_\psi \mathbf{T}^{-1}_\psi = \mathbf{I}\end{equation}

	\noindent meaning the largest eigenvalue of this matrix is the singular value of the identity matrix, which is trivially unitary. Therefore

\begin{equation} \left\lVert \mathbf{T}^{-1}_\psi \right\rVert_2 = 1 \label{eq:balanced_3p_ode_solution_eq3} \end{equation}

	Finally, substituting \eqref{eq:balanced_3p_ode_solution_eq3} into \eqref{eq:balanced_3p_ode_solution_eq4} yields

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi\mathbf{z}_{dq}^B\right\rVert_2 \leq ae^{-bt} + \left\lvert z_0(t) \right\rvert .\label{eq:balanced_3p_ode_solution_eq5}\end{equation}

	Specifically, if $\mathbf{z}(t)$ reconstructs $\mathbf{x}$ through the same initial conditions,

\begin{equation} \left\lVert \mathbf{x} - \mathbf{T}^{-1}_\psi\mathbf{x}_{dq}^B\right\rVert_2 \leq \left\lvert x_0(t) \right\rvert ,\end{equation}

	\noindent where $\mathbf{x}_{dq}^B = \left[x_q(t),x_0(t),0\right]^\transpose$. Let $\mathbf{x}_B = \mathbf{T}_\psi^{-1}\mathbf{x}_{dq}^B$ and the proof is complete.

\hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

	In essence what corollary \ref{corollary:bounds_solution_3p_ode} states is that the distance between $\mathbf{x}$ , the solution of the original system, and the balanced three-phase version $\mathbf{z}^B$ of the solution of the dq-equivalent system is basically a fading exponential added to $z_0$. In the case the initial conditions of the dq0 equivalent system are the same as that of $x(t)$, the distance is only $\left\lvert x_0(t)\right\rvert$. Thus, it follows that if $z_0(t)$ vanishes assymptotically, then $\mathbf{x}$ becomes ``assymptotically balanced'', in the sense that it tends to a three-phase balanced quantity. The simplest case where $z_0(t)$ vanishes in time is, obviously, if the forcing $\mathbf{f}$ is balanced. In this case, $z_0(t) = 0$ is clearly a solution to the zero-sequence portion of \eqref{eq:theo_3p_ode_solution_complex_equiv}.

	Therefore, if $z_0(t)$ vanishes, then $\mathbf{x}^B$ is a stable steady-state solution of $\mathbf{x}$. This steady-state solution the nice property that it reconstructed by a signal $\mathbf{z}_{dq}^B$ which zero-sequence component is null, meaning that $\mathbf{x}$ tends to a balaced three-phase quantity; therefore this stable solution admits a purely phasorial representation $X(t) = x_d(t) + jx_q(t)$. Ultimately, the result of corollary \ref{corollary:bounds_solution_3p_ode} means that the stability of the steady-state solution $\mathbf{x}^B$ is the very same stability as that of $x_0(t)$, in the sense that the difference $\left\lVert \mathbf{x} - \mathbf{x}^B\right\rVert$ is bound by the same function. Therefore if $x_0$ is assymptotically stable so is $\mathbf{x}^B$; if it is exponentially stable, so is the steady-state solution. Therefore, the least needed characteristic of the three-phase forcing $\mathbf{f}_3(t)$ that causes the steady-state solution of the original ODEs \eqref{eq:theo_3p_ode_solution_original_ode} to be stable is that its zero-sequence component $f_0(t)$ define a stable ODE; this means that $\mathbf{f}_3(t)$ does not need to be actually balanced to yield a balanced solution.

	In short, $\mathbf{x}(t)$ will tend to an assymptotic quantity if the combination of the system coefficients $\left(\alpha_k\right)_{k=0}^n$, the apparent frequency $\omega(t)$ and the zero-sequence component of the forcing $f_0(t)$ are such that the equation

\begin{equation}
	\sum\limits_{i=0}^n \eta_i^n \dfrac{d^i z_0}{dt^i} - f_0 = 0,\ \eta_i(t) = \sum\limits_{k=i}^{n} \alpha_k {k\choose p} \left[\sum\limits_{c=0}^{k-i} B_{\left(k-p,c\right)}\left(\omega,\dot{\omega},\ddot{\omega},...,\omega^{(k-p-c)}\right)\right] \label{eq:3p_zeroseq_ode}
\end{equation}

	\noindent has an assymptotically vanishing solution $z_0(t)$. There is, unfortunately, no way to know preemptively if such is the case because this differential equation is linear but not time-invariant as the coefficients are time-varying; this is especially disappointing because one expects that if the forcing $\mathbf{f}_3$ is balanced, then the response $\mathbf{x}(t)$ of the system will also be balanced. Naturally, in the static case, if $\omega(t) = \omega_0$ one can prove using the same line of thought as subsection \ref{subsec:discussion_complexification} that if $f_0$ is identically null at $\omega_0$ then the system yields a Hurwitz-stable linear differential equation with constant coefficients — therefore $z_0$ tends to zero exponentially, thus $\mathbf{x}$ tends exponentially to a balanced quantity. Given the right initial conditions, $z_0$ is identically null and $\mathbf{x}$ is balanced at all times.	

	For time-varying frequencies, the time-varying nature of the coefficients pose a great challenge. While there exist many results about linear systems with time-varying coefficients (see for instance chapter 12 of \cite{beffaWeaklyNonlinearSystems2024}) guaranteeing stability (and by correlation guaranteeing that $z_0$ vanishes as time grows) is not as direct as LTI systems. For the specific case of equation \eqref{eq:3p_zeroseq_ode}, we will show in chapter \ref{chapter:choice_apparent_frequency} section \ref{sec:3p_assymp_freq} that if the circuit is very ``quick'' — the roots of the Hurwitz polynomial of the time-domain differential equation

\begin{equation} H(x) \sum\limits_{k=0}^n \alpha_k x^k \end{equation}

	\noindent have negative but large real parts — and the apparent frequency $\omega(t)$ is equivalent (in a sense that will be formally defined) to a synchronous value $\omega_0$ that is sufficiently small (the frequency is ``slow'') then the equivalent zero-sequence differential equation yields a Hurwitz stable differential equation, so that if $f_0(t)$ is bounded then $z_0(t)$ is also bounded. Particularly, if the forcing $\mathbf{f}_3$ is balanced and $f_0$ is null, then $z_0$ will assymptotically tend to zero, being zero at all times given proper initial conditions.

	In general, three-phase circuits are designed so that each phase is identical and they share the same loads. In this case, if the phases are excited by balanced excitations, then their responses (voltages and currents) will also be balanced; hence such a circuit is called a balanced circuit. The simplicity of such circuits is that because the quantities involved inevitably tend to a balanced quantity, then they can all be transformed into phasors; this allows for a single-phase representation of the balanced three-phase network, due to the fact that if the behavior of a single phase is known, then the behavior of the other two are easily drawn from the known phase.

%-------------------------------------------------
\section{Three-phase Generalized Complex Power} %<<<1

	To complete the Three-Phase Dynamic Phasors modelling, we now show that the proposed transform is able to generate a notion of complex power for three-phase circuits under generalized sinusoidal regimens.	

\begin{theorem}[Generalized Three-Phase Complex Power]\label{theo:3p_activereactivepower} %<<<
	Let $V = m_v(t)e^{j\phi_v(t)}$ and $I = m_i(t)e^{j\phi_i(t)}$ represent the three-phase dynamical phasors of the balanced voltage $\mathbf{v} = \left[v_a,v_b,v_c\right]^\transpose$ across and balanced current $\mathbf{i} = \left[i_a,i_b,i_c\right]^\transpose$ through a three-phase bipole and consider the quantity 

\begin{equation} S(t) = \left<V(t),I(t)\right> = P(t) + jQ(t)\ \left\{\begin{array}{l} P(t) = m_v(t)m_i(t)\cos\left[\phi_v(t) - \phi_i(t)\right] \\[3mm] Q(t) = m_v(t)m_i(t)\sin\left[\phi_v(t) - \phi_i(t)\right] \end{array}\right. \end{equation}

	\noindent called \textbf{complex power}. Then $S(t)$ is such that the instantaneous power performed by each phase is

\begin{equation} p_\alpha(t) = \dfrac{1}{3}P\left\{1 + \cos\left[2\left(\psi(t) + \phi_v(t) + 2\alpha\right) \right]\right\} + \dfrac{1}{3}Q\sin\left[2\left(\psi(t) + \phi_v(t) + 2\alpha \right)\right]  \end{equation}

	\noindent where $\alpha = 0$ for phase a, $-2\pi/3$ for phase b and $+2\pi/3$ for phase c. Finally, the total power performed by the three-phase bipole is

\begin{equation} p_{3\phi}(t) = \overbrace{v_a(t)i_a(t)}^{p_a(t)} + \overbrace{v_b(t)i_b(t)}^{p_b(t)} + \overbrace{v_c(t)i_c(t)}^{p_c(t)} = P(t).\end{equation}
\end{theorem}
\textbf{Proof:} basically a re-proof of theorem \ref{theo:activereactivepower}, but for three phases. First write

\begin{equation} p_{3\phi}(t) = \overbrace{v_a(t)i_a(t)}^{p_a(t)} + \overbrace{v_b(t)i_b(t)}^{p_b(t)} + \overbrace{v_c(t)i_c(t)}^{p_c(t)}.\end{equation}

	Because the voltage and current are supposed balanced,

\begin{equation} \mathbf{v} = \sqrt{\dfrac{2}{3}} m_v(t) \left[\begin{array}{c} \cos\left(\psi(t) + \phi_v(t)\right) \\[3mm] \cos\left(\psi(t) + \phi_v(t) - \dfrac{2\pi}{3}\right) \\[3mm] \cos\left(\psi(t) + \phi_v(t) + \dfrac{2\pi}{3}\right)\end{array}\right],\ \mathbf{i} = \sqrt{\dfrac{2}{3}} m_i(t) \left[\begin{array}{c} \cos\left(\psi(t) + \phi_i(t)\right) \\[3mm] \cos\left(\psi(t) + \phi_i(t) - \dfrac{2\pi}{3}\right) \\[3mm] \cos\left(\psi(t) + \phi_i(t) + \dfrac{2\pi}{3}\right)\end{array}\right] .\end{equation}

	\noindent meaning

\begin{equation} p_{3\phi}(t) = \dfrac{2}{3} m_v(t)m_i(t) \left[\begin{array}{l} \overbrace{\cos\left(\psi(t) + \phi_v(t)\right)\cos\left(\psi(t) + \phi_i(t)\right)}^{p'_a(t)} + \\[3mm] \hspace{5mm} \overbrace{\cos\left(\psi(t) + \phi_v(t) - \dfrac{2\pi}{3}\right)\cos\left(\psi(t) + \phi_i(t) - \dfrac{2\pi}{3}\right)}^{p'_b(t)} + \\[3mm] \hspace{10mm} \overbrace{\cos\left(\psi(t) + \phi_v(t) + \dfrac{2\pi}{3}\right)\cos\left(\psi(t) + \phi_i(t) + \dfrac{2\pi}{3}\right)}^{p'_c(t)} \end{array} \right] .\end{equation}

	Consider $\alpha\in\left\{-\frac{2\pi}{3},0,\frac{2\pi}{3}\right\}$ and let the expression 

\begin{equation} p_\alpha(t) = \dfrac{2}{3}m_v(t)m_i(t)\cos\left(\psi(t) + \phi_v(t) + \alpha\right)\cos\left(\psi(t) + \phi_i(t) + \alpha\right), \end{equation} 

	\noindent such that $p_a(t) = p_0(t),\ p_b(t) = p_{-\frac{2\pi}{3}},\ p_c(t) = p_{+\frac{2\pi}{3}}$,  and denote $\Delta\phi(t) = \phi_v(t) - \phi_i(t)$. Then $\phi_v(t) + \phi_i(t) = 2\phi_v(t) - \Delta\phi(t)$; therefore using

\begin{equation} \cos(a)\cos(b) = \dfrac{1}{2}\left[\cos(a+b) + \cos(a-b)\right],\end{equation}

	\noindent one obtains

\begin{align} p_\alpha(t) &= \dfrac{m_v(t)m_i(t)}{3} \left[ \cos\left(2\psi(t) + \phi_v(t) + \phi_i(t) + 2\alpha\right) + \cos\left(\phi_v(t) - \phi_i(t)\right)\right] \nonumber\\[3mm] &= \dfrac{m_v(t)m_i(t)}{3} \left\{\cos\left[2\left(\psi(t) + \phi_v(t)\right) - \Delta\phi(t) + \alpha\right] + \cos\left[\Delta\phi(t)\right]\right\} \end{align}

	Using $\cos(a-b) = \cos(a)\cos(b) + \sin(a)\sin(b)$,

\begin{equation} p_\alpha(t) = \dfrac{m_v(t)m_i(t)}{3} \left\{ \begin{array}{l} \cos\left(\Delta\phi(t)\right)\left\{\raisebox{3mm}{} 1 + \cos\left[2\left(\psi(t) + \phi_v(t) + 2\alpha\right)\right]\right\} + \\[3mm] \hspace{20mm} +\sin\left(\Delta\phi(t)\right)\sin\left[2\left(\psi(t) + \phi_v(t) + 2\alpha\right)\right] \end{array} \right\} . \label{eq:3p_nonst_complex_apparent_power_eq1} \end{equation}

	Let

\begin{equation} P = m_v(t)m_i(t) \cos\left(\Delta\phi(t)\right),\ Q = m_v(t)m_i(t) \sin\left(\Delta\phi(t)\right) \end{equation}

	\noindent then

\begin{equation} p_\alpha(t) = \dfrac{1}{3}P\left\{1 + \cos\left[2\left(\psi(t) + \phi_v(t) + 2\alpha\right) \right]\right\} + \dfrac{1}{3}Q\sin\left[2\left(\psi(t) + \phi_v(t) + 2\alpha \right)\right] . \label{eq:3p_nonst_complex_apparent_power_eq2} \end{equation}

	Now 

\begin{align}
	p_{3\phi}(t) &= p_{0}(t) + p_{\left(-\frac{2\pi}{3}\right)}(t) + p_{\left(+\frac{2\pi}{3}\right)}(t) = \nonumber\\[3mm] &= \dfrac{1}{3}P\left[ 3 + \begin{array}{l} \cos\left[2\left(\psi(t) + \phi_v(t)\right) \right] + \\[3mm] \hspace{5mm} + \cos\left[2\left(\psi(t) + \phi_v(t) + \dfrac{4\pi}{3}\right) \right] \\[3mm] \hspace{10mm} + \cos\left[2\left(\psi(t) + \phi_v(t) - \dfrac{4\pi}{3}\right) \right] \end{array}\right] + \dfrac{1}{3}Q\left[\begin{array}{l} \sin\left[2\left(\psi(t) + \phi_v(t)\right) \right] + \\[3mm] \hspace{5mm} + \sin\left[2\left(\psi(t) + \phi_v(t) + \dfrac{4\pi}{3}\right) \right] \\[3mm] \hspace{10mm} + \sin\left[2\left(\psi(t) + \phi_v(t) - \dfrac{4\pi}{3}\right) \right] \end{array}\right]
\end{align}

	\noindent and since

\begin{equation} \cos\left(x\right) + \cos\left(x + \dfrac{4\pi}{3}\right) + \cos\left(x - \dfrac{4\pi}{3}\right) = 0 \end{equation}

	\noindent for any $x$, then this means $p_{3\phi}(t) = P(t)$. \hfill$\blacksquare$
\vspace{5mm}
\hrule
\vspace{5mm} %>>>

	It is now simple to see that the expression \eqref{eq:3p_nonst_complex_apparent_power_eq1} for $p_\alpha$ can be used to draw three-phase versions of theorems \ref{theo:activepowerperiod} and \ref{theo:direct_quad_current_nonst}. More specifically, define

\begin{equation} v_\alpha(t) = m_v(t)\cos\left( \psi(t) + \phi_v(t) + \alpha\right),\ i_\alpha(t) = m_i(t)\cos\left(\psi(t) + \phi_i(t) + \alpha\right) \end{equation}

	\noindent where $\alpha = 0$ for phase a, $-2\pi/3$ for phase b and $+2\pi/3$ for phase c, it is simple to prove that there exists some $T(t)$ such that

\begin{equation} \dfrac{1}{T(t)}\int_{t}^{t+T(t)} p_\alpha(s)ds = P(t) \end{equation}

	\noindent and that

\begin{equation} i(t) = \sqrt{\dfrac{3}{2}}\dfrac{P(t)}{m_i(t)}\cos\left(\psi(t) + \phi_v(t)\right) + \sqrt{\dfrac{3}{2}}\dfrac{Q(t)}{m_v(t)}\sin\left(\psi(t) + \phi_v(t)\right) ,\end{equation}

	\noindent meaning that the three-phase active and reactive powers have the exact same physical meanings as the single-phase counterparts.

%-------------------------------------------------
\section{Some circuit analysis in three-phase domain and example simulation} %<<<1

	Finally, we want to repeat the results of theorems \ref{theo:kirchoff_current_1p} through \ref{theo:1p_inductive_impedance} for a three-phase scenario. The proof of Kirchoff's Laws is elementary and will not be re-done.

\begin{theorem}[Time-dependant 3$\phi$ capacitive impedance]\label{theo:3p_capacitive_conductance} % <<<
Let $\mathbf{v} = \left[v_a,v_b,v_c\right]$ a balanced 3$\phi$ voltage across a three-phase bank capacitors of value $C$, like in the figure below. Denote $V = \mathbf{P^{\omega}_{3\phi}}\left[\mathbf{v}\right] = v_d\left(t\right) + jv_q\left(t\right)$ as the corresponding phasor of $\mathbf{v}$, $\omega$ as its apparent frequency and $\psi = \int_{0}^{t} \omega(x)dx$. Also let $\mathbf{T}_\psi$ be the $dq0$ transform matrix at $\phi(t)$.

% THREEPHASE CAPACITOR <<<
\begin{center}
        \begin{tikzpicture}[american,scale=1,transform shape,line width=0.75, cute inductors, voltage shift = 1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
	\ctikzset{/tikz/circuitikz/voltage/distance from node=22mm}
		\draw (0,0)                             node (starta) {} to [short,o-o] ++(6,0) node (enda) {};
		\draw ([shift=({4,0})]starta.center) to [C,l=$C$, *-*,f>^=$i_a$,v=$v_a(t)$] ++(0,-6) node(bottoma) {};
		\draw (starta |- bottoma) to [short,o-o] (enda |- bottoma) ;
%	
		\draw[preaction={draw,white,line width=2mm}] ([shift=({2.5,-0.5})]starta.center) node (startb) {} to [short,o-o] ++(6,0) node (endb) {};
		\draw ([shift=({4,0})]startb.center) to [C,l=$C$, *-*,f>^=$i_b$,v=$v_b(t)$] ++(0,-6) node(bottomb) {};
		\draw (startb |- bottomb) to [short,o-o] (endb |- bottomb) ;
%	
		\draw[preaction={draw,white,line width=2mm}] ([shift=({2.5,-0.5})]startb.center) node (startc) {} to [short,o-o] ++(6,0) node (endc) {};
		\draw ([shift=({4,0})]startc.center) to [C,l=$C$, *-*,f>^=$i_c$,v=$v_c(t)$] ++(0,-6) node(bottomc) {};
		\draw (startc |- bottomc) to [short,o-o] (endc |- bottomc) ;
        \end{tikzpicture}
\end{center} %>>>

	Then the 3$\phi$ current through the bank of capacitors $\mathbf{i} = \left[i_a,i_b,i_c\right]$ is such that

\begin{align}
\left\{\begin{array}{l}
        i_d = C\dfrac{dv_d}{dt} - \omega C v_q \\[5mm]
        i_q = C\dfrac{dv_q}{dt} + \omega C v_d \\[5mm]
        i_0 = 0
\end{array}\right.
\end{align}

	Therefore the phasor

\begin{equation} I = C\dfrac{dV}{dt} + j\omega(t) C V \end{equation}

	\noindent is equal to the phasor corresponding to $\mathbf{i}$, $\mathbf{P^{\omega}_{3\phi}}\left[\mathbf{i}\right] = i_d\left(t\right) + ji_q\left(t\right)$.

\end{theorem}
\textbf{Proof:} writing the time differential equations,

\begin{equation} \mathbf{i} = 
\left[\begin{array}{c} i_a \\ i_b \\ i_c \end{array}\right] = 
\left[\begin{array}{c} C\dfrac{dv_a}{dt} \\[5mm] C\dfrac{dv_b}{dt} \\[5mm] C\dfrac{dv_c}{dt} \end{array}\right] \Leftrightarrow
 \mathbf{i} = C\dfrac{d\mathbf{v}}{dt} \end{equation}

	Applying the $dq0$ to both sides at the angle $\psi$,

\begin{align}
	\mathbf{i}_{dq0} &= \mathbf{T}_\psi \mathbf{i} \nonumber\\[3mm]
	&= \mathbf{T}_\psi C\dfrac{d\mathbf{v}}{dt} \nonumber\\[3mm] 
	&\substack{\text{(Lemma \ref{theo:dq0_3p_diff})} \\ =}\hspace{2mm} \mathbf{T}_\psi C\left[ \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{v}_{dq0}\right) + \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{v}_{dq0} \right] \nonumber\\[3mm] 
	&= C\left[ \mathbf{T}_\psi \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{v}_{dq0}\right) + \mathbf{T}_\psi \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{v}_{dq0} \right] \nonumber\\[3mm] 
	&= C\left[ \dfrac{d}{dt}\left(\mathbf{v}_{dq0}\right) + \mathbf{T_P}\left(\theta\right)\dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{v}_{dq0} \right] \nonumber\\[3mm]
	&\substack{\text{(Lemma \ref{lemma:t_ndifftminus_product})} \\ =}\hspace{2mm} C\left\{ \dfrac{d}{dt}\left(\mathbf{v}_{dq0}\right) + \dfrac{d\psi}{dt} \left[\begin{array}{ccc}    0 & -1 & 0 \\[5mm] 1 & 0  & 0 \\[5mm]  0 & 0  & 0 \end{array}\right] \mathbf{v}_{dq0} \right\} \nonumber\\[3mm]
%
%
&= \left[\begin{array}{l}
        C\dfrac{dv_d}{dt} - \omega C v_q \\[5mm]
        C\dfrac{dv_q}{dt} + \omega C v_d \\[5mm]
        C\dfrac{dv_0}{dt}
\end{array}\right]
\end{align}

	Now, because $\mathbf{v}$ is a balanced 3$\phi$ voltage, $v_0 \equiv 0$ and $V = \mathbf{P^{\omega}_{3\phi}}\left[\mathbf{v}\right] = v_d + jv_q$ completely describes $\mathbf{v}$, and the complex equation

\begin{equation} I = C\dfrac{dV}{dt} + j\omega(t)C V \end{equation}

	\noindent is such that $I$ is the phasor representation $\mathbf{P_{3\phi}}\left[\mathbf{i}\right]$ of $\mathbf{i}$.  \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
%>>>

\begin{theorem}[Time-dependant 3$\phi$ inductive impedance]\label{theo:3p_inductive_impedance} %<<< 
Let $\mathbf{i} = \left[i_a,i_b,i_c\right]$ be a balanced 3$\phi$ current across a three-phase bank of inductors of value $L$, like in the figure below. Denote $I = \mathbf{P^{\omega}_{3\phi}}\left[\mathbf{i}\right] = i_d\left(t\right) + ji_q\left(t\right)$ as the corresponding phasor of $\mathbf{i}$, $\omega$ as its apparent frequency and $\psi = \int_{0}^{t} \omega(x)dx$. Also let $\mathbf{T}_\psi$ be the $dq0$ transform matrix at $\psi(t)$.

% THREEPHASE INDUCTOR BANK <<<
\begin{center}
        \begin{tikzpicture}[american,scale=1,transform shape,line width=0.75, cute inductors, voltage shift = 1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
	\ctikzset{/tikz/circuitikz/voltage/distance from node=10mm}
		\draw (0,0) to [short,f>_=$i_a$,o-] ++(1,0) to [L,l=$L$, -o] ++(4,0);
		\draw (1,-0.5) to [open, v^=$v_a(t)$] ++(4,0) ;
%
		\draw (0.5,-2) to [short,f>_=$i_b$,o-] ++(1,0) to [L,l=$L$, -o] ++(4,0);
		\draw (1.5,-2.5) to [open, v^=$v_b(t)$] ++(4,0) ;
%
		\draw (1,-4) to [short,f>_=$i_c$,o-] ++(1,0) to [L,l=$L$, -o] ++(4,0);
		\draw (2,-4.5) to [open, v^=$v_c(t)$] ++(4,0) ;
        \end{tikzpicture}
\end{center} %>>>

	Then the 3$\phi$ voltage across the inductors $\mathbf{v} = \left[v_a,v_b,v_c\right]$ is such that

\begin{align}
\left\{\begin{array}{l}
        v_d = L\dfrac{di_d}{dt} - \omega L i_q \\[5mm]
        v_q = L\dfrac{di_q}{dt} + \omega L i_d \\[5mm]
        v_0 = 0
\end{array}\right.
\end{align}

	Therefore the phasor

\begin{equation} V = L\dfrac{dI}{dt} + j\omega(t)L I \end{equation}

	is equal to the phasor representation of $\mathbf{v}$, $\mathbf{P^{\omega}_{3\phi}}\left[\mathbf{v}\right] = v_d(t) + jy_q(t)$.
\end{theorem}
\textbf{Proof:} writing the time differential equations,

\begin{equation} \mathbf{i} = 
\left[\begin{array}{c} v_a \\ v_b \\ v_c \end{array}\right] = 
\left[\begin{array}{c} L\dfrac{di_a}{dt} \\[5mm] L\dfrac{di_b}{dt} \\[5mm] L\dfrac{di_c}{dt} \end{array}\right] \Leftrightarrow
 \mathbf{v} = L\dfrac{d\mathbf{i}}{dt} \end{equation}

	Applying the $dq0$ to both sides at the angle $\psi$,

\begin{align}
	\mathbf{v}_{dq0} &= \mathbf{T}_\psi \mathbf{v} \nonumber\\[3mm]
	=& \mathbf{T}_\psi L\dfrac{d\mathbf{i}}{dt} \nonumber\\[3mm] 
	\substack{\text{(Lemma \ref{theo:dq0_3p_diff})} \\ =}\hspace{2mm}& \mathbf{T}_\psi L\left[ \mathbf{T}^{-1}_\psi \dfrac{d}{dt}\left(\mathbf{i}_{dq0}\right) + \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{i}_{dq0} \right] \nonumber\\[3mm] 
	=& L\left[ \mathbf{T}_\psi \mathbf{T}^{-1}_\psi  \dfrac{d}{dt}\left(\mathbf{i}_{dq0}\right) + \mathbf{T}_\psi \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi \right) \mathbf{i}_{dq0} \right] \nonumber\\[3mm] 
	=& L\left[ \dfrac{d}{dt}\left(\mathbf{i}_{dq0}\right) + \mathbf{T}_\psi \dfrac{d}{dt}\left(\mathbf{T}^{-1}_\psi  \right) \mathbf{i}_{dq0} \right] \nonumber\\[3mm] 
	\substack{\text{(Lemma \ref{lemma:t_ndifftminus_product})}\\ =}\hspace{2mm}& L\left\{ \dfrac{d}{dt}\left(\mathbf{i}_{dq0}\right) + \dfrac{d\psi}{dt} \left[\begin{array}{ccc}    0 & -1 & 0 \\[5mm] 1 & 0  & 0 \\[5mm]  0 & 0  & 0 \end{array}\right] \mathbf{i}_{dq0} \right\} \nonumber\\[3mm]
	=&
\left[\begin{array}{c}
        L\dfrac{di_d}{dt} - \omega L i_q \\[5mm]
        L\dfrac{di_q}{dt} + \omega L i_d \\[5mm]
        L\dfrac{di_0}{dt}
\end{array}\right]
\end{align}

	Now, because $\mathbf{i}$ is a balanced 3$\phi$ current, $v_0 = 0$ and $I = \mathbf{P^{\omega}_{3\phi}}\left[\mathbf{i}\right]$ completely describes $\mathbf{i}$ and the complex equation

\begin{equation} V = L\dfrac{dI}{dt} + j\omega(t)L I,  \end{equation}

	\noindent is such that $V$ is the phasor representation $\mathbf{P_{3\phi}}\left[\mathbf{v}\right]$ of $\mathbf{v}$.  \hfill$\blacksquare$

\vspace{5mm}
\hrule
\vspace{5mm}
% >>>

	And we now use theorems \ref{theo:3p_capacitive_conductance} and \ref{theo:3p_inductive_impedance} to yield an exemplary modelling of a three-phase system.

\begin{example}[Dynamic Phasor modelling of a three-phase inverter-based Power System]\label{example:3p_eps_modelling}

	Consider the circuit of figure \ref{fig:ibr_modelling_example}, comprised of an inverter device with a $LR$ current filter of inductance $L_F$ and resistance $R_F$. The inverter outputs a balanced three-phase bridge voltage $e(t)$ and a three-phase bus current $i(t)$. The system is attached to an infinite bus $V_\infty$ through a double transmission line of inductance $2L$ (resulting an inductance $L$ when both lines are operational) and resistance $R$ (resulting a resistance $R$ when both lines are operational), and the terminal voltage at the connection point is $v(t)$.

	The system is equipped with two controllers. The first controller is a synchronization block in the form of a Phase-Locked Loop, schematized in figure \ref{fig:3p_pll_curr_control}. This PLL works by estimating the frequency of $v(t)$ at the connection point, and outputs a frequency $\omega_P(t)$ that is passed to the inverter bridge, such that $e(t)$ is generated with an apparent frequency $\omega_P(t)$. The PLL works by generating a local DQ frame and rotating this frame, by adjusting $\omega_P$, so as to align the DQ frame to $V_q$. This is done by estimating $V_q$ in real time and vanishing $V_q$ through a PI controller, thus estimating the frequency of the voltage $v(t)$.

	Further, the system is controlled by the current control of figure \ref{fig:3p_curr_control}. This current control consists of two PI controllers that aim to set the phasor of the current $I(t)$ to a setpoint $I_d^* + jI_q^*$. This setpoint is supposed static for this modelling. For this example, we will prove that the PI controllers of the current control adopt high integral gains, such that the current $I(t)$ reaches the setpoint much quicker than the system reaction, meaning we can consider $I_d(t)$ and $I_q(t)$ as equal to their setpoints at all times.

% MODELLING EXAMPLE: INVERTER SYSTEM <<<
\begin{figure}[htb!]
\centering
        \begin{tikzpicture}[american,scale=1,transform shape,line width=0.75, cute inductors, voltage shift = 1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
	\ctikzset{/tikz/circuitikz/voltage/distance from node=10mm}
		\node [draw, minimum width=20mm, very thick, minimum height=20mm] (inv_block) at (0,0) {};
		\draw (0,-0.6) to [D] ++ (0,1.2);
		\node (econn) at (2,0) {};
		\draw (inv_block.east) -- (econn.center);
		\draw [line width=1mm] ([shift=({0,0.5})]econn.center) -- ++(0,-1);
		\node (elabel) at ([shift=({0,1})]econn.center) {$e(t)$};
		\draw (econn.center) to [short,f>=$i(t)$] ++(2,0) to [L,l=$L_F$] ++(1,0) to [R,l=$R_F$] ++(2,0) node (vconn) {};
		\draw [line width=1mm] ([shift=({0,1})]vconn.center) -- ++(0,-2);
		\node (vlabel) at ([shift=({0,1.5})]vconn) {$v(t)$};

		\node (midnode) at ($(inv_block.center)!0.5!(vconn.center)$) {};

		\draw ([shift=({0, 0.75})]vconn.center) to [short] ++(0.5,0) to [L,l=$2L$] ++(1,0) to [R,l=$2R$] ++(2,0) node (vinfup) {};
		\draw ([shift=({0,-0.75})]vconn.center) to [short] ++(0.5,0) to [L,l=$2L$] ++(1,0) to [R,l=$2R$] ++(2,0);

		\node (vinfconn) at (inv_block -| vinfup) {};
		\draw [line width=1mm] ([shift=({0,1})]vinfconn.center) -- ++(0,-2);
		\draw (vinfconn.center) to [short] ++(1,0) to [esource,l=$V_\infty$,sources/scale=1.25, name=vinfsource] ++(1,0);
		\node (vinflabel) at (vinfsource) {{\Large $\infty$}};

		\node [draw, minimum width=20mm, very thick, minimum height=20mm, below=20mm of midnode] (pll_block) {PLL};

		\draw[->] (vconn) |- ([shift=({1mm,0})]pll_block.east);
		\draw[->] (pll_block.west) -| ([shift=({0,-1mm})]inv_block.south);

		\node (omega_pll_label) at ([shift=({-10mm,3mm})]pll_block.west) {$\omega_P(t)$};
        \end{tikzpicture}
	\caption{Inverter-based circuit for example modelling of nonstationary three-phase system.}
	\label{fig:ibr_modelling_example}
\end{figure} %>>>

	Before modelling this system, one needs to get a full grasp of all references and phasorial representations involved. Naturally, the device responsible for generating the angle and time references is the synchronization device, the PLL. When the PLL is turned on and starts counting time at $t=0$, it essentially generates two frames: a static real-imaginary frame and a mobile DQ frame. The DQ frame starts exactly in phase with the real-imaginary frame at $t=0$, and rotates at the time-varying $\omega_P$ angular frequency that is the PLL estimation of the frequency of $v(t)$. What the PLL essentially does is adjust the DQ frame, through the frequency estimation, so that the DQ frame is in phase with the Dynamic Phasor $V(t)$ of $v(t)$, generated against that same DQ frame. This is done by vanishing quadrature signal $V_q(t)$ through a PI controller which output is a frequency deviation $\Delta\omega(t)$, which is then added to the synchronous frequency $\omega_0$ to generate the frequency estimation $\omega_P$. As such, if $V_q > 0$, this means that $V(t)$ is ahead of the DQ frame, and $\Delta\omega$ rises to match the DQ frame to the voltage; conversely, if $V_q < 0$ then $V(t)$ is behind the DQ frame, and $\Delta\omega$ lowers to match the frame to the voltage.

% PLL SUBSTYSTEM <<<
\begin{figure} 
\centering
\scalebox{1}{
\begin{tikzpicture}[scale=1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]

\node[left] at (0,0) (signalinputb) {$v_b(t)$};
\node[left] at ([shift=({0, 5mm})]signalinputb.east) (signalinputa) {$v_a(t)$};
\node[left] at ([shift=({0,-5mm})]signalinputb.east) (signalinputc) {$v_c(t)$};

\node [draw, minimum width=2cm, very thick, minimum height=2cm, right=1cm of signalinput] (abdq_block) {};

\draw[->] (signalinputa.east) -- ([shift=({-1mm,0})] signalinputa -| abdq_block.west);
\draw[->] (signalinputb.east) -- ([shift=({-1mm,0})] signalinputb -| abdq_block.west);
\draw[->] (signalinputc.east) -- ([shift=({-1mm,0})] signalinputc -| abdq_block.west);

\draw (abdq_block.south west) -- (abdq_block.north east);
\node at ([shift=({{ 2cm*sqrt(2)/4},{-2cm*sqrt(2)/4}})]abdq_block.north west) {\large $abc$};
\node at ([shift=({{-2cm*sqrt(2)/4},{ 2cm*sqrt(2)/4}})]abdq_block.south east) {\large $dq0$};

\draw[->] ([shift=({0, 3.33mm})]abdq_block.east) -- ([shift=({10mm,3.33mm})]abdq_block.east) node[at end, above] {$V_d(t)$};

\node (arrowpi) at ([shift=({30mm,-3.33mm})]abdq_block.east) {};
\node at (arrowpi) [draw, minimum width=2cm, very thick, minimum height=10mm] (pi_block) {$\xi_P + \dfrac{\xi_I}{s}$};

\draw[->] ([shift=({0,-3.33mm})]abdq_block.east) -- ([shift=({-1mm,0})]pi_block.west) node[midway, below] {$V_q(t)$};

\node (pilabel) at ([shift=({0mm,2mm})]pi_block.north) {PI Controller};

\node[draw, circle, very thick, minimum size=10mm, right=2cm of pi_block] (sum) {}; % SUM CIRCLE
\node (omegazero_node) at ([shift=({0mm,15mm})]sum.north) {$\omega_0$};
\draw[->] (omegazero_node.south) -- ([shift=({0mm,1mm})]sum.north);
\node at ([shift=({3mm,2mm})]sum.north) {$+$};

\draw[->] (pi_block.east) -- ([shift=({-1mm,0})]sum.west) node[midway, below] {$\Delta\omega$};
\node at ([shift=({-2mm,3mm})]sum.west) {$+$};

\node [right=8mm of pi_block] (midway_deltaomega) {};
\node [draw, very thick, minimum width=1cm, minimum height=1cm, above=15mm of midway_deltaomega] (angle_integrator) {$\dfrac{1}{s}$};
\draw[->] (midway_deltaomega.center) -- ([shift=({0mm,-1mm})]angle_integrator.south);
\draw[->] (angle_integrator.north) -- ([shift=({0mm, 10mm})]angle_integrator.north);
\node at ([shift=({0,12mm})]angle_integrator.north) {$\Delta\psi(t)$};

\node [right=10mm of sum] (midway_omega) {};
\node [draw, very thick, minimum width=1cm, minimum height=1cm, below=10mm of midway_omega] (psi_integrator) {$\dfrac{1}{s}$};
\draw[->] (sum.center -| psi_integrator) -- ([shift=({0mm,1mm})]psi_integrator.north);

\node [below=10mm of psi_integrator] (midway_psi) {};
\draw[-] (psi_integrator.south) -- (midway_psi.center);
\draw[->] (midway_psi.center) -| ([shift=({0mm,-1mm})]abdq_block.south);

\node [right, right=10mm of midway_omega] (omeganode) {$\omega_P$};
\draw[->] (sum.east) -- ([shift=({-3mm,0mm})]omeganode.center);

\node [right=10mm of midway_psi] (psinode) {$\psi$};
\draw[->] (midway_psi.center) -- ([shift=({-2mm,0mm})]psinode.center);
\end{tikzpicture}
}
\caption{Three-phase Phase Locked Loop synchronization subsystem for the circuit of figure \ref{fig:ibr_modelling_example}.}
\label{fig:3p_pll_curr_control}
\end{figure}
%>>>

	Insofar as the DQ frame and the real-imaginary frames are locally generated, to complete the modelling one needs to consider the angle reference of the grid, which is defined by the infinite bus voltage $v_\infty$. This voltage has by definition a static frequency $\omega_0$, and a constant amplitude. This means that with respect to the static real-imaginary frame the vector $V_\infty$ has constant amplitude and rotates at a fixed frequency $\omega_0$. More importantly, this voltage has a fixed phase with respect to the synchronous reference of the grid; the problem here being that the PLL subsystem has no knowledge of the grid reference, and it must be estimated. In order to do this, a vector $R$ for ``reference'' is generated; this vector also starts in phase with the real axis and spins at the synchronous frequency $\omega_0$ and simulates the synchronous grid reference with respect to the local DQ frame, such that by definition the angle displacement between $R$ and $V_\infty$ is a fixed $\phi_0$.	The phasorial diagram is shown in figure \ref{fig:dynamic_phasor_dqaxis_ibr}.

	By definition, the DQ frame and the synchronous reference $R$ have an angle displacement that is given by 

\begin{equation} \Delta\psi(t) = \int_0^t \Delta\omega(s) ds = \int_0^t \left[\omega_P(s) - \omega_0\right]ds, \end{equation}

	\noindent thus measuring how advanced the DQ frame is with respect to the real reference vector $R$. Because $V_\infty$ has a constant angle diference $\phi_0$ with respect to $R$, then naturally it starts at $t=0$ as $V_\infty = \left\lvert V_\infty\right\rvert e^{j\phi_0}$, meaning that with respect to the static frame it is described in time as $V_\infty = \left\lvert V_\infty\right\rvert e^{j\phi_0}e^{j\omega_0 t}$. Thus it  has an angle displacement with the DQ frame of $\phi_0 + \Delta\psi(t)$. Therefore, with respect to the DQ frame, the infinite bus voltage is modelled as

\begin{equation} V_\infty = \left\lvert V_\infty\right\rvert e^{j\left(\phi_0 + \Delta\psi(t)\right)} \end{equation}

	\noindent and $\phi_0$ is calculated from the initial conditions of the system.  If the entire diagram is spun by $-j\psi(t)$, placing the DQ frame as the reference frame, one achieves the representation of all quantities with respect to the DQ frame, as shown in figure \ref{fig:dynamic_phasor_dqaxis_ibr_dqframe} which is a copy of figure \ref{fig:dynamic_phasor_dqaxis_ibr} but with all quantities rotated by $-j\psi(t)$.

% DYNAMIC PHASOR DIAGRAM OF THREE-PHASE SYSTEM <<<
\begin{figure}[htb!]
\centering
\scalebox{0.8}{
	\begin{tikzpicture}[scale=2,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
		\draw [->] (   -2mm,  0   ) -- (   40mm,  0   ) node (xaxis) {};
		\draw [->] (      0, -2mm ) -- (   0   ,  40mm) node (yaxis) {};

		\node (reAxisLabel) at (42mm,0) {Re};
		\node (imAxisLabel) at (0,42mm) {Im};

		\draw [->, black!50] (0,0) -- ({40mm*cos(25)}, {40mm*sin(25)});
		\draw [->, black!50] (0,0) -- ({40mm*cos(115)},{40mm*sin(115)});

		\node[label={[text=stewartpink, label distance=1mm]0:$Re^{j\omega_0 t}$}] (ReAxisLabel) at ({40mm*cos(10)} ,{40mm*sin(10)})  {};
		\draw[->, stewartpink] (0,0) -- (ReAxisLabel.center);
		\draw [->, stewartpink] ({25mm*cos(10)},{25mm*sin(10)}) arc[start angle=10, end angle = 23, radius = 25mm];
		\node [stewartpink] (philabel) at ({28mm*cos(17)},{28mm*sin(17)}) {$\Delta\psi(t)$};

		\node[right,black!50] (DAxisLabel) at ({42mm*cos(25) - 2mm} ,{42mm*sin(25)})  {$D$};
		\node[black!50]       (QAxisLabel) at ({42mm*cos(115)},      {42mm*sin(115)}) {$Q$};

		\node [black!50] (omegat) at ({35mm*cos(32)},{35mm*sin(32)}) {$\omega_P(t)$};
		\draw [-{Stealth[inset=0mm,length=3.5mm,angle'=50]}, black!50, line width = 1mm] ({35mm*cos(20)},{35mm*sin(20)}) arc[start angle=20, end angle = 30, radius = 35mm];

		\node [stewartpink] (omegat) at ({35mm*cos(17)},{35mm*sin(17)}) {$\omega_0$};
		\draw [-{Stealth[inset=0mm,length=3.5mm,angle'=50]}, stewartpink, line width = 1mm] ({35mm*cos(5)},{35mm*sin(5)}) arc[start angle=5, end angle = 15, radius = 35mm];

		\draw [->,black!50,thick] ({38mm*cos(0)},{38mm*sin(0)}) arc[start angle=0, end angle = 23, radius = 38mm];
		\node [right,gray] (psilabel) at ({39mm*cos(18)},{39mm*sin(18)}) {$\psi(t)$};

		\node [right, stewartyellow] (elabel) at ({35mm*cos(65)},{35mm*sin(65)}) {$E(t)e^{j\psi(t)}$};
		\draw [->, stewartyellow] (0,0) -- (elabel);

		\node[right, stewartblue] (vlabel) at ({40mm*cos(45)},{40mm*sin(45)}) {$V(t)e^{j\psi(t)}$};
		\draw [->, stewartblue] (0,0) -- (vlabel);
		\draw [->, stewartblue] ({25mm*cos(25)},{25mm*sin(25)}) arc[start angle=25, end angle=40, radius = 25mm];
		\node [stewartblue] (phivlabel) at ({28mm*cos(32)},{28mm*sin(32)}) {$\phi_v(t)$};

		\node[label={[text=stewartgreen, label distance=1mm]0:$\left\lvert V_\infty\right\rvert e^{j\phi_0}e^{j\omega_0 t}$}] (vinflabel) at ({42mm*cos(-20)},{42mm*sin(-20)}) {};
		\draw [->,   stewartgreen] (0,0) -- (vinflabel.center);
		\draw [->,   stewartgreen] ({30mm*cos(10)},{30mm*sin(10)}) arc[start angle=10, end angle = -18, radius = 30mm];
		\node [stewartgreen] (philabel) at ({33mm*cos(-15)},{33mm*sin(-15)}) {$\phi_0$};

		\node[right, stewartpurple] (ilabel) at ({35mm*cos(-10)},{35mm*sin(-10)}) {$I(t)e^{j\psi(t)}$};
		\draw [->,   stewartpurple] (0,0) -- (ilabel);
	\end{tikzpicture}
	}
	\caption
[Phasor diagram for the system being studied in the real-imaginary static frame.]
{Phasor diagram for the system being studied in the real-imaginary static frame. Note: in this scenario, $\phi_0$ is negative for clarity of the schematic.}
	\label{fig:dynamic_phasor_dqaxis_ibr}
\end{figure} %>>>

% DYNAMIC PHASOR DIAGRAM OF THREE-PHASE SYSTEM IN THE DQ FRAME <<<
\begin{figure}[htb!]
\centering
\scalebox{0.8}{
	\begin{tikzpicture}[scale=2,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
		\draw [->, black!50] (   -2mm,  0   ) -- (   40mm,  0   ) node (xaxis) {};
		\draw [->, black!50] (      0, -2mm ) -- (   0   ,  40mm) node (yaxis) {};

		\node[right,black!50] (DAxisLabel) at ({42mm*cos(0) - 2mm} ,{42mm*sin(0)})  {$D$};
		\node[black!50]       (QAxisLabel) at ({42mm*cos(90)},      {42mm*sin(90)}) {$Q$};

		\node [label={[text=stewartpink, label distance=1mm]0:$Re^{-j\Delta\psi(t)}$}] (ReAxisLabel) at ({42mm*cos(-15)} ,{42mm*sin(-15)})  {};
		\draw [->, stewartpink] (0,0) -- (ReAxisLabel.center);
		\draw [->, stewartpink] ({20mm*cos(-15)},{20mm*sin(-15)}) arc[start angle=-15, end angle = -2, radius = 20mm];
		\node [stewartpink] (philabel) at ({23mm*cos(-8)},{23mm*sin(-8)}) {$\Delta\psi(t)$};

		%\node [black!50] (omegat) at ({35mm*cos(7)},{35mm*sin(7)}) {$\omega(t)$};
		%\draw [-{Stealth[inset=0mm,length=3.5mm,angle'=50]}, black!50, line width = 1mm] ({35mm*cos(-5)},{35mm*sin(-5)}) arc[start angle=-5, end angle = 5, radius = 35mm];

		\node [stewartpink] (omegat) at ({38mm*cos(-8)},{38mm*sin(-8)}) {$\omega_0 - \omega_P(t)$};
		\draw [-{Stealth[inset=0mm,length=3.5mm,angle'=50]}, stewartpink, line width = 1mm] ({38mm*cos(-20)},{38mm*sin(-20)}) arc[start angle=-20, end angle = -10, radius = 38mm];

		\node[right, stewartyellow] (elabel) at ({35mm*cos(40)},{35mm*sin(40)}) {$E(t)$};
		\draw [->, stewartyellow] (0,0) -- (elabel);

		\node[right, stewartblue] (vlabel) at ({40mm*cos(20)},{40mm*sin(20)}) {$V(t)$};
		\draw [->, stewartblue] (0,0) -- (vlabel);
		\draw [->, stewartblue] ({25mm*cos(0)},{25mm*sin(0)}) arc[start angle=0, end angle = 17, radius = 25mm];
		\node [stewartblue] (phivlabel) at ({28mm*cos(8)},{28mm*sin(8)}) {$\phi_v(t)$};

		\node[label={[text=stewartgreen, label distance=1mm]0:$\left\lvert V_\infty\right\rvert e^{j\left(\phi_0 + \Delta\psi(t)\right)}$}] (vinflabel) at ({42mm*cos(-45)},{42mm*sin(-45)}) {};
		\draw [->,   stewartgreen] (0,0) -- (vinflabel.center);
		\draw [->,   stewartgreen] ({30mm*cos(-15)},{30mm*sin(-15)}) arc[start angle=-15, end angle = -43, radius = 30mm];
		\node [stewartgreen] (philabel) at ({33mm*cos(-40)},{33mm*sin(-40)}) {$\phi_0$};

		\node[right, stewartpurple] (ilabel) at ({35mm*cos(-35)},{35mm*sin(-35)}) {$I(t)$};
		\draw [->,   stewartpurple] (0,0) -- (ilabel);

	\end{tikzpicture}
	}
	\caption
[Phasor diagram for the system being studied in the mobile DQ frame.]
{Phasor diagram for the system being studied in the mobile DQ frame. Note: in this scenario, $\phi_0$ is negative for clarity of the schematic.}
	\label{fig:dynamic_phasor_dqaxis_ibr_dqframe}
\end{figure} %>>>

	We first model the circuit.  It is only natural to adopt the frequency signal $\omega_P$ for the Dynamic Phasor Transform of the modelling. Once this is set, one can start modelling the system through the Dynamic Phasor relationships of theorems \ref{theo:3p_capacitive_conductance} and \ref{theo:3p_inductive_impedance}, obtaining

\begin{equation} \left\{\begin{array}{l} E = V + R_F I + L_F\left(\dot{I} + j\omega L I\right) \\[3mm] V = \left|V\right|_\infty e^{j\left(\phi_0 + \Delta\psi\right)} + RI + L\left(\dot{I} + j\omega L I\right) \end{array}\right. \label{eq:circuit_3pmodel_complex} \end{equation}

	\noindent and separating the system into direct and quadrature components,

\begin{equation} \left\{\begin{array}{l}E_d = V_d + R_FI_d - \omega L_FI_q + L_F \dot{I}_d\\[2mm] E_q = R_FI_q + \omega L_FI_d + V_q + L_F \dot{I}_q\\[2mm] V_d = \left|V\right|_\infty\cos\left(\phi_0 + \Delta\psi\right) + RI_d - \omega L I_q + L \dot{I}_d \\[2mm] V_q = RI_q + \omega LI_d - \left|V\right|_\infty\sin\left(\phi_0 + \Delta\psi\right) +  L \dot{I}_q\end{array}\right. \label{eq:circuit_3pmodel_vq} \end{equation}

	Where $\omega$ is the apparent frequency signal adopted, which is understood as being $\omega_P$ from now on. Using these equations we can achieve a model of the PLL. The original PLL equations are

\begin{equation} \left\{\begin{array}{l} \dot{\left(\Delta\psi\right)} = \omega_P - \omega_0 \\[3mm] \dot{\omega}_P = \xi_IV_q + \xi_P\dot{V}_q \end{array}\right. \end{equation}

	\noindent and one obtains both $V_q$ and $\dot{V}_q$ from the electrical equations \eqref{eq:circuit_3pmodel_vq}:

\begin{gather}
	\hspace{-5cm} \dfrac{d\omega_P}{dt} = \xi_I\left[ RI_q + \omega_P L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] + \nonumber\\[3mm] \hspace{3cm} + \xi_P\left[  R \dfrac{dI_q}{dt} + L\omega_P \dfrac{dI_d}{dt} + L \dfrac{d\omega_P}{dt}I_d + \left|V\right|_\infty\dfrac{d\Delta\psi}{dt}\cos\left(\phi_0+\Delta\psi\right) \right]
\end{gather}

	\noindent now considering that $d\left(\Delta\psi\right)/dt = \omega_P$ and isolating $d\omega_P/dt$,

\begin{gather}
        \left(1-\xi_P LI_d\right)\dfrac{d\omega_P}{dt} = \xi_I\left[ RI_q + \omega_P L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] + \\[3mm] \hspace{25mm} +  \xi_P\left[  R \dfrac{dI_q}{dt} + L\omega_P \dfrac{dI_d}{dt}\right] + \xi_P\left[ \left|V\right|_\infty\Delta\omega \cos\left(\phi_0+\Delta\psi\right) \right] \nonumber\\[5mm]
%
        \dfrac{d\omega_P}{dt} = \xfrac{5mm}{3mm}{\xi_I\left[ RI_q + \omega L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] + \xi_P\left(R \dfrac{dI_q}{dt} + L\omega_P \dfrac{dI_d}{dt}\right) + \xi_P\left[\raisebox{5mm}{} \left|V\right|_\infty\Delta\omega \cos\left(\phi_0 + \Delta\psi\right) \right]}{1-\xi_PLI_d} \label{eq:diffomega_diq_did}
\end{gather}

	This equation makes the dynamic modelling of the PLL-controlled system; the equations for $I_d$ and $I_q$ and their derivatives are needed to complete the model, which we obtain from the model of the current control. The current control works as follows: a setpoint $I_d^* + jI_q^*$ is supplied to the current control, and the control supplies references $E^*_d + jE^*_q$ for the direct and quadrature components of the bridge voltage $e(t)$ so that the bridge acts to impose a three-phase voltage $\left[e_a,e_b,e_c\right]$. We suppose that the switching bridge is fast enough so that the voltage $e(t)$ is immediately set to the setpoints, that is, $E_d = E^*_d$ and $E_q = E^*_q$ at all times.

% CURRENT CONTROL SYSTEM <<<
\begin{figure} 
\centering
\scalebox{0.75}{
\begin{tikzpicture}[american,scale=1,transform shape,line width=0.75, cute inductors, voltage shift = 1,>={Stealth[inset=0mm,length=1.5mm,angle'=50]}]
\node (origin) at (0,0) {};

\node [draw, very thick, isosceles triangle, minimum height=15mm, minimum width=15mm] at (0mm, 15mm) (omegaLGainD) {$\omega L_F$};
\node [draw, circle, very thick, minimum size=10mm, right=20mm of omegaLGainD] (sumDomega) {}; % SUM CIRCLE
\node at ([shift=({-2mm,-3mm})]sumDomega.west) {$-$};
\node at ([shift=({-3mm, 2mm})]sumDomega.north) {$+$};

\node [draw, very thick, isosceles triangle, minimum height=15mm, minimum width=15mm] at (0mm,-15mm) (omegaLGainQ) {$\omega L_F$};
\node [draw, circle, very thick, minimum size=10mm, right=20mm of omegaLGainQ] (sumQomega) {}; % SUM CIRCLE
\node at ([shift=({-2mm, 3mm})]sumQomega.west) {$+$};
\node at ([shift=({-3mm,-2mm})]sumQomega.south) {$+$};

\node at (-10mm, 40mm) [draw, very thick, isosceles triangle, minimum height=15mm, minimum width=15mm] (rGainUp) {$R_F$};
\node at (-10mm,-40mm) [draw, very thick, isosceles triangle, minimum height=15mm, minimum width=15mm] (rGainDown) {$R_F$};

\node at (-40mm, 60mm) [draw, circle, very thick, minimum size=10mm] (sumDin) {}; % SUM CIRCLE
\node at ([shift=({-2mm, 3mm})]sumDin.west) {$+$};
\node at ([shift=({-3mm,-2mm})]sumDin.south) {$-$};

\node at (-40mm,-60mm) [draw, circle, very thick, minimum size=10mm] (sumQin) {}; % SUM CIRCLE
\node at ([shift=({-2mm,-3mm})]sumQin.west) {$+$};
\node at ([shift=({-3mm, 2mm})]sumQin.north) {$-$};

\node at (60mm, 60mm) [draw, circle, very thick, minimum size=10mm] (sumDoff) {}; % SUM CIRCLE
\node at ([shift=({-2mm, 3mm})]sumDoff.west) {$+$};
\node at ([shift=({-3mm,-2mm})]sumDoff.south) {$+$};

\node at (60mm,-60mm) [draw, circle, very thick, minimum size=10mm] (sumQoff) {}; % SUM CIRCLE
\node at ([shift=({-2mm,-3mm})]sumQoff.west) {$+$};
\node at ([shift=({-3mm, 2mm})]sumQoff.north) {$+$};

\path[name path=qSumInOut] (sumQin) -- (sumQoff) node [pos=0.5, midway] (piQ_block_node) {};
\path[name path=dSumInOut] (sumDin) -- (sumDoff) node [pos=0.5, midway] (piD_block_node) {};

\node at (piD_block_node) [draw, minimum width=2cm, very thick, minimum height=10mm] (piD_block) {$k_P^d + \dfrac{k_I^d}{s}$};
\node at (piQ_block_node) [draw, minimum width=2cm, very thick, minimum height=10mm] (piQ_block) {$k_P^q + \dfrac{k_I^q}{s}$};

\node (idIn) [left=60mm of omegaLGainD]    {$I_d$};
\node (iqIn) [left=60mm of omegaLGainQ]  {$I_q$};
\node (idInBreak) [right=40mm of idIn] {};
\node (iqInBreak) [right=40mm of iqIn] {};

\path[name path=northsouth] (sumDin) -- (sumQin);

\path[name path=Dhor] (idIn) -- (omegaLGainD);
\path[name path=Qhor] (iqIn) -- (omegaLGainQ);
\path[name path=DhorUp, very thick] (rGainUp.west) -- ([shift=({-50mm,0})]rGainUp.west);
\path[name path=QhorDown, very thick] (rGainDown.west) -- ([shift=({-50mm,0})]rGainDown.west);

\path [name intersections={of=northsouth and Dhor,by=idInBreak}];
\path [name intersections={of=northsouth and Qhor,by=iqInBreak}];
\path [name intersections={of=northsouth and DhorUp,by=idInBreakRGain}];
\path [name intersections={of=northsouth and QhorDown,by=iqInBreakRGain}];

\node (idRefIn) [left=20mm of sumDin] {$I_d^*$};
\node (iqRefIn) [left=20mm of sumQin] {$I_q^*$};

\draw[->] ([shift={(4mm,0mm)}]idRefIn) to ([shift=({-2mm,0})]sumDin.west);
\draw[->] ([shift={(4mm,0mm)}]iqRefIn) to ([shift=({-2mm,0})]sumQin.west);

\draw[->] (idInBreak.center) to ([shift=({-17mm,0})]omegaLGainQ.west) to ([shift=({-2mm,0})]omegaLGainQ.west);
\draw[->] (iqInBreak.center) to ([shift=({-17mm,0})]omegaLGainD.west)   to ([shift=({-2mm,0})]omegaLGainD.west)  ;

\draw[->] ([shift={(4mm,0mm)}]idIn.center) -| ([shift={(0mm,-2mm)}]sumDin.south);
\draw[->] ([shift={(4mm,0mm)}]iqIn.center) -| ([shift={(0mm, 2mm)}]sumQin.north);

\draw[->] (idInBreakRGain.center) to ([shift={(-2mm, 0mm)}]rGainUp.west);
\draw[->] (iqInBreakRGain.center) to ([shift={(-2mm, 0mm)}]rGainDown.west);

\draw[->] (sumDin.east) -- ([shift={(-2mm, 0mm)}]piD_block.west);
\draw[->] (piD_block.east) -- ([shift={(-2mm, 0mm)}]sumDoff.west) node[midway, above] {$V_d^*$};

\draw[->] (sumQin.east) -- ([shift={(-2mm, 0mm)}]piQ_block.west);
\draw[->] (piQ_block.east) -- ([shift={(-2mm, 0mm)}]sumQoff.west) node[midway, below] {$V_q^*$} ;
\draw[->] (sumDomega.east) -| ([shift={(0mm, -2mm)}]sumDoff.south);
\draw[->] (omegaLGainD.east) -- ([shift={(-2mm, 0mm)}]sumDomega.west);

\draw[->] (rGainUp.east) -| ([shift={(0mm, 2mm)}]sumDomega.north);
\draw[->] (rGainDown.east) -| ([shift={(0mm,-2mm)}]sumQomega.south);
\draw[->] (sumQomega.east) -| ([shift={(0mm,  2mm)}]sumQoff.north);
\draw[->] (omegaLGainQ.east) -- ([shift={(-2mm, 0mm)}]sumQomega.west);

\node [draw, minimum width=30mm, very thick, minimum height=150mm] (bridgeblock) at ([shift=({40mm,0})] origin -| sumQoff) {};

\draw ([shift={(0mm, -15mm)}]bridgeblock.center) to [D,/tikz/circuitikz/bipoles/length=3cm,line width=0.75mm] ([shift={(0mm, 15mm)}]bridgeblock.center);

\draw[->] (sumQoff.east) -- ([shift={(-1mm, 0mm)}]bridgeblock.west |- sumQoff) node[midway,below] {$E_d^*$};
\draw[->] (sumDoff.east) -- ([shift={(-1mm, 0mm)}]bridgeblock.west |- sumDoff) node[midway,above] {$E_q^*$};

\draw[->] ([shift={(0mm, 20mm)}]bridgeblock.east) -- ([shift={(10mm, 20mm)}]bridgeblock.east) node[right] {$e_a$};
\draw[->] ([shift={(0mm,  0mm)}]bridgeblock.east) -- ([shift={(10mm,  0mm)}]bridgeblock.east) node[right] {$e_b$};
\draw[->] ([shift={(0mm,-20mm)}]bridgeblock.east) -- ([shift={(10mm,-20mm)}]bridgeblock.east) node[right] {$e_c$};

\end{tikzpicture}
}
\caption{Current control subsystem for the inverter system of figure \ref{fig:ibr_modelling_example}.}
\label{fig:3p_curr_control}
\end{figure}
%>>>

	In order to enforce the current setpoints, the current controller aims to adjust the terminal voltage $V(t)$ by inputting the differences $I_d^* - I_d$ and $I_q^* - I_q$ into PI controllers and generating reference signals $V_d^*,V_q^*$:

\begin{equation} \left\{\begin{array}{l}
\dfrac{d V_d }{dt} = k_P^d\dfrac{d\left(I_d^* - I_d\right)}{dt} + k_I^d\left(I_d^* - I_d\right) \\[5mm]
%
\dfrac{d V_q}{dt} = k_P^q\dfrac{d\left(I_q^* - I_q\right)}{dt} + k_I^q\left(I_q^* - I_q\right) 
\end{array}\right. \label{eq:vdiff}
\end{equation}

	However, the system itself cannot adjust the terminal voltage; rather, it can actuate on the bridge voltage. To this extent, it supposes that the relationship between $E$ and $V$ is given by

\begin{equation} E = V + \left(R_F + j\omega L_F\right)I \left\{\begin{array}{l} E_d = V_d + R_FI_d - j\omega I_q \\[3mm] E_q = V_q + R_FI_q + j\omega I_d \end{array}\right. \label{eq:example_quasistatic_supposition}\end{equation}

	\noindent therefore yielding from \eqref{eq:vdiff}:

\begin{equation} \left\{\begin{array}{l}
\dfrac{d\left(R_FI_d - \omega L_FI_q + V_d\right)}{dt} = k_P^d\dfrac{d\left(I_d^* - I_d\right)}{dt} + k_I^d\left(I_d^* - I_d\right) + R_F\dfrac{dI_d}{dt} - \dfrac{d\left(\omega L_F I_q\right)}{dt} \\[5mm]
%
\dfrac{d\left(R_FI_q + \omega L_FI_d + V_q\right)}{dt} = k_P^q\dfrac{d\left(I_q^* - I_q\right)}{dt} + k_I^q\left(I_q^* - I_q\right) + R_F\dfrac{dI_q}{dt} + \dfrac{d\left(\omega L_F I_d\right)}{dt} 
\end{array}\right. \label{eq:evdiff}
\end{equation}

	\noindent and this generates the ``crossed signals'' seen on the current control schematic on figure \ref{fig:3p_curr_control}. This control clearly supposes a static phasor framework, highlighting its incompatibility with the Dynamic Phasor modelling. However, since this is a widely-used controller, it will be maintained for this modelling, and a better controller will be proposed later in this thesis. Developing the PI controller equations of the current controller. Using the circuit equations \eqref{eq:circuit_3pmodel_vq} on \eqref{eq:evdiff},

\begin{equation}
\left\{\begin{array}{l}
\dfrac{d}{dt}\left[\left|V\right|_\infty\cos\left(\phi_0+\Delta\psi\right) + RI_d - \omega LI_q\right] = k_P^d\dfrac{d\left(I_d^* - I_d\right)}{dt} + k_I^d\left(I_d^* - I_d\right) \\[5mm]
%
\dfrac{d}{dt}\left[R I_q + \omega_P L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] = k_P^q\dfrac{d\left(I_q^* - I_q\right)}{dt} + k_I^q\left(I_q^* - I_q\right) 
\end{array}\right.
\end{equation}

        And developing this system,

\begin{equation}
\left\{\begin{array}{l}
-\left|V\right|_\infty \Delta\omega\sin\left(\phi_0+\Delta\psi\right) + R\dfrac{dI_d}{dt} - L\left(\omega_P \dfrac{dI_q}{dt} + \dfrac{d\omega_P}{dt}I_q\right) = k_P^d\dfrac{d\left(I_d^* - I_d\right)}{dt} + k_I^d\left(I_d^* - I_d\right) \\[5mm]
%
R\dfrac{dI_q}{dt} + L\left(\omega_P \dfrac{dI_d}{dt} + \dfrac{d\omega_P}{dt}I_d\right) - \left|V\right|_\infty\Delta\omega\cos\left(\phi_0+\Delta\psi\right) = k_P^q\dfrac{d\left(I_q^* - I_q\right)}{dt} + k_I^q\left(I_q^* - I_q\right) 
\end{array}\right. \label{sys:idiq_system_1}
\end{equation}

        Substituting \eqref{eq:diffomega_diq_did} into the first equation,

\begin{gather}
\left(R - \dfrac{LI_q\xi_PL\omega}{1-\xi_PL_FI_d} + \xi_P^d\right)\dfrac{dI_d}{dt} - L_F\left(\omega_P + \dfrac{I_q\xi_PR}{1-\xi_PL_FI_d}\right) \dfrac{dI_q}{dt} = \nonumber\\[5mm] \hspace{30mm} = L_FI_q\left\{\xfrac{3mm}{2mm}{\xi_I\left[ RI_q + \omega_P L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] + \xi_P \left|V\right|_\infty\Delta\omega \cos\left(\phi_0+\Delta\psi\right)}{1-\xi_PL_FI_d}\right\} + \nonumber\\[3mm] \hspace{20mm} + \left|V\right|_\infty \Delta\omega\sin\left(\phi_0+\Delta\psi\right) + k_P^d\dfrac{dI_d^*}{dt} + k_I^d\left(I_d^* - I_d\right) \label{eq:d_diff_eq}
\end{gather}

        Now substitute \eqref{eq:diffomega_diq_did} into the second equation of \eqref{sys:idiq_system_1}:

\begin{gather}
\left(R + \dfrac{LI_dk_PR}{1-k_PL_FI_d} + k_P^q \right)\dfrac{dI_q}{dt} + L\left(\omega_P + \dfrac{I_dk_PL_F\omega_P}{1-k_PL_FI_d}\right)\dfrac{dI_d}{dt} = \nonumber\\[5mm]
%
\hspace{20mm}-LI_d\left\{\xfrac{2mm}{2mm}{\xi_I\left[ RI_q + \omega_P L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] + \xi_P\left|V\right|_\infty\Delta\omega \cos\left(\phi_0+\Delta\psi\right)}{1-\xi_PL_FI_d}\right\} + \nonumber\\[3mm] \hspace{20mm} + \left|V\right|_\infty\Delta\omega\cos\left(\phi_0+\Delta\psi\right) + k_P^q\dfrac{dI_q^*}{dt} + k_I^q\left(I_q^* - I_q\right) \label{eq:q_diff_eq}
\end{gather}

	Thus \eqref{eq:d_diff_eq} and \eqref{eq:q_diff_eq} form a system of equations from which $\dot{I}_d$ and $\dot{I}_q$ can be obtained. We now make a timescale argument on the controller equations: dividing \eqref{eq:d_diff_eq} by $k_I^d$ and \eqref{eq:q_diff_eq} by $k_I^q$, and then making these gains very high, one obtains $I_d^* - I_d \approx 0$ and $I_q^* - I_q \approx 0$. Reestated, by means of adoption of high integral gains for the current control the PI controllers become considerably fast so that we can consider that the components $I_d$ and $I_q$ are very close to their references at all times. Since we are adopting constant references, we can use $\dot{I}_d = \dot{I}_q = 0$ and simplify the PLL model \eqref{eq:diffomega_diq_did} as

\begin{equation} \dfrac{d\omega_P}{dt} = \xfrac{5mm}{3mm}{\xi_I\left[ RI_q + \omega L I_d - \left|V\right|_\infty\sin\left(\phi_0+\Delta\psi\right)\right] + \xi_P\left[\raisebox{5mm}{} \left|V\right|_\infty\Delta\omega \cos\left(\phi_0 + \Delta\psi\right) \right]}{1-\xi_PLI_d} \label{eq:diffomega_diq_did_simplified} \end{equation}

	Therefore, the circuit equations \eqref{eq:circuit_3pmodel_vq} and the simplified PLL equations \eqref{eq:diffomega_diq_did_simplified} form a differential-algebraic model of the system. Using these equations, we simulate a line-break fault. At $t=1s$, one of the transmission lines that ties the terminal connection $v(t)$ to the infinite bus $v_\infty(t)$ breaks, and is restored at $t=2s$. Note that the system modelling during fault is obtained by substituting $L$ by $2L$ and $R$ by $2R$ due to one line not being operational.

	The parameters and initial conditions adopted are shown in tables \ref{tab:3p_sim_params} and \ref{tab:3p_sim_init_conds}. The resulting plots of the simulation are shown in figure \ref{fig:freqsignal_3psim} for the PLL frequency $\omega_P$, \ref{fig:voltagesignals_3psim} for the terminal voltage phasor $V(t)$ and figure \ref{fig:powersimsignals} for the active and reactive power supplied by the inverter.

% TABLES OF PARAMETERS <<<
\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
\begin{center}
\begin{tabular}{ c|c|c|c|c|c|c|c|c|c } 
\hline 
Parameter & $L$ & $R$ & $L_F$ & $R_F$ & $\left\lvert V_\infty \right\rvert$ & $\phi_0$ & $\xi_I$ & $\xi_P$ & $\omega_0$ \\
\hline
Value & 10mH & 0 $\Omega$ & 2H & 10m$\Omega$ & 100 V & $\pi/6$ & 10 & 5 & 120$\pi$ \\ 
\hline
\end{tabular}
\end{center}
\caption{Parameter values adopted for the three-phase system simulation.}
\label{tab:3p_sim_params}
\end{table} %>>>

% TABLES OF INITIAL CONDS <<<
\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
\begin{center}
\begin{tabular}{ c|c|c|c|c} 
\hline 
Parameter & $I_d$ & $I_q$ & $V_d$ & $V_q$ \\
\hline
Value & 1A & 13.262647 A & 82.845892 V & 0 V \\ 
\hline
\end{tabular}
\end{center}
\caption{Initial values adopted for the three-phase system simulation.}
\label{tab:3p_sim_init_conds}
\end{table}
 %>>>

% FREQUENCY SIGNAL FROM SIMULATION <<<
\begin{figure}[t]
\centering
                \begin{tikzpicture}
                        \begin{axis}[
                                width  = \textwidth,
                                height = \textwidth / 1.618 ,
%                               title={Frequency signal $\omega$},
                                xlabel={$t (s)$},
                                ylabel={$\omega_P \left(\times 120\pi\right)$},
				ymajorgrids=true,
                                xmajorgrids=true,
                                xmin=0, xmax=7,
                                ymin=0.997, ymax=1.0024,
                                xtick={0,1,...,7},
                                ytick={0.997,0.998,...,1.002},
				yticklabel style={/pgf/number format/precision=3},
                                legend pos=north east,
                                legend cell align={left},
                                every axis plot/.append style={very thick, no marks},
                        ]
			\addplot[color=blue, thick] table [x index=0,y index=1,col sep=comma] {data/3psim/data_3psim.csv};
                        \end{axis}
                \end{tikzpicture}
        \caption{Resulting frequency signal of fault simulation.}
        \label{fig:freqsignal_3psim}
\end{figure}
% >>>

% VOLTAGE SIGNALS FROM SIMULATION <<<
\begin{figure}[h]
\centering
                \begin{tikzpicture}
                        \begin{axis}[
                                width  = \textwidth,
                                height = \textwidth / 1.618 ,
				ylabel near ticks,
                                xlabel={$t (s)$},
                                ylabel={$V_d,V_q\left(\times \left|V_\infty\right|\right)$},
				ymajorgrids=true,
                                xmajorgrids=true,
                                xmin=0, xmax=7,
                                ymin=-0.55, ymax=1,
                                ytick={-0.5,-0.25,...,1},
                                xtick={0,1,...,7},
                                legend pos=south east,
                                legend cell align={left},
                                every axis plot/.append style={very thick, no marks},
                        ]
			\addplot[color=blue, thick] table [x index=0,y index=2,col sep=comma] {data/3psim/data_3psim.csv};
			\addplot[color=red , thick] table [x index=0,y index=3,col sep=comma] {data/3psim/data_3psim.csv};
			\legend{$V_q(t)$,$V_d(t)$}
                        \end{axis}
                \end{tikzpicture}
        \caption{Resulting voltage signals of fault simulation.}
        \label{fig:voltagesignals_3psim}
\end{figure}
% >>>

% POWER SIGNALS <<<
\begin{figure}[h]
\centering
                \begin{tikzpicture}
                        \begin{axis}[
                                width  = \textwidth,
                                height = \textwidth / 1.618 ,
                                xlabel={$t (s)$},
                                ylabel={$P,Q \left(\times\left|V_\infty\right|I_d\right)$},
                                xmin=0, xmax=7,
                                ymin=-0.5, ymax=1,
				ymajorgrids=true,
                                xmajorgrids=true,
                                xtick={0,1,...,7},
                                ytick={-0.4,-0.2,...,0.8},
                                legend pos=south east,
                                legend cell align={left},
                                every axis plot/.append style={very thick, no marks},
                        ]
			\addplot[color=blue, thick] table [x index=0,y index=4,col sep=comma] {data/3psim/data_3psim.csv};
			\addplot[color=red, thick] table [x index=0,y index=5,col sep=comma]  {data/3psim/data_3psim.csv};
			\legend{$P(t)$,$Q(t)$}
                        \end{axis}
		\end{tikzpicture}
\vspace{-5mm}
        \caption{Resulting power signals of fault simulation.}
        \label{fig:powersimsignals}
\end{figure}
% >>>

\examplebar
\end{example}
